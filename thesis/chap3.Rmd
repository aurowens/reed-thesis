---
header-includes:
- \usepackage{amssymb,amsthm,amsmath}
- \usepackage{chemarr}
- \usepackage{algorithm}
- \usepackage{algpseudocode}
- \usepackage{pifont}
- \usepackage{float}
- \usepackage{tikz}

output: pdf_document
---

<!--
You can delete the header-includes (lines 3-6 above) if you like and also the chunk below since it is loaded in the skeleton.Rmd file.  They are included so that chap3.Rmd will compile by itself when you hit Knit PDF.
-->

```{r include_packages_2, include = FALSE}
# This chunk ensures that the reedtemplates package is
# installed and loaded. This reedtemplates package includes
# the template files for the thesis and also two functions
# used for labeling and referencing
if(!require(devtools))
  install.packages("devtools", repos = "http://cran.rstudio.com")
if(!require(dplyr))
    install.packages("dplyr", repos = "http://cran.rstudio.com")
if(!require(ggplot2))
    install.packages("ggplot2", repos = "http://cran.rstudio.com")
if(!require(reedtemplates)){
  library(devtools)
  devtools::install_github("ismayc/reedtemplates")
  }
library(reedtemplates)
library(tree)
library(reedtemplates)
library(MASS)
library(ggplot2)
library(randomForest)
library(maptree)
library(knitr)
library(GGally)
library(wesanderson)
library(reshape)

```
```{r infforestcode3, echo=FALSE, message=FALSE, warning=FALSE}
library(tree)
library(reedtemplates)
library(MASS)
library(ggplot2)
library(randomForest)
library(maptree)
library(knitr)
library(GGally)
library(reshape)
library(ggplot2)
library(plotly)
library(gridExtra)
###################################GROWING A TREE#########################################
rss.node <- function(i,y,x) { 
  yleftpred <- mean(y[x< i])
  yrightpred <- mean(y[x >= i])
  rssl <- sum((y[x< i]-yleftpred)^2)
  rssr <- sum((y[x >= i] - yrightpred)^2)
  return(rssl + rssr)
}

rss.leaf <- function(a){
  a1 <- rep(mean(a), length(a))
  sum((a-a1)^2)
}

max.cor <- function(yy,sxs){
  
  max <- list()
  cors <- cbind(rep(0,ncol(sxs)),seq(1,ncol(sxs)))
  for(i in 1:ncol(sxs)){
    cors[i,1] <- ifelse(sd(sxs[,i]) == 0,0, suppressWarnings(cor(yy, sxs[,i])))
  }
  
  cors[is.na(cors[,1]), 1] <- 0
  
  if(sum(abs(cors[,1])) == 0){
      
      return(NULL)
    }
  
  if (sum(cors[,1] == max(abs(cors[,1]), na.rm = TRUE), na.rm = TRUE) > 1) {
    
    maxes <- cors[(as.numeric(cors[,1] == max(abs(cors[,1]), na.rm = TRUE),1)),]
    max1 <- maxes[maxes[,1] == max(maxes[,1]),2]
    
  } else {
    
    max1 <- cors[abs(cors[,1]) == max(abs(cors[,1]), na.rm = TRUE),2]
  }
  
  if(is.null(ncol(sxs[,max1]))){
  max[[length(max)+1]] <- sxs[,max1]
  max[[length(max)+1]] <- names(sxs)[max1]
  return(max)
  }else{
    #print(sxs[,max1])
  }
  
}

split.rf <- function(y,x, min) {
  parent.rss <- rss.leaf(y)
  sp <- list()
   if(length(x) <= min) {
    return(NULL)
  } else {
  if(sd(y) < .05){
      return(NULL)
  }
  if(sd(as.vector(x)) < .05){
      return(NULL)
    }
 
    
    #gives a warning if it cannot find an optimal solution and instead gives the max point
    op.partition <- suppressWarnings(optimise(rss.node, interval = range(x), 
                                              upper = max(x), y=y,x=x, maximum = FALSE))
    leaf.rssl <- rss.leaf(y[x < op.partition[[1]]])
    leaf.rssr <-rss.leaf(y[x < op.partition[[1]]])
    if(parent.rss < leaf.rssl | parent.rss < leaf.rssr){
       return(NULL)
     }
       
    
    if (length(x[x < op.partition[[1]]]) < min | length(x[x >= op.partition[[1]]]) < min){
      
      return(NULL)
    }
    sp[[length(sp)+1]] <- op.partition[[1]] #partition on x
    sp[[length(sp)+1]]<-  mean(y[x< op.partition[[1]]]) #ypred left daughter
    sp[[length(sp)+1]]<- mean(y[x >= op.partition[[1]]]) #ypred right daughter
    sp[[length(sp)+1]]<- x[x < op.partition[[1]]] #xsleftdaughter
    sp[[length(sp)+1]]<- x[x >= op.partition[[1]]] #xsrightdaughter
    sp[[length(sp)+1]] <- rss.leaf( y[x < op.partition[[1]]]) #ldrss
    
    return(sp)
  }
}

tree.rf <- function(y,xs, mtry, bootsampled, min = 5){
  tree <- list()
  tree.frame <- list()
  tree.exc <- list()
  leaf.partitions <- list()
  bootsample <- seq(1, length(y))
  
  
  #if( length(y) != nrow(xs)){
   # return("error, length y != dim xs")
 # }
  
  noder <- function(y,xs,mtry, min){
    
    ##what i want this function to do:
    ###given y,xs (like an interval of them):
    ####1. find the split between them 
    
    frame <- node1(y,xs, mtry, min)
    #df <-  rbind(df,frame)
    ####2. call itself on each side of the split
    split <- as.numeric(frame[5])
    spliton <- frame[1]
    
    if(frame[1] == "<leaf>"){
      return(frame)
    } else {
      if(is.null(ncol(xs))){
        yhatl <- y[xs < split]
        xsl <- xs[xs < split]
        yhatr <- y[xs >= split]
        xsr <- xs[xs >= split]
      }else{
        yhatl <- y[xs[,spliton[[1]]] < split]
        xsl <- xs[xs[,spliton[[1]]] < split,]
        yhatr <- y[xs[,spliton[[1]]] >= split]
        xsr <- xs[xs[,spliton[[1]]] >= split, ]
      }
      #    if(length(yhatl) < min & length(yhatr) < min){
      #      print(df)
      #      return(df)
      #    } else {
      #y = yhatr
      #xs = xsr
      noder(y = yhatr,xs = xsr,mtry, min)
      noder(y = yhatl,xs = xsl,mtry, min)
    }
    #  }
  }
  node1 <- function(y, xs, mtry, min){
    onex <- FALSE
    
    if(is.null(ncol(xs))){
      onex <- TRUE
    }
    if(onex == FALSE){
      xssrd <- xs[,sample(ncol(xs),mtry)]
      In <- names(xs) %in% names(xssrd)
      In <- !In 
      NotIn <- names(xs)[In]
    } else {
      xssrd <- as.data.frame(xs)
      NotIn <- NULL
    }
    
    frame <- data.frame("", 0,0,0,0)
    frame[,1] <- as.character(frame[,1])
    leaf.p <- data.frame(rep(0,2), rep(0,2), rep(0,2))
    
    
    if(length(y) < min) {
      frame[1,] <- c("<leaf>",nrow(xssrd), rss.leaf(y), mean(y), 0)
      
      tree.frame <<- rbind(tree.frame, frame)
      leaf.p <- sapply(xs, range)
      leaf.partitions[[length(leaf.partitions)+1]] <<- leaf.p
      return(frame)
      
    } else {
      if (onex){
        maxxr <- list(xs, "X")
      } else {
        maxxr <- max.cor(yy = y,sxs= xssrd)
      }
     if(is.null(dim(maxxr[[1]]))){
      sprd <- split.rf(y, maxxr[[1]], min)
     }
      if(is.null(sprd)) {
        frame[1,] <- c("<leaf>",nrow(xssrd),rss.leaf(y),  mean(y), 0)
        
        tree.frame <<- rbind(tree.frame, frame)
        leaf.p <- sapply(xs, range)
        leaf.partitions[[length(leaf.partitions)+1]] <<- leaf.p
        return(frame)
        
      } else if(length(maxxr[[1]] < sprd[[1]]) < min |length(maxxr[[1]] >= sprd[[1]]) < min  ) {
        
        frame[1,] <- c("<leaf>",nrow(xssrd),rss.leaf(y), mean(y), 0)
        
        tree.frame <<- rbind(tree.frame, frame)
        leaf.p <- sapply(xs, range)
        leaf.partitions[[length(leaf.partitions)+1]] <<- leaf.p
        return(frame)
      }
      
      frame[1,] <- c(maxxr[[2]],
                     nrow(xssrd),
                     rss.node(sprd[[1]], y, maxxr[[1]]),
                     mean(y),
                     sprd[[1]])
      # frame[2:4] <- as.numeric(frame[2:4])
    }
    tree.exc <<- rbind(tree.exc, NotIn)
    tree.frame <<- rbind(tree.frame, frame)
    return(frame)
  }
  
  if(bootsampled){
    bootsample <- sample(length(y), length(y), replace = TRUE)
    xs <- xs[bootsample,]
    y <- y[bootsample]
    
  }
  #min <-5
  noder(y, xs, mtry,min)
  names(tree.frame) <- c("var", "n", "dev", "ypred","split.cutleft")
  tree[[1]] <- bootsample
  tree[[2]] <- tree.frame
  tree[[3]] <- tree.exc
  return(tree)
}

###################################AUX TREE FUNCTIONS#######################################
predict.tree.rf <- function(t,xs) {
  t <- t[[2]] 
  first.split <- t[1,]
  onex <- FALSE
  
  if(sum(is.na(t$ypred)) > 0){
    return(NULL)
  }
  if(first.split$var == "<leaf>") {
    return(first.split$ypred)
  }
  
  t$n <- as.numeric(t$n)
  t$split.cutleft <- as.numeric(t$split.cutleft)
  rdn <- t$n[2]
  ldn <- t$n[1] - rdn 
  ldname <- as.numeric(row.names(t[t$n == ldn,]))
  
  if(is.null(ncol(xs))){
    onex <- TRUE
  }
  
  if(length(ldname) > 1) {
    
    right.daughter <- t[1:(ldname[length(ldname)]-1),]
    left.daughter <- t[ldname[length(ldname)]:nrow(t),]
    
  } else {
    
    right.daughter <- t[1:(ldname[length(ldname)]),]
    left.daughter <- t[ldname:nrow(t),]
  }
  
  
  j <- 0
  if(onex){
    predictions <- c(rep(100000, length(xs)))
  } else {
  predictions <- c(rep(100000, nrow(xs)))
  }
  ###SKIPPING CONDITION
  #there's no need to map out the whole tree and have it on file, each y just needs the tree to 
  #a. check the split condition, i.e. xs[,var] < split.cutleft 
  #b. if yes, go two ahead
  #b.2. if no, go one ahead
  #c. stop at leaf, pred[i] <- ypred
  
  if(onex) {
    for (i in 1: length(xs)) {
      xsh <- xs[i]
      if(xsh < first.split$split.cutleft){
        #left daughter
        ld <- left.daughter[1,]
        j <- 1
        while (ld$var != "<leaf>"){
          if (xsh < ld$split.cutleft) {
            j <- j+2
          } else {
          j <- j+1
          }
          ld <- left.daughter[j,]
        }
        predictions[i] <- ld$ypred
      } else {
       #right daughter
        rd <- right.daughter[2,]
        j <- 2
        while (rd$var != "<leaf>"){
          if (xsh < rd$split.cutleft) {
            j <- j+2
          } else {
            j <- j+1
          }
          rd <- right.daughter[j,]
        }
        predictions[i] <- rd$ypred
      }    
    }
  } else {
  
    for(i in 1:(nrow(xs))){
      xsh <- xs[i,]
      if(xsh[,first.split$var] < first.split$split.cutleft){
        #left daughter
        ld <- left.daughter[1,]
        j <- 1
        while (ld$var != "<leaf>"){
          if (xsh[,ld$var] < ld$split.cutleft) {
            j <- j+2
          } else {
            j <- j+1
          }
          ld <- left.daughter[j,]
        }
        predictions[i] <- ld$ypred
      } else {
        #right daughter
        rd <- right.daughter[2,]
        j <- 2
       while (rd$var != "<leaf>"){
          if (xsh[,rd$var] < rd$split.cutleft) {
            j <- j+2
          } else {
            j <- j+1
          }
         rd <- right.daughter[j,]
        }
        predictions[i] <- rd$ypred
      }
    }
  }
  return(predictions)
}

rssforest <- function(rf, y, xs){
  rss <- sum((y[-rf[[1]][[1]]] - as.numeric(predict.tree.rf(rf[[1]],xs[-rf[[1]][[1]],])))^2)
  rss.frame <- rss
  for(i in 2:(length(rf))){ ##random forest is a list, pairs of trees + bootsamples
    t <- rf[[i]]
    rss.frame <- rbind(rss.frame,sum((y[-rf[[i]][[1]]] - as.numeric(predict.tree.rf(t,xs[-rf[[i]][[1]],])))^2))
    
    ##send tree off to inftrees with the -bootsampled data
    ##get back frame
  }
  
  ##generate distribution of vi's from frames - pval
  return(mean(rss.frame))
}

###################################GROWING A FOREST#########################################

rforest <- function(y, xs,  mtry, ntree) { 
  #imputs: formula to be tested and the dset to test on. outputs: list,
  #contains: total oob error, trees, and bootsample for the tree  
  rforest <- list() #define empty list for the rf. length = ntree*2+1
  #where each tree is a list, bootsample + frame  
  xsi <- c()
  di <- data.frame()
  # form <- as.character(form)
  # yn <- form[2]
  # yi <- as.numeric(names(d) == yn)
  # y <- d[,yi]
  # xs <- d[,(yi + 1 - 2)*-1]
  
  while (length(rforest) < ntree) {
    rforest[[length(rforest)+1]] <- tree.rf(y,xs,mtry, bootsampled = TRUE)
  }
  return(rforest)
}


###################################INFFOREST#################################################

infforest <- function(rf,y,xs) {
  v <- inftrees(rf[[1]],y[-rf[[1]][[1]]],xs[-rf[[1]][[1]],])
  vi.frame <- v
  for(i in 2:(length(rf))){ ##random forest is a list, pairs of trees + bootsamples
    t <- rf[[i]]
    vi.frame <- rbind(vi.frame,inftrees(t, y[-t[[1]]], xs[-t[[1]],]))
    
    ##send tree off to inftrees with the -bootsampled data
    ##get back frame
  }
  
  ##generate distribution of vi's from frames - pval
  return(vi.frame)
}


infforest.test <- function(inf){
  p <- rep(10000, ncol(inf))
  mu <- c()
  sd <- c()
  for(i in 1:ncol(inf)){
    mu[i] <- mean(inf[,i])
    sd[i] <- sd(inf[,i])
    p[i] <- pnorm(0,mean = mu[i], sd = sd[i])
  }
  return(data.frame("var" =  names(as.data.frame(inf)), "P(var > 0)"= p, "mean" = mu, "sd" = sd))
}

##############################Y PERMUTED VARIABLE IMPORTANCE################################

yperm.inf <- function(rf,y,xs) {
  v <- ytrees(rf[[1]],y[-rf[[1]][[1]]],xs[-rf[[1]][[1]],])
  v1 <- v
  for (i in 2:length(rf)){
    v1 <-  ytrees(rf[[i]],y[-rf[[1]][[1]]],xs[-rf[[1]][[1]],])
    v <- rbind(v,v1)
  }
  
  return(v)
}

##############################CONDITIONAL VARIABLE IMPORTANCE################################

conditional.inf <- function(rf, y, xs) {
  v <- strobltrees(rf[[1]],y[-rf[[1]][[1]]],xs[-rf[[1]][[1]],])
  v1 <- v
  for (i in 2:length(rf)){
    v1 <-  strobltrees(rf[[i]],y[-rf[[1]][[1]]],xs[-rf[[1]][[1]],])
    v <- rbind(v,v1)
  }
  
  return(v)
}

###############################PERMUTED VARIABLE IMPORTANCE################################

permuted.inf <- function(rf, y, xs) {
  v <- breimantrees(rf[[1]],y[-rf[[1]][[1]]],xs[-rf[[1]][[1]],])
  
  for (i in 2:length(rf)){
    v <- rbind(v, breimantrees(rf[[1]],y[-rf[[1]][[1]]],xs[-rf[[1]][[1]],]))
  }
  
  return(v)
  
}

###################################INFTREES SUPPORT###########################################

inftrees <- function(t, y, xs) {
  
  rss.post <- rep(0, ncol(xs))
  vi <- rep(0, ncol(xs))
  
  ta <- t
  t <- t[[2]]
  
  # set.seed(1)
  #for(i in 1:ncol(xs)){
  #  rss.pre[i] <- VIdev(t, names(xs)[i])
  #}
  
  rss.pre <- sum((y - as.numeric(predict.tree.rf(ta, xs)))^2)/length(y)
  
  set.seed(1)
  for(i in 1:ncol(xs)){
    xs.for.permuting.i <- xs
    ti <- tree.rf(xs[,i], xs[,-i], mtry = ncol(xs[,-i]), bootsample = FALSE) 
    if(nrow(ti[[2]]) <= 3) {
        xs.for.permuting.i[,i] <- sample(xs.for.permuting.i[,i])
      } else {
          xi.partitions <- predict.tree.rf(ti, xs[,-i])
          xs.for.permuting.i$groups <- as.factor(xi.partitions)
          for (j in 1:length(levels(xs.for.permuting.i$groups))) {
            for(n in 1:ncol(xs)){
              xs.for.permuting.i[xs.for.permuting.i$group == levels(xs.for.permuting.i$groups)[j],n] <-
                sample(xs.for.permuting.i[xs.for.permuting.i$group == levels(xs.for.permuting.i$groups)[j],n], 
                     length(xs.for.permuting.i[xs.for.permuting.i$group == levels(xs.for.permuting.i$groups)[j],n]))
            }
          }
      }
    predictions.for.y.xi.perm <- predict.tree.rf(ta,xs.for.permuting.i)
    rss.post[i] <- abs(sum((y - as.numeric(predictions.for.y.xi.perm))^2)/length(y) - rss.pre)
    #ex <- sum(names(xs)[i] ==  ta[[3]])/nrow(ta[[3]])
    # if (is.null(ex)){
    #   rss.post[i] <- rss.post[i] *0
    # } else if(ex > .7) {
     # rss.post[i] <- rss.post[i] * (ex)
    # }
  }
  rss.post <- rss.post/max(abs(rss.post))
  names(rss.post) <- names(xs)
  return(rss.post)
}

ytrees <- function(t,y,xs) {
  d <- rbind(y,xs)
  rss.post <- rep(0, ncol(xs))
  ta <- t
  t <- t[[2]]
  predictions <- as.numeric(predict.tree.rf(ta, xs))
  rss.pre <- sum((y - predictions)^2)/length(y)
  xs.for.permuting.i <- xs
  yperm <- y
  for(i in 1:ncol(xs)){
    corrr <- sapply(xs[,-i],cor, y = xs[,i])
    xs.i <- xs[,-i]
    xs.i <-  (xs.i[,abs(corrr) > .2])
    xs.i <- cbind(xs[,i], xs.i)
    if(ncol(xs.i) == 1){
      yperm <- sample(y)
      rss.post[i] <- (rss.pre-sum((yperm - predictions)^2)/length(y) )
      xs.i <- xs
       yperm <- y
    } else {
      fr <- t[t$var %in% names(xs.i)[-1],]
      if(nrow(fr) == 0) {
      yperm <- sample(y)
      rss.post[i] <- (rss.pre-sum((yperm - predictions)^2)/length(y) )
      xs.i <- xs
      yperm <- y
      } else {
        for (j in 1:length(levels(as.factor(fr$var)))) {
          fri <- fr[fr$var == levels(as.factor(fr$var))[j],]
          for(n in 1:nrow(fri)){
            yperm[xs.i[,fri$var[n]] < fri$split.cutleft[n]] <- sample(y[xs.i[,fri$var[n]] < fri$split.cutleft[n]])
          }
        }
        rss.post[i] <- (rss.pre - sum((yperm - predictions)^2)/length(y))
        xs.i <- xs
        yperm <- y
      }
    }
  }
  if(sum(abs(rss.post)) == 0) {
    return(rss.post)
  } else{
   # rss.post <- rss.post/max(abs(rss.post))
    # names(rss.post) <- names(xs)
    return(rss.post)
  }

}


strobltrees <- function(t,y,xs) {
  rss.post <- rep(0, ncol(xs))
  
  ta <- t
  t <- t[[2]]
  
  predictions <- as.numeric(predict.tree.rf(ta, xs))
  rss.pre <- sum((y - predictions)^2)/length(y)
  
  xs.for.permuting.i <- xs
  
  for(i in 1:ncol(xs)){
    corrr <- sapply(xs[,-i],cor, y = xs[,i])
    xs.i <- xs[,-i]
    xs.i <-  (xs.i[,abs(corrr) > .2])
    xs.i <- cbind(xs[,i], xs.i)
    
    if(ncol(xs.i) == 1 | is.null(dim(xs.i))){
      xs.i <- sample(xs.i)
      xs.ii <- xs
      xs.ii[,i] <- xs.i
      names(xs.ii) <- names(xs)
      predictions.for.y.xi.perm <- predict.tree.rf(ta,xs.ii)
      rss.post[i] <- abs(sum((y - as.numeric(predictions.for.y.xi.perm))^2)/length(y) - rss.pre)
      xs.i <- xs
    } else {
        fr <- t[t$var %in% names(xs.i)[-1],]
        if(nrow(fr) == 0) {
          xs.i[,1] <- sample(xs.i[,1])
          xs.ii <- xs
          xs.ii[,i] <- xs.i[,1]
          names(xs.ii) <- names(xs)
          predictions.for.y.xi.perm <- predict.tree.rf(ta,xs.ii)
          rss.post[i] <- abs(sum((y - as.numeric(predictions.for.y.xi.perm))^2)/length(y) - rss.pre)
          xs.i <- xs
      } else {
        for (j in 1:length(levels(as.factor(fr$var)))) {
          fri <- fr[fr$var == levels(as.factor(fr$var))[j],]
          for(n in 1:nrow(fri)){
            xs.i[xs.i[,fri$var[n]] < fri$split.cutleft[n] ,1] <- sample(xs.i[xs.i[,fri$var[n]] < fri$split.cutleft[n] ,1])
          }
        }
        xs.ii <- xs
        xs.ii[,i] <- xs.i[,1]
        names(xs.ii) <- names(xs)
        predictions.for.y.xi.perm <- predict.tree.rf(ta,xs.ii)
        rss.post[i] <- abs(sum((y - as.numeric(predictions.for.y.xi.perm))^2)/length(y) - rss.pre)
        xs.i <- xs
        ex <- sum(names(xs)[i] ==  ta[[3]])/nrow(ta[[3]])
        if(ex > .8){
          rss.post[i] <- rss.post[i] * (ex)
        }
      }
    }
  }
  if(sum(abs(rss.post)) == 0) {
    return(rss.post)  
  } else{
    rss.post <- rss.post/max(abs(rss.post))
    # names(rss.post) <- names(xs)
    return(rss.post)
  }
  
}

breimantrees <- function (t,y,xs){
  ta <- t
  t <- t[[2]]
  
  rss.pre <- sum((y - as.numeric(predict.tree.rf(ta, xs)))^2)
  
  rss.post <- rep(0, ncol(xs))
  xs.for.permuting <- xs
  for (i in 1:ncol(xs)){
    xs.for.permuting[,i] <- sample(xs.for.permuting[,i], replace = TRUE)
    predictions.for.y.xi.perm <- predict.tree.rf(ta,xs.for.permuting)
    rss.post[i] <- abs(sum((y - as.numeric(predictions.for.y.xi.perm))^2) - rss.pre)
    xs.for.permuting <- xs
  }
  rss.post <- rss.post/max(abs(rss.post))
  names(rss.post) <- names(xs)
  return(rss.post)
}

```
```{r loaddatad3, echo=FALSE}
rep(0, 12) -> mu #mean of each variable

diag(12) -> sigma #creates a 12 by 12 diagonal matrix with 1's down the diagonal

sigma -> sigma1
.9 -> sigma1[1,2]
.9 -> sigma1[2,1] #adding the block correlation between the first four variables
.9 -> sigma1[1,3]
.9 -> sigma1[3,1]
.9 -> sigma1[3,2]
.9 -> sigma1[2,3]
.9 -> sigma1[1,4]
.9 -> sigma1[4,1]
.9 -> sigma1[2,4]
.9 -> sigma1[4,2]
.9 -> sigma1[4,3]
.9 -> sigma1[3,4]

1000 -> n #number of observations
set.seed(1)
mvrnorm(n =n, mu = mu, Sigma = sigma1) -> sim1000 #sampling from the multivariate normal 
c(5,5,2,0,-5,-5,0,0,0,0,0,0) -> bts #these are the betas 

rnorm(1000, mean = 0, sd = .5) -> e #error terms

rep(0, 1000) -> ys #init a vector of zeros

for( i in 1:1000){ #go row by row and create the ys based on the function of e, bts, and sim1000
ys[i] <- sim1000[i,1]*bts[1]+
    sim1000[i,2]*bts[2]+ 
    sim1000[i,3]*bts[3]+ 
    sim1000[i,4]*bts[4]+ 
    sim1000[i,5]*bts[5]+ 
    sim1000[i,6]*bts[6]+
    sim1000[i,7]*bts[7]+ 
    sim1000[i,8]*bts[8]+ 
    sim1000[i,9]*bts[9]+ 
    sim1000[i,10]*bts[10]+ 
    sim1000[i,11]*bts[11]+ 
    sim1000[i,12]*bts[12] +
    e[i]
}

d1 <- as.data.frame(sim1000)
d1$y <- ys



n <- 10000
c(5,5,2,0,-5,-5,0,0,0,0,0,0) -> bts #these are the betas 
rnorm(1000, mean = 0, sd = .5) -> e #error terms



x1 <- rnorm(1000)

x2 <- 2*log(abs(x1)) + rnorm(1000)

x3 <- 2*log(abs(x2)) + rnorm(1000)

x4 <- 2*log(abs(x3)) + rnorm(1000)

mvrnorm(n =n, mu = mu, Sigma = sigma) -> simx5x12

data.frame(x1,x2,x3,x4,simx5x12) -> d2
              
c("X1","X2","X3","X4","X5","X6", "X7","X8","X9","X10","X11","X12") -> names(d2)

rep(0, 1000) -> ys #init a vector of zeros

for( i in 1:1000){ #go row by row and create the ys based on the function of e, bts, and d2
ys[i] <- d2[i,1]^2*bts[1]+
    d2[i,2]^2*bts[2]+ 
    d2[i,3]^2*bts[3]+ 
    d2[i,4]^2*bts[4]+ 
    d2[i,5]*bts[5]+ 
    d2[i,6]*bts[6]+
    d2[i,7]*bts[7]+ 
    d2[i,8]*bts[8]+ 
    d2[i,9]*bts[9]+ 
    d2[i,10]*bts[10]+ 
    d2[i,11]*bts[11]+ 
    d2[i,12]*bts[12] +
    e[i]
}

d2$y <- ys

```
```{r comparisons, echo=FALSE, cache=TRUE, message=FALSE, error=FALSE, warnings=FALSE}
set.seed(2)
rf1 <- rforest(d1$y[1:200],d1[1:200, 1:12],mtry = 7,ntree = 300)
set.seed(2)
condinf1 <- conditional.inf(rf1, d1$y[1:200],d1[1:200, 1:12])
set.seed(2)
perminf <- permuted.inf(rf1, d1$y[1:200],d1[1:200, 1:12])
```

```{r weremelting, echo=FALSE, message=FALSE, warning=FALSE}
c1 <- melt(as.data.frame(condinf1[,1:6], names = names(d2[,-13])))
p1 <- melt(as.data.frame(perminf[,1:6], names = names(d2[,-13])))
```


#Random Forest Variable Importance

To implement the various variable importance measures discussed in this chapter and in chapter 4, functions for creating trees, random forests, and their importance measures were created. The trees were fit using the standard two-part CART-like algorithm. The function chooses a variable to split on with linear correlation with respect to $Y$, but instead of looking for correlations above a certain threshold which is common, it chooses the variable with the highest correlation when compared to its peers. This alleviates the situation where a variable with a non-linear relationship would be passed over again and again. The splitting is then done via minimization of the following function with respect to $i$:

$$RSS_{node} (i,X,Y) = RSS_{leaf}(Y|X <i) + RSS_{leaf}(Y|X \geq i) $$
$$RSS_{leaf} = \sum_{y \in Y} (y - \hat{y})^2 $$


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Note that $\hat{y}$ is the mean value of $Y$. This function considers the regression case only, and only numeric predictors. Leaves are created when the resultant split would be unsatisfactory, i.e. at least one of these cases applies: one daughter node would have five members or less, the split on the chosen variable would not result in a decrease in RSS, or the response contained in the node, or the predictor considered for the split, are already homogeneous. This generates very large trees: a quality that is not an issue in random forests but may be problematic in a single-tree setting. 


```{r, echo=FALSE, warning=FALSE, message=FALSE}
set.seed(1)
t1 <- tree.rf(d2$y[1:140],d2[1:140, 1:4],mtry = 4, bootsampled = FALSE)
t <- t1[[2]]
t$var[t$var == "<leaf>"] = "leaf"

t$dev <- round(as.numeric(t$dev),2)
t$ypred <- round(as.numeric(t$ypred),2)
t$split.cutleft <- round(as.numeric(t$split.cutleft),2)

```


```{r, fig.pos= 'H', echo = FALSE, message = FALSE}
kable(t, caption="\\label{tab:tabhomegrowntree}t, a home-grown tree grown on the first four columns and the first 140 rows of D2")
```

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;There are several ways to display a tree, but when it is displayed as in table \ref{tab:tabhomegrowntree} it is read in the following way: each row corresponds to a node of the tree which contains a certain number, `n` observations. This number of observations, or rows in the data set is naturally a subset of both the original data set and the subsets above the node on the tree. Here our predictions, `ypred`, are the mean of the $Y$ values included in the node. If there is an optimal and allowable split, [^7] then the chosen variable, `var`, and the $RSS_{node}$, `dev`, are recorded.[^8] The value of the variable in question that acts as the split point is recorded as `split.cutleft`. If there is no split on the node in question, then `var` will be recorded as `<leaf>` and the `dev` value will be the value of $RSS_{leaf}$ at this node. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The tree output is read roughly from top to bottom, with a coda in the middle. The first row corresponds to the first node, or the node that includes the entire data set. The second row is the beginning of the right subtree or the right daughter of the first node. This pattern continues, favoring the right daughter, until a leaf is reached. The left daughter of the first node is found after all of the splits off of the right daughter have finished but is easily identified as the row with a value of `n` that is exactly the difference between the `n` values of the first two rows. In the case where the right daughter contained many more observations of the original data set, there may be a node within the right subtree that contains the same number of observations as the left daughter of the first node.[^9] In this case, the left daughter is simply the second row with this property. The pattern of following the right daughter until a leaf is reached continues with the left subtree. 

[^7]: Recall that we only allow splits to take place that split the data into two groups, each with more than five members. 

[^8]: It's the convention to call the $RSS_{node}$ the deviance at a node $N$, but, of course, this only makes sense when the node is a leaf. 

[^9]: This occurs when the variable in question has a group of values that are, 1) larger in magnitude than the mean, and 2) are seperated from the general trend. Essentially this occurs when we have unscaled data with outliers. 

##Breiman et al. Introduce Permuted Variable Importance (1984)

###Variable Importance on a Single Tree

Breiman et al. in *Classification and Regression Trees* [-@bibCART] propose a method for variable importance for individual trees that stems from their definition of $\tilde{s}$, a surrogate split. Surrogate splits help Breiman et al. deal with several common problems: missing data, masking, and variable importance. They are defined using logic that resembles that behind random forests. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Before we discuss surrogate splits, let's cover an obvious definition of variable importance for a single tree. In the tree represented by table \ref{tab:tabhomegrowntree}, define variable importance as the number of splits on each variable. This would allow us to answer the question: how useful (important) was variable $X_i$ in constructing our model for $Y$? Just by counting the splits on that variable, we would arrive at the following ranking: 

```{r, echo = FALSE}
v.des <- rep(0,4)

for(i in 1:4) {
  v.des[i] <- sum(names(d2)[i] == t$var)
}

v.des <- data.frame("variable" = names(d2)[1:4], "appearances in tree"=v.des)
library(dplyr)
v.des <- arrange(v.des, desc(appearances.in.tree))
```

```{r, echo=FALSE, message=FALSE}
kable(v.des, caption="\\label{tab:tabbvi}The number of splits on each variable in the tree T.")
```

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;There are several downfalls to this method. One, trees are variable. If we were to resample this data and fit another tree, it's likely that this ranking would change. Two, in the case where two variables are close enough to each other that they could act as stand-ins for one another, these rankings are much less interesting. We are lucky in this case to know without doubt that $X_1$ has a rich relationship with $X_2$ and the other predictors included in this model (see chapter 2, section 1). This leads us to believe that while $X_1$ is left out of these rankings, it just as easily could have been included instead of $X_2$, or one of the other predictors. $X_1$ had bad luck by not being in this model and it wouldn't make sense to say that the $X_1$ is the least important predictor of $Y$ when it is very nearly identical to $X_2$. However, it's possible that the tree algorithm would only pick one of the correlated predictors to be included in a model at a time. Is it possible that we can grasp this relationship by only fitting one tree?

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This dilemma is solved by surrogate splits. To set the stage for surrogate splitting, imagine a CART tree, $t$, fit on some data set $D$ according to the formula $Y \sim X$ where $X_i \in X, \\ if \\ i \in 1:p$. Now say that we're only considering a single node, $N$, in $t$. The node $N$ contains the subset of the rows in the original data set $D$, $D_N$. $D_N$ is determined by the previous nodes and splits in the tree. 
 
\begin{center}
\begin{tikzpicture}[level/.style={sibling distance=60mm/#1}]
\node [circle,draw] (z){$N:\left\{ D_N \right\}$};
\end{tikzpicture}
\end{center}

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;On that node, we have the split on $X_j$ where $X_j < a$. This gives us two daughter nodes to $N$, $N_L$ and $N_R$. 


\usetikzlibrary{positioning}
\begin{center}
\begin{tikzpicture}[level/.style={sibling distance=60mm/#1}]
\node [circle,draw] (z) {$N:\left\{ D_N, \left\{X_j < a \right\}\right\}$};
\node [circle,draw] (a) [below left = of z] {$N_L:\left\{ D_{N_L} \right\}$};
\node [circle,draw] (b) [below right = of z] {$N_R:\left\{ D_{N_R} \right\}$};
\end{tikzpicture}
\end{center}
    
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The data sets $D_{N_L}$ and $D_{N_R}$ are subsets of $D_N$ and when combined, they equal $D_N$. They are determined by the rule: if a row of observations has a value of $X_j < a$ then it is a member of $D_{N_L}$, if the value of $X_j$ in that row is greater than or equal to $a$ then it belongs to $D_{N_R}$. $X_j$ was chosen to split on in node $N$ because the correlation between the subsets of $X_j$ and $Y$ in $D_N$ was stronger than the correlations between $Y$ and any of the other predictors in that subset of the original data. Imagine, however, that split on $X_i$ would lead to very similar[^10] left and right daughter nodes to the daughter nodes generated by the split on $X_j$. This occurs even though $X_i$ and $Y$ had a lower correlation than $Y$ and $X_j$. This would be considered a surrogate split for our original split on $N$. Now define variable importance for a predictor $X_j$ across the tree $t$ as the decrease in $RSS_{node}$ according to the split on $X_j$, whether *surrogate* or not. This allows $X_j$ and $X_i$ to share the importance measure, if both $X_j$ and $X_i$ would have provided a similar, valuable split on node $N$. In *Classification and Regression Trees*, Breiman et al, outline several potential problems with this method that they do not attempt to solve. First, that this is only one of a number of reasonable ways to define variable importance. Second, the variable importances for variables $X_1,..,X_p$ can be affected by outliers or random fluctuations within the data. (Ch 5.3). The second problem is mitigated when we move from single trees to a random forest, but the first is a problem with variable importance in general. 

[^11]: This is intentionally vague. The level of similarity considered "similar enough" depends on the properties of the data set and there's no guarantee that suitable surrogate splits exist. [@bibCART]


###Variable Importance for a Random Forest

One way to define variable importance for a random forest follows directly from Breiman et al's definition for a single tree. Recall that each tree in a random forest is fit to a bootstrapped sample of the original observations. To estimate the test error, therefore no cross validation is needed - each tree is simply tested against the test set of observations that were not in that tree's training, or in bag, set. Additionally, we may be interested in defining variable importance for a predictor $X_j$ by considering the predictive capabilities of the other $p-1$ predictors. Recall: a random forest is a set of trees that are de-correlated with each other because at each split on each tree, less than half of the predictors are not even considered as possible candidates for splitting. To estimate the importance of $X_j$ given the other variables $X_{-j}$ and their relationship with $Y$, we can consider the "test" RSS of the set of trees that did not ever split on $X_j$. These values are then averaged over the subset forest that did not include $X_j$. A large value would imply that in trees that included $X_j$, the predictive capabilities were increased.  

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;To formalize that idea, let's refer to the set of trees that did not consider $X_j$, $t_{-X_j}^c$. Now, $t_{X_j}^c \subset R$, the random forest. The subset of the original data that will be tested on each tree, $t$, is $\bar{B}_t$. The dimensions of $\bar{B}_t$ are $\nu_t$ x $p$, where $p$ is the number of predictors and $\nu_t \leq n$. The number of trees in $t_{x_j}^c$ is $\mu$ where $\mu \leq ntree$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Now, base variable importance is:

$$VI_{\alpha}(X_j, R) =  \sum_{t \in t_{x_j}^c} \frac 1 {\nu_t} RSS(t,\bar{B}_t)$$


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;However, this method poses some problems. Namely, while variable importance for random forests is more stable than for the variable importance values for CART (this is because the model is less variable in general), it is lacking the traditional inferential capabilities of other regression models. In an effort to derive a p-value for variable importance values, Breiman (2001b) describes a *permuted variable importance* or $VI_{\beta}$ that does not utilize $t_{x_j}^c$.

\begin{algorithm}
\caption{Permuted Variable Importance for Random Forests, $VI_{\beta}$}
\label{breiman}
\begin{algorithmic}
\State Fit a random forest, $R$ on the data set $D$ estimating the model $Y \sim X_1,...,X_p$.
\For{each $X_j \in {{X_1,...,X_p}}$}
\For{each $t \in R$}
\State Calculate: $\Phi_o =  \frac 1 {\nu_t} RSS(t,\bar{B}_t)$
\State Permute $X_j$. Now find $\Phi^* =  \frac 1 {\nu_t} RSS(t,\bar{B}_t^*)$
\State The difference between these values, $\Phi^* - \Phi_o$,  is the variable importance for $X_j$ on $t$,  
\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In other words, we start with one tree in the random forest, $t_u$, and one variable, $X_j$, where $1 \leq u \leq ntree$ and $1 \leq j \leq p$. There are two subsets of the original data associated with $t_u$, one is the subset used to generate the tree ${B}^t$ and the other is the rest of the original data set, notated as $\bar{B}_t$. We calculate the residual sum of squares for $t_u$ on the new (to $t_u$) data, $\bar{B}_t$. Then we alter $\bar{B}_t$ by randomly shuffling $X_j$. This means that in one row of $\bar{B}_t$, the values are the same as they were before, except the values for $X_j$ may be interchanged with the values in other rows. Then RSS is calculated again and compared with the RSS before the shuffling took place. As each tree in the random forest is fit to a bootstrapped sample of the original data set and splits on a fraction of the possible predictors, the tree-wise computation gives an estimate of the distribution of $VI_{\beta}(X_j)$.  

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;To visualize the output, the permuted variable importance distribution, of this algorithm on a random forest we will fit a random forest to the data set $D_1$ from the chapter 2. When fitting a random forest, one considers the formula, the data, the number of trees to fit, ($ntree$), and the number of variables to consider at each split, ($mtry$). We have fit the random forest for the formula $Y \ sim V$ on the data set $D_1$, with $mtry = 7$ and $ntree = 300$. The distributions of permuted variable importance in figure \ref{fig:figpermDist} are for the first six variables in $D_1$. Recall that these were the only variables used to create $Y$. The permuted variable importance values for variables $V_7,V_8,V_{10},V_{11},V_{12}$ are zero for each of the 300 trees. 

```{r, echo = FALSE, fig.pos = 'H', fig.cap="\\label{fig:figpermDist}The distribution of permuted variable importance for the first six variables in D1."}
ggplot(data = p1, aes(fill = variable, group = variable, x = value)) +
  stat_density(alpha = .9)+
  ylab("")+
  xlab("")+
  labs(fill="")+
  scale_fill_manual(values=wes_palette(n=6, name="BottleRocket"))
```

##Strobl et al Respond (2008)

Strobl et al respond to Breiman's method with one main argument: the null hypothesis implied by the permutation distribution utilized in permuted variable importance is that $X_i$ is independent of $Y$ **and** the rest of the predictors so the null hypothesis will be rejected in the case where $X_j$ is independent of $Y$ but not some subset of the other predictors. As correlation among the predictors is very common in data sets that are used for random forests, this is a large problem for Breiman's method. To alleviate this difficulty, Strobl et al propose a permutation scheme under the null hypothesis that $X_j$, given its relationship with the other predictors, is independent of $Y$.  

\begin{algorithm}
\caption{Conditional Variable Importance for Random Forests, $VI_{\gamma}$}
\label{strobl}
\begin{algorithmic}[1]
\State Fit a random forest, $R$ on the data set $D$ fitting the model $Y \sim X_1,...,X_p$.
\For{each $t \in R$}
\State Calculate: $\Psi_o =  \frac 1 {\nu_t} RSS(t,\bar{B}_t)$
\For{each $X_i \in {{X_1,...,X_p}}$}
\State Select $Z \subset \left \{ X_1,...,X_{i-1}, X_{i+1},...,X_p \right \}$ to condition on when permuting $X_j$
\State Use the cutpoints on each variable in $Z$ to create a grid on $X_j$
\State Permute $X_j$ with respect to this grid
\State Now find $\Psi^* =  \frac 1 {\nu_t} RSS(t,\bar{B}_t^*)$
\State The difference between these values, $\Psi^* - \Psi_o$,  is the variable importance for $X_j$ on $t$,  
\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This method is fairly similar to permuted variable importance, but there are a few key differences. Given a tree $t_u$ and a variable $X_j$, first we find the out of bag RSS, then we permute. In this case, however, our permutations or shuffling of $X_j$ is not always done blindly. If $X_j$ has no (or low) empirical correlation with each of its fellow predictors, then $X_j$ is shuffled exactly as in permuted variable importance. If that is not the case, then we select the set, $Z$, of the predictors with the strongest empirical correlation [^12] to $X_j$. Recall that our tree $t_u$ contains many nodes, and each node contains a subset of the data along with a split that determines the subsets of the daughter nodes. We feed the out of bag sample for $t_u$ into $t_u$ and look at all the subsets of data in nodes that split on a predictor in $Z$. This time when we shuffle $X_j$ it will only be in these subsets. The union of these subsets is called $\bar{B}_t^*$ and is used to calculate the permuted RSS. 

[^12]: The authors behind @bibstrobl2008 recommend constructing the set $Z$ from prior information about the data or as the set of predictors where each one has empirical correlation greater than or equal to .2 with $X_j$. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;To visualize the output from algorithm \ref{alg:strobl}, conditional permuted variable importance was calculated for the same random forest from \ref{fig:figpermDist}. The distribution of conditional permuted variable importance on the random forest fit on $D_1$, is represented in \ref{fig:figconDist}. 

```{r,fig.pos = 'H', echo=FALSE, fig.cap="\\label{fig:figconDist}The distribution of conditional permuted variable importance for the first six variables in D1."}
ggplot(data = c1, aes(fill = variable, group = variable, x = value)) +
  stat_density(alpha = .9)+
  ylab("")+
  xlab("")+
  labs(fill="")+
  scale_fill_manual(values=wes_palette(n=6, name="BottleRocket"))
```



