---
header-includes:
- \usepackage{graphicx,latexsym}
- \usepackage{amssymb,amsthm,amsmath}
- \usepackage{longtable,booktabs,setspace}
---

<!--
You can delete the header-includes (lines 3-6 above) if you like and also the chunk below since it is loaded in the skeleton.Rmd file.  They are included so that chap3.Rmd will compile by itself when you hit Knit PDF.
-->

```{r include_packages_2, include = FALSE}
# This chunk ensures that the reedtemplates package is
# installed and loaded. This reedtemplates package includes
# the template files for the thesis and also two functions
# used for labeling and referencing
if(!require(devtools))
  install.packages("devtools", repos = "http://cran.rstudio.com")
if(!require(dplyr))
    install.packages("dplyr", repos = "http://cran.rstudio.com")
if(!require(ggplot2))
    install.packages("ggplot2", repos = "http://cran.rstudio.com")
if(!require(reedtemplates)){
  library(devtools)
  devtools::install_github("ismayc/reedtemplates")
  }
library(reedtemplates)
library(tree)
library(reedtemplates)
library(MASS)
library(ggplot2)
library(randomForest)
library(maptree)
library(knitr)
library(GGally)
library(reshape)
#flights <- read.csv("data/flights.csv")
thesis <- c("#245e67", "#90bd98", "#cfd0a0", "#c49e46", "#d28383")
```

```{r, echo = FALSE}
w1 <- rnorm(1000)

w2 <- 2*log(abs(w1)) + rnorm(1000)

w3 <- 2*log(abs(w2)) + rnorm(1000)

w4 <- 2*log(abs(w3)) + rnorm(1000)

n <- 1000

rep(0, 8) -> mu #mean of each variable

diag(8) -> sigma #creates a 12 by 12 diagonal matrix with 1's down the diagonal

mvrnorm(n =n, mu = mu, Sigma = sigma) -> simw5w12

data.frame(w1,w2,w3,w4,simw5w12) -> d3
              
c("W1","W2","W3","W4","W5","W6", "W7","W8","W9","W10","W11","W12") -> names(d3)

rep(0, 1000) -> ys #init a vector of zeros
c(5,5,2,0,-5,-5,-2,0,0,0,0,0) -> bts 

rnorm(1000, mean = 0, sd = .5) -> e 

for( i in 1:1000){ #go row by row and create the ys based on the function of e, bts, and d3
ys[i] <- d3[i,1]*bts[1]+
    d3[i,2]*bts[2]+ 
    d3[i,3]*bts[3]+ 
    d3[i,4]*bts[4]+ 
    d3[i,5]*bts[5]+ 
    d3[i,6]*bts[6]+
    d3[i,7]*bts[7]+ 
    d3[i,8]*bts[8]+ 
    d3[i,9]*bts[9]+ 
    d3[i,10]*bts[10]+ 
    d3[i,11]*bts[11]+ 
    d3[i,12]*bts[12] +
    e[i]
}

d3$y <- ys
```

###TO DO

- Fix the algorithms to work for regression
- Make the OOB references more clear
- Expand around viz created today 3/8

#Random Forest Variable Importance

##Breiman et al Introduce Permuted Variable Importance (1984)

###Variable Importance on a Single Tree

Breiman et al in *Classification and Regression Trees* (1984) propose a method for variable importance for individual trees that stems from their definition of $\tilde{s}$, a surrogate split. Surrogate splits help Brieman et al deal with several common problems one may have: modeling with missing data, diagnosing masking, and variable importance. They are defined using logic that resembles that behind random forests. 

Definitions

Assume the standard structure for tree models. Let $D$ be the dataset composed of $D = {Y, X_1,...X_p}$, where the model we would like to estimate is of the form $T: Y \sim X_1,...X_p$. For any node $t \in T(D)$, $s*$ is the best split of the node into daughters $t_r$ and $t_l$. Take $X_i \in D$ and let $S_i$ be the set of all of the splits on $X_i$ in $T$. Then set $\bar{S_i}$ equal to the complement of $S_i$, $\bar{S_i} = S_i^c$. For any possible split $s_i \in S_i \cup \bar{S_i}$, $s_i$ will split the node $t$ into two daughters, $t_{i,l}$ and $t_{i,r}$. Count the number of times that $s*$ and $s_i$, while splitting differently, generate the same left daughter $t_{l}$ as $N_{LL}$ and the number of times they generate the same right daughter as $N_{RR}$. Then the probability that a case falls within $t_L \cap t'_L$ is $P(t_L \cap t'_L) = \sum_j \frac{\pi(j) N_j(LL)}{N_j}$ and the probability that a case falls within $t_R \cap t'_R$ is $P(t_R \cap t'_R) = \sum_j \frac{\pi(j) N_j(RR)}{N_j}$. Where $\pi(j)$ is the prior assumed for the the jth variable.Finally, the probability that a surrogate split predicts $s*$ is $P(s*, s_M) = (t_R \cap t'_R) + P(t_L \cap t'_L)$. Then the surrogate split is the value of $s*$ that maximizes this probability. It is denoted $\tilde{s}$

A surrogate split $\tilde{s}$,is one that estimates the best possible univariate split $s*$ on node $t$.

**Defintion: Variable Importance, Single Tree**


$$VI_{tree}(X_i, T) = \sum_{t \in T} \Delta RSS(\tilde{s_i}, t)$$
Or the decrease of RSS attributable to $X_i$ across the tree $T$. In *Classification and Regression Trees*,
Brieman et al, outline several potential problems with this method that the do not attempt to solve. First, that this is only one of a number of reasonable ways to define variable importance. Second, the variable importances for variables $X_1,..,X_p$ can be effected by outliers or random fluctuations within the data. (Ch 5.3)

###Variable Importance for a Random Forest

One way to define variable importance for a random forest follows directly from Breiman et al's definition for a single tree. Recall that each tree in a random forest is fit to a bootstrapped sample of the original observations. To estimate the test error, therefor, no cross validation is needed - each tree is simply tested against the test set of observations that were not in that tree's initial training set. To determine variable importance for a predictor $X_j$, we look at the RSS of the each tree's prediction that did not split on $X_j$. These values are then averaged over the subset forest that did not include $X_j$. A large value would imply that in trees that included $X_j$, the predictive capabilities were increased.  

To formalize that idea, let's refer to the set of trees that did not consider $X_j$, $T_{x_j}^c$. Now, $T_{x_j}^c \subset R$, the random forest. The subset of the original data that will be tested on each tree, $t$, is $\bar{B}^t$. The dimensions of $\bar{B}^t$ are $\nu_t$ x $p$, where $p$ is the number of predictors and $\nu \leq n$. The number of trees in $T_{x_j}^c$ is $\mu$ where $\mu \leq ntree$

Now, base variable importance is:

$$VI_{\alpha}(X_j, R) =  \sum_{t \in T_{x_j}^c} \frac 1 {\nu_t} RSS(t,\bar{B}_t)$$


However, this method poses some problems. Namely, while variable importance for random forests is more stable than for the variable importance values for CART, (this is because the model is less variable in general), it is lacking the traditional inferential capabilities of other regression models. In an effort to derive a p-value for variable importance values, Breiman 2001b, describes a *permuted variable importance* or $VI_{\beta}$ that does not utilize $T_{x_j}^c$.

\begin{algorithm}
\caption{Permuted Variable Importance for Random Forests, $VI_{\beta}$}
\label{breiman}
\begin{algorithmic}[1]
\State Fit a random forest, $R$ on the dataset $D$ fitting the model $Y \sim X_1,...,X_p$.
\For{each $X_i \in {{X_1,...,X_p}}$}
\For{each $t \in R$}
\State Calculate: $\Phi_o =  \frac 1 {\nu_t} RSS(t,\bar{B}^t)$
\State Permute $X_i$. Now find $\Phi^* =  \frac 1 {\nu_t} RSS(t,\bar{B}_t^*)$
\State The difference between these values, $\Phi^* - \Phi_o$,  is the variable importance for $X_j$ on $t$,  
\EndFor
\State Average over all $t \in R$ 
 $$VI_{\beta}(X_j) = \frac 1 {ntree} \sum^{ntree} \Phi^* - \Phi_o$$
 $$VI_{\beta}(X_j) = \frac 1 {ntree} \sum^{ntree} \frac 1 {\nu_t} RSS(t,\bar{B}_t^*) - \frac 1 {\nu_t} RSS(t,\bar{B}^t)$$
\EndFor
\end{algorithmic}
\end{algorithm}


 

##Strobl et al Respond (2008)

Strobl et al (2008) respond to Brieman's method with one main argument: the null hypothesis implied by the permutation distribution utilized in permuted variable importance is that $X_i$ is independent of $Y$ **and** ${X_j \notin X_1,...,X_p}$ so the null hypothesis will be rejected in the case where $X_j$ is independent of $Y$ but not some subset of the other predictors. As correlation among the predictors is very common in data sets that are used for random forests, this is a large problem for Breiman's method. 

To alleiviate this difficulty, Strobl et al propose a permutation scheme under the null hypothesis that $X_j$ given it's relationship with the other predictors is independent of $Y$.  

\begin{algorithm}
\caption{Conditional Variable Importance for Random Forests}
\label{strobl}
\begin{algorithmic}[1]
\State Fit a random forest, $RF$ on the dataset $D$ fitting the model $Y \sim X_1,...,X_p$
\For{each $T_i \in RF$}
\State Calculate the oob error before permutation: $$\frac{\sum_{i \in \bar{B}^{T}} I(y = \hat{y})} {|\bar{B}^{T}|}$$
\For{each $X_i \in {{X_1,...,X_p}}$}
\State Permute the dataset, $D$, conditional on the grid created by the splits on $X_i$ in $T_i$
\State Calculate the oob error again and compare with the estimate before permutation $$VI_{perm2}(X_i, T_i) = \frac{\sum_{i \in \bar{B}^{T}} I(y = \hat{y})} {|\bar{B}^{T}|} -\frac{\sum_{i \in \bar{B}^{T}} I(y = \hat{y^p})} {|\bar{B}^{T}|}$$
\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

Finally, as above, the permuted variable importance for each variable $X_i \in {{X_1,..,X_p}}$ is the sum of $VI_{perm2}(X_i, T_i)$ over all $T_i \in RF$.


AN EXAMPLE:

Fit a simple tree:

```{r, echo = FALSE, warning=FALSE, message=FALSE}

s <- sample(1000, 200)

  
d3lite <- d3[s,c(1,2,13)]


t <- tree(y ~., data = d3lite)
t <- prune.tree(t, best = 5)

plot(t)
text(t)

yhats <- predict(t, d3lite[, -13])
d3lite$group <- as.factor(yhats)
d3lite$group <- ifelse(d3lite$group == levels(d3lite$group)[1], "A",
                ifelse(d3lite$group == levels(d3lite$group)[2], "B",
                ifelse(d3lite$group == levels(d3lite$group)[3], "C",
                ifelse(d3lite$group == levels(d3lite$group)[4], "D", "E"))))
d3lite$ys <- yhats

library(ggplot2)
library(plotly)

ggplot(d3lite, aes(x=W1, y=W2, color = group)) +
  geom_point()+
  ggtitle("Partitions on the Predictor Space W1,W2 from T1")+
  scale_color_manual(values = thesis)

group <- levels(as.factor(d3lite$group))
predictions <- levels(as.factor(yhats))
rangeW1 <- range(d3lite$W1[d3lite$group == "A"])
rangeW1 <- rbind(rangeW1, range(d3lite$W1[d3lite$group == "B"]))
rangeW1 <- rbind(rangeW1, range(d3lite$W1[d3lite$group == "C"]))
rangeW1 <- rbind(rangeW1, range(d3lite$W1[d3lite$group == "D"]))
rangeW1 <- rbind(rangeW1, range(d3lite$W1[d3lite$group == "E"]))

rangeW2 <- range(d3lite$W2[d3lite$group == "A"])
rangeW2 <- rbind(rangeW2, range(d3lite$W2[d3lite$group == "B"]))
rangeW2 <- rbind(rangeW2, range(d3lite$W2[d3lite$group == "C"]))
rangeW2 <- rbind(rangeW2, range(d3lite$W2[d3lite$group == "D"]))
rangeW2 <- rbind(rangeW2, range(d3lite$W2[d3lite$group == "E"]))

t1 <- cbind("Group"= group,"Predicted Value of Y" = predictions,
            "Min(W2)" = round(rangeW2[,1],2), "Max(W2)" = round(rangeW2[,2],2), 
            "Min(W1)" = round(rangeW1[,1],2),"Max(W1)" = round(rangeW1[,2],2))

row.names(t1) <- c()
kable(t1)
```


If we permute the $\omega_1$ values in group $B$, this is what that plot looks like:

```{r,echo = FALSE, warning=FALSE, message=FALSE}
dp <- d3lite

bs <- filter(dp, group == "B")
bs$W1 <- sample(bs$W1)

dp[dp$group == "B",] <- bs
ggplot(dp, aes(x=W1, y=W2, color = group)) +
  geom_point()+
  ggtitle("Partitions on the Predictor Space W1,W2 from T1")+
  scale_color_manual(values = thesis)

```


##Inferential Variable Importance

This thesis hopes to be a reponse to conditional variable importance as outlined by Strobl et al 2008. First is that the practice of permuting given the partitions from the model $Y \sim X_1,...,X_p$ instead of $X_j \sim X_1,..,X_p$ 


