---
header-includes:
- \usepackage{amssymb,amsthm,amsmath}
- \usepackage{chemarr}
- \usepackage{algorithm}
- \usepackage{algpseudocode}
- \usepackage{pifont}
- \usepackage{float}
output: pdf_document
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = "H")
```
<!--
You can delete the header-includes (lines 3-5 above) if you like and also the chunk below since it is loaded in the skeleton.Rmd file.  They are included so that chap2.Rmd will compile by itself when you hit Knit PDF.
-->

```{r include_reedtemplates_2, include = FALSE}
# This chunk ensures that the reedtemplates package is installed and loaded
# This reedtemplates package includes the template files for the thesis and also
# two functions used for labeling and referencing
if(!require(devtools))
  install.packages("devtools", repos = "http://cran.rstudio.com")

if(!require(reedtemplates)){
  library(devtools)
  devtools::install_github("ismayc/reedtemplates")
  }
library(reedtemplates)
library(MASS)
library(ggplot2)
library(randomForest)
library(maptree)
library(knitr)
library(GGally)
library(reshape)
library(gridExtra)

thesis <- c("#245e67", "#90bd98", "#cfd0a0", "#c49e46", "#d28383")
```

#Simulations and Comparisons

Our goal for this chapter is to compare trees, random forests, and linear models. In this chapter, we will use simulated data instead of the orange data set. One reason for this is theoretical consistency. One hopes that one's results will not be rendered null and void by any misstep in the data collection that comes to light. This also ensures that these simulations can be repeated by later researchers, but, granted, it does not make for the most exciting analysis. For now, consider $Y$ to be our response variable. In the first simulation, $V$ will be the set of our predictors, and $V_j$ to be a predictor in $V$. The formula will be the same for each model: $Y \sim V$. In our second simulation, our set of predictors will be denoted as $X$ and $X_j$ will be a member of $X$. The formula in this case is $Y \sim X$. A single row in $D_1$ will be denoted as $v$, and a single row in $D_2$ as $x$. 

##Simulated Data

Random forests excel in predicting outcomes with correlated predictors, although these situations can make it difficult to perform intelligible inference. In a situation in which the correlated predictors are $X_1$ and $X_2$ and the formula we're estimating is $Y \sim X_1 + X_2$, it can be difficult to say if $X_1$ or $X_2$ is truly the better predictor. To illustrate this idea, compare a few existing methods, and explore methods of inference on tree based models, we will simulate two data sets with different correlation structures. We will focus more on the correlation structure between the predictors than on their relationships with the response and this will be reflected in the simulations.  

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The first simulated dataset is generated under the same scheme as in [@bibstrobl2008]. Under this method, the 13 x 1000 data set, $D_1$, has 12 predictors, $V_1,..,V_{12}$, where $V_j \sim N(0,1)$. The first four are block correlated to each other with $\rho = .9$. They are related to $Y$ by the linear equation: $$Y = 5 \cdot V_1 + 5 \cdot V_2 + 2 \cdot V_3 + 0 \cdot V_4 + -5 \cdot V_5 + -5\cdot V_6 + 0\cdot V_7 + 0 \cdot ..... + E, E \sim N(0,\frac 1 2 )$$ Note in table \ref{tab:tabcorSim1}, the coefficients for $V_7,...,V_{12}$ are all zero. 

```{r stroblSim, warning=FALSE, message=FALSE, echo=FALSE}
rep(0, 12) -> mu #mean of each variable

diag(12) -> sigma #creates a 12 by 12 diagonal matrix with 1's down the diagonal

sigma -> sigma1
.9 -> sigma1[1,2]
.9 -> sigma1[2,1] #adding the block correlation between the first four variables
.9 -> sigma1[1,3]
.9 -> sigma1[3,1]
.9 -> sigma1[3,2]
.9 -> sigma1[2,3]
.9 -> sigma1[1,4]
.9 -> sigma1[4,1]
.9 -> sigma1[2,4]
.9 -> sigma1[4,2]
.9 -> sigma1[4,3]
.9 -> sigma1[3,4]

1000 -> n #number of observations
set.seed(1)
mvrnorm(n =n, mu = mu, Sigma = sigma1) -> sim1000 #sampling from the multivariate normal 
c(5,5,2,0,-5,-5,0,0,0,0,0,0) -> bts #these are the betas 

rnorm(1000, mean = 0, sd = .5) -> e #error terms

rep(0, 1000) -> ys #init a vector of zeros

for( i in 1:1000){ #go row by row and create the ys based on the function of e, bts, and sim1000
ys[i] <- sim1000[i,1]*bts[1]+
    sim1000[i,2]*bts[2]+ 
    sim1000[i,3]*bts[3]+ 
    sim1000[i,4]*bts[4]+ 
    sim1000[i,5]*bts[5]+ 
    sim1000[i,6]*bts[6]+
    sim1000[i,7]*bts[7]+ 
    sim1000[i,8]*bts[8]+ 
    sim1000[i,9]*bts[9]+ 
    sim1000[i,10]*bts[10]+ 
    sim1000[i,11]*bts[11]+ 
    sim1000[i,12]*bts[12] +
    e[i]
}

d1 <- as.data.frame(sim1000)
d1$y <- ys

cord1 <- round(cor(d1)[c(1:7),c(1:7,13)], digits = 3)
cord1 <- cbind(cord1, "beta"= bts[1:7])
```

```{r, fig.pos = 'H', echo=FALSE, message = FALSE}
kable(cord1, caption="\\label{tab:tabcorSim1}Empirical correlations and coefficients of the variables in the first simulated data set")
```

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In the last column of table \ref{tab:tabcorSim1}, the coefficient "beta", refers to the function used to generate the $Y$ values. Although $V4$ was not included in the model $Y \sim V1,..V_{12}$, its strong correlation with more influential predictors $V_1,...,V_3$ ensures that it still shows a strong, empirical linear correlation with $Y$. A linear model would likely *overstate* the effect of $V_4$ on $Y$. [^4] [^5]

```{r, warning=FALSE, message=FALSE, echo=FALSE}
t <-  melt(d1[,1:5])
#levels(t$variable) <- rev(levels(t$variable))
a <- ggplot(data = t, aes(x = value, fill = variable)) + 
  geom_density(alpha = .8)+
  scale_fill_manual(values = thesis)+
  xlab(" ")+
  ylab(" ")
b <- ggplot(data = d1, aes(x = V4, y = y)) + 
  geom_point(color = thesis[4], alpha = .8) 
```

```{r figdenv4y, echo = FALSE, fig.height = 2,fig.pos = 'H', fig.cap="Relation between V4 and Y. This relation has empirical linear correlation = .789", results="asis"}
b
```

[^4]: A brief note on uncertainty is needed here. It's true that in this setting we can say that $V_4$ is actually unimportant to understanding $Y$, but in situations with real data this is profoundly more difficult to parse. Often like in the social science situations that Morgan and Sonquist encountered, the real relationship between correlated predictors is complicated and often there is some theoretical backing or other insight that is gained to include variables that may not be important to the model. 

[^5]: Another point that could be said is that, no $V_4$ is not unimportant, $V_1, V_2,$ and $V_3$ are just stand ins for the real star, $V_4$, as they are nearly the same ($\rho \sim 1$). Then the real relationship represented here is $Y \sim (5 + 5 + 2) \cdot V_4 + -5 \cdot V_5 + -5 \cdot V_6 + -2 \cdot V_7$. This model is not unsuccessful in capturing the structure of the data, and this is typically the practice used to model data with highly correlated predictors. If this seems philosophically satisfying to you, the rest of this thesis may seem a bit inconsequential.

```{r , echo = FALSE, fig.pos='H', fig.height=2, fig.cap="\\label{fig:figdenv1v5}Empirical densities for V1 through V4", results="asis"}
a
```

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The densities of $V_1,...,V_4$ in figure \ref{fig:figdenv1v5} are all very similar due to the way they were generated. $D_1$ represents the case where some of the predictors are linearly correlated with each other, but that is not the only possible correlation structure. The data set $D_2$ is simulated similarly to $D_2$ in that $D_2$ contains twelve predictors and one response variable. The first four variables are generated in the following way: 
$$X_1 \sim N(0,1)$$
$$X_2 = log(X_1) + E, E \sim N(0,1)$$
$$X_3 = log(X_2) + E, E \sim N(0,1)$$
$$X_4 = log(X_4) + E, E \sim N(0,1)$$

This simulation scheme leads to the first four variables having an obvious relationship between each other, but relatively low linear correlations, as seen in figure \ref{fig:corstructd2}. Predictors are sampled by $X_5,...,X_{12} \sim N(0,1)$. The $Y$ values are generated according to the following formula:

$$Y = 5 \cdot (X_1)^2 + 5 \cdot(X_2)^2 + 2 \cdot (X_3)^2 + 0 \cdot X_4 + -5 \cdot X_5 + -5\cdot X_6 + 0\cdot X_7 + 0 \cdot ..... + E, E \sim N(0,\frac 1 2 )$$ 

```{r logAbs,echo=FALSE, message=FALSE}
w1 <- rnorm(1000)

w2 <- 2*log(abs(w1)) + rnorm(1000)

w3 <- 2*log(abs(w2)) + rnorm(1000)

w4 <- 2*log(abs(w3)) + rnorm(1000)

sigma <- diag(8)


mvrnorm(n =n, mu = mu[1:8], Sigma = sigma) -> simw5w12

data.frame(w1,w2,w3,w4,simw5w12) -> d2
              
c("X1","X2","X3","X4","X5","X6", "X7","X8","X9","X10","X11","X12") -> names(d2)

rep(0, 1000) -> ys #init a vector of zeros

for( i in 1:1000){ #go row by row and create the ys based on the function of e, bts, and d2
ys[i] <- d2[i,1]^2*bts[1]+
    d2[i,2]^2*bts[2]+ 
    d2[i,3]^2*bts[3]+ 
    d2[i,4]*bts[4]+ 
    d2[i,5]*bts[5]+ 
    d2[i,6]*bts[6]+
    d2[i,7]*bts[7]+ 
    d2[i,8]*bts[8]+ 
    d2[i,9]*bts[9]+ 
    d2[i,10]*bts[10]+ 
    d2[i,11]*bts[11]+ 
    d2[i,12]*bts[12] +
    e[i]
}

d2$y <- ys

cord2 <- round(cor(d2)[c(1:7),c(1:7,13)], digits = 3)
cord2 <- cbind(cord2, "beta"= bts[1:7])


```

```{r , echo = FALSE, fig.cap="\\label{fig:corstructd2}Correlation structure of the first four variables in D2", results="asis"}
ggpairs(
  data =d2,
  columns = c(1:4)
)
```

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The correlation structure in $D_2$ is much more difficult to capture with a single line. The relationships between the first four predictors form striking, symmetrical scatter plots in figure \ref{fig:corstructd2}. This information is considered again in table \ref{tab:tabcorSim2}, where the empirical correlations of the first seven variables are presented along with their observed correlations with $Y$ and their simulation coefficients. 


```{r, echo=FALSE, message=FALSE}
cord2 <- round(cor(d2)[c(1:7),c(1:7,13)], digits = 3)
cord2 <- cbind(cord2, "beta"= bts[1:7])
```

```{r, echo = FALSE, message = FALSE}
kable(cord2, caption="\\label{tab:tabcorSim2}Empirical correlations and coefficients of the first seven predictors and the response using the second simulated dataset")
```

##Models and Comparisons

####CART: Regression Trees

```{r,echo=FALSE, message=FALSE,cache=TRUE}
library(tree)
t2 <- tree(y~., round(d2))
#t1 <- prune.tree(t1, k = 2)
#t2 <- tree(y~., round(d2))
#plot(cv.tree(t2))
#t2 <- prune.tree(t2, k = 4)
#t3 <- tree(y~., round(d1))
#plot(cv.tree(t3))
#t3 <- prune.tree(t3, k = 6)

t2$frame$yval <- round(t2$frame$yval)
#t2$frame$yval <- round(t2$frame$yval)
#t3$frame$yval <- round(t3$frame$yval)
```

```{r , echo = FALSE, fig.pos = 'H', fig.cap="\\label{fig:figcarts}CART representing Y~ X, from D2", results="asis"}
#par(mfrow=c(1,3))

draw.tree(t2, digits = 2, col = c(thesis, rep(thesis, 3)))
#draw.tree(t3, digits = 2, col = c(thesis, rep(thesis, 3)))
```

The CART tree representing the model $Y \sim X$ in figure \ref{fig:figcarts} is easy enough to understand. Starting at the very top of the tree, predictions can be made based on the values of the leaves (or ending nodes) given the requirements of the path to get there. Trees can be quite variable, so to get a better idea of the differences between the methods let's run a simulation. This simulation scheme will take advantage of the non linearity present in $D_2$. 

\begin{algorithm}
\caption{Simulation Scheme 2.1}
\label{sim2.1}
\begin{algorithmic}[1]
\For{$i \leq 1000$ }
\State Randomly sample $\frac 2 3$  of the observations in  $D_2$  to a training set,  $D_{2, train}^i$. The other observations,  $x \in D_2, x \notin D_{2, train}^i$ form the testing set $D_{2, test}^i$
\State Fit a tree, $T^i$, to the data under the model $Y \sim X_1,...,X_2$ using the observations in $D_{2}^i$
\State Calculate the $MSE_{test}$ of the model using the equation:
    $MSE_{test} = \frac 1 n \sum (y_j - \hat{y_j})^2$
\EndFor
\end{algorithmic}
\end{algorithm}


Note that $n$ is the number of observations in  $D_{1, test}^i$, $y_j \in D_{2, test}^i, \hat{y_j} \in  T^i(D_{2, test}^i)$ for $1 \leq j \leq n$ This produces one distribution of $MSE_{test}$ for CART. This simulation scheme will be repeated for the linear model and the random forest and the $MSE_{test}$ distributions are compared in figure \ref{fig:baggedvcartvforest}. Note that the scales on the x-axis are drastically different for each of these models. The $MSE_{test}$ distribution for the random forest has such low variance, and low $MSE_{test}$ it would be difficult to display it on the same plot as the other two. 


```{r fig10,echo=FALSE, error=FALSE, warning=FALSE, message=FALSE,cache=TRUE}
testmseC <- rep(0,1000)
#testmseCt <- rep(0,1000)

for(i in 1:1000){
  train <- sample(1000, 666)
 #tCt <- ctree(y~., d1[train,])
  tC <- tree(y~., d2[train,])
  testmseC[i] <- mean((d2[-train,]$y - predict(tC, d2[-train,]))^2)
#  testmseCt[i] <- mean((d1[-train,]$y - predict(tCt, d1[-train,]))^2)
}

testmse <- (testmseC)

tmse<- melt(as.data.frame(testmse))

a <- ggplot(aes(x = value), data = tmse) + 
  geom_density(alpha = .5, color = thesis[3])+
  scale_fill_manual(values = thesis)+
  guides(fill=guide_legend(title=NULL))+
 # scale_x_continuous(limits = c(0,40))+
  xlab(" ")+
  ylab(" ")
```

```{r fig11, echo=FALSE,error=FALSE, warning=FALSE, message=FALSE,cache=TRUE}
mselm <- rep(0,1000)

for (i in 1:1000){
  train = sample(1000,666)
  b <- lm(y~., data = d2[train,])
  mselm[i] <- mean((d2$y[-train] - predict(b, d2[-train,]))^2)
}

mselm<- melt(as.data.frame(mselm))

b <- ggplot(aes(x = value), data = mselm) + 
  geom_density(alpha = .5, color = thesis[2], alpha = .8)+
  scale_fill_manual(values = thesis)+
  guides(fill=guide_legend(title=NULL))+
 # scale_x_continuous(limits = c(0,40))+
  xlab(" ")+
  ylab(" ")

```

```{r fig12, echo=FALSE,error=FALSE, warning=FALSE, message=FALSE,cache=TRUE}

mseRF <- rep(0,1000)

#start <- Sys.time()
for(i in 1:1000){
  train = sample(1000,666)
  rf <- randomForest(x = d2[,1:12], y= d2[,13], subset = train, ntree = 100)
  mseRF[i] <- mean((d2$y[-train] - predict(rf, d2[-train,]))^2)
}

#
mserf <- data.frame(RandomForest= mseRF)

mserf<- melt(mserf)

c <- ggplot(aes(x = value), data = mserf) + 
  geom_density(alpha = .5, color = thesis[1], alpha = .8)+
  scale_fill_manual(values = thesis)+
  guides(fill=guide_legend(title=NULL))+
 # scale_x_continuous(limits = c(0,40))+
  xlab(" ")+
  ylab(" ")
```


```{r, message=FALSE, warnings=FALSE,echo = FALSE, fig.pos= 'H', fig.height=3 ,fig.cap="\\label{fig:baggedvcartvforest}The simulated MSE distributions of CART, linear model, and the random forest on D2", results="asis"}
grid.arrange(a,b,c, ncol=3)
```


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The linear model is characteristically less flexible and less prone to over-fitting than either of the tree-based methods, CART and random forests, and has a $MSE_{test}$ distribution that is quite peaked. CART is flexible and suffers from high variance. The random forest models perform much better on average than either the CART or the linear model, due to both the non-linear relationships between $Y$ and the predictors and the random forest's ability to decorrelate each of the trees by restricting the variables available on each split. See chapter 3 for more discussion on the enforced heterogeneity of trees in the random forest. As $MSE_{test}$ is a test of *predictive* accuracy it is not surprising that the random forest performed admirably. On a certain level, that is what they are designed to do. The linear model is not popular in predictive situations but it is ubiquitous in inferential ones.


