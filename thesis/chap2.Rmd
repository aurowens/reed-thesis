---
header-includes:
- \usepackage{amssymb,amsthm,amsmath}
- \usepackage{chemarr}
- \usepackage{algorithm}
- \usepackage{algpseudocode}
- \usepackage{pifont}
- \usepackage{float}
output: pdf_document
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = "H")
```
<!--
You can delete the header-includes (lines 3-5 above) if you like and also the chunk below since it is loaded in the skeleton.Rmd file.  They are included so that chap2.Rmd will compile by itself when you hit Knit PDF.
-->

```{r include_reedtemplates_2, include = FALSE}
# This chunk ensures that the reedtemplates package is installed and loaded
# This reedtemplates package includes the template files for the thesis and also
# two functions used for labeling and referencing
if(!require(devtools))
  install.packages("devtools", repos = "http://cran.rstudio.com")

if(!require(reedtemplates)){
  library(devtools)
  devtools::install_github("ismayc/reedtemplates")
  }
library(reedtemplates)
library(MASS)
library(ggplot2)
library(randomForest)
library(maptree)
library(knitr)
library(GGally)
library(reshape)
library(gridExtra)

thesis <- c("#245e67", "#90bd98", "#cfd0a0", "#c49e46", "#d28383")
```



#Simulations and Comparisons

###Simulated Data

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Tree-based methods shine in predictive situations with correlated predictors, although these situations can pose problems for inference. In a situation with correlated predictors $X_1$ and $X_2$, and the tree model we are considering is $Y \sim X_1 + X_2$, it is difficult to say how much of the modeled effect on $Y$ is due to $X_1$ or $X_2$. To illustrate this idea, compare a few existing methods, and explore methods of inference on tree based models two datasets will be simulated with different correlation structures. We will be focused more on the correlation structure between the predictors than on their relationships with the response and this will be reflected in the simulations.  

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;To aid in comparisons between the methods, one of the simulated datasets considered in this paper will be generated from the same method as used in (Strobl et al, 2008b). Under this method, the 13 x 1000 data set, $D_1$, has 12 predictors, $V_1,..,V_{12}$, where $V_j \sim N(0,1)$. The first four are, however, block correlated to each other with $\rho = .9$. They are related to $Y$ by the linear equation: $$Y = 5 \cdot V_1 + 5 \cdot V_2 + 2 \cdot V_3 + 0 \cdot V_4 + -5 \cdot V_5 + -5\cdot V_6 + 0\cdot V_7 + 0 \cdot ..... + E, E \sim N(0,\frac 1 2 )$$ Note that the coefficients for $V_7,...,V_{12}$ are all zero. 

```{r stroblSim, warning=FALSE, message=FALSE, echo=FALSE}
rep(0, 12) -> mu #mean of each variable

diag(12) -> sigma #creates a 12 by 12 diagonal matrix with 1's down the diagonal

sigma -> sigma1
.9 -> sigma1[1,2]
.9 -> sigma1[2,1] #adding the block correlation between the first four variables
.9 -> sigma1[1,3]
.9 -> sigma1[3,1]
.9 -> sigma1[3,2]
.9 -> sigma1[2,3]
.9 -> sigma1[1,4]
.9 -> sigma1[4,1]
.9 -> sigma1[2,4]
.9 -> sigma1[4,2]
.9 -> sigma1[4,3]
.9 -> sigma1[3,4]

1000 -> n #number of observations
set.seed(1)
mvrnorm(n =n, mu = mu, Sigma = sigma1) -> sim1000 #sampling from the multivariate normal 
c(5,5,2,0,-5,-5,-2,0,0,0,0,0) -> bts #these are the betas 

rnorm(1000, mean = 0, sd = .5) -> e #error terms

rep(0, 1000) -> ys #init a vector of zeros

for( i in 1:1000){ #go row by row and create the ys based on the function of e, bts, and sim1000
ys[i] <- sim1000[i,1]*bts[1]+
    sim1000[i,2]*bts[2]+ 
    sim1000[i,3]*bts[3]+ 
    sim1000[i,4]*bts[4]+ 
    sim1000[i,5]*bts[5]+ 
    sim1000[i,6]*bts[6]+
    sim1000[i,7]*bts[7]+ 
    sim1000[i,8]*bts[8]+ 
    sim1000[i,9]*bts[9]+ 
    sim1000[i,10]*bts[10]+ 
    sim1000[i,11]*bts[11]+ 
    sim1000[i,12]*bts[12] +
    e[i]
}

d1 <- as.data.frame(sim1000)
d1$y <- ys

cord1 <- round(cor(d1)[c(1:7),c(1:7,13)], digits = 3)
cord1 <- cbind(cord1, "beta"= bts[1:7])
```

```{r, fig.pos = 'H', echo=FALSE, message = FALSE}
kable(cord1, caption="\\label{tab:tabcorSim1}Empirical correlations and coefficients of the variables in the first simulated data set")
```

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In the last column of table \ref{tab:tabcorSim1}, "beta", although $V4$ was not included in the model $Y \sim V1,..V_{12}$, it has a strong correlation with more influential predictors $V_1,...,V_3$ insures that it still shows a strong, empirical linear correlation with $Y$. A linear model would likely *overstate* the effect of $V_4$ on $Y$. [^3] [^4]

```{r, warning=FALSE, message=FALSE, echo=FALSE}
t <-  melt(d1[,1:5])
#levels(t$variable) <- rev(levels(t$variable))
a <- ggplot(data = t, aes(x = value, fill = variable)) + 
  geom_density(alpha = .8)+
  scale_fill_manual(values = thesis)+
  xlab(" ")+
  ylab(" ")
b <- ggplot(data = d1, aes(x = V4, y = y)) + 
  geom_point(color = thesis[4], alpha = .8) 
```

```{r figdenv4y, echo = FALSE, fig.height = 2,fig.pos = 'H', fig.cap="Relation between V4 and Y. This relation has empirical linear correlation = .789", results="asis"}
b
```



[^3]: A brief note on uncertainty is needed here. It's true that in this setting we can say that $V_4$ is actually unimportant to understanding $Y$, but in situations with real data this is profoundly more difficult to parse. Often like in the social science situations that Morgan and Sonquist encountered, the real relationship between correlated predictors is complicated and often there is some theoretical backing or other insight that is gained to include variables that may not be important to the model. 

[^4]: Another point that could be said is that, no $V_4$ is not unimportant, $V_1, V_2,$ and $V_3$ are just stand ins for the real star, $V_4$, as they are nearly the same ($\rho \sim 1$). Then the real relationship represented here is $Y \sim (5 + 5 + 2) \cdot V_4 + -5 \cdot V_5 + -5 \cdot V_6 + -2 \cdot V_7$. This model is not unsuccessful in capturing the structure of the data, and this is typically the practice used to model data with highly correlated predictors. If this seems philosophically satisfying to you, the rest of this thesis may seem a bit inconsequential.

```{r figdenv1v5, echo = FALSE, fig.pos='H', fig.height=2, fig.cap="Empirical densities for V1 through V4", results="asis"}
a
```

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; As in figure \ref{fig:figdenv1v5}, the densities of $V_1,...,V_4$ are all very similar due to the way they were generated. 

$D_1$ represents the case where some of the predictors are linearly correlated with each other, but that is not the only possible correlation structure. The data set $D_2$ is simulated similarly to $D_2$ in that $D_2$ contains twelve predictors and one response variable. The response, $Y$, is related to the predictors by the same equation as in $D_1$. The first four variables, however, are related to each other in the following way: 
$$\omega_1 \sim N(1,0)$$
$$\omega_2 = log(\omega_1) + E, E \sim N(1,0)$$
$$\omega_3 = log(\omega_2) + E, E \sim N(1,0)$$
$$\omega_4 = log(\omega_4) + E, E \sim N(1,0)$$

This simulation scheme leads to the first four variables having an obvious relationship between each other, but relatively low linear correlations. (See figure \ref{fig:corstructD3})

```{r logAbs,echo=FALSE, message=FALSE}
w1 <- rnorm(1000)

w2 <- 2*log(abs(w1)) + rnorm(1000)

w3 <- 2*log(abs(w2)) + rnorm(1000)

w4 <- 2*log(abs(w3)) + rnorm(1000)

mvrnorm(n =n, mu = mu, Sigma = sigma) -> simw5w12

data.frame(w1,w2,w3,w4,simw5w12) -> d3
              
c("W1","W2","W3","W4","W5","W6", "W7","W8","W9","W10","W11","W12") -> names(d3)

rep(0, 1000) -> ys #init a vector of zeros

for( i in 1:1000){ #go row by row and create the ys based on the function of e, bts, and d3
ys[i] <- d3[i,1]*bts[1]+
    d3[i,2]*bts[2]+ 
    d3[i,3]*bts[3]+ 
    d3[i,4]*bts[4]+ 
    d3[i,5]*bts[5]+ 
    d3[i,6]*bts[6]+
    d3[i,7]*bts[7]+ 
    d3[i,8]*bts[8]+ 
    d3[i,9]*bts[9]+ 
    d3[i,10]*bts[10]+ 
    d3[i,11]*bts[11]+ 
    d3[i,12]*bts[12] +
    e[i]
}

d3$y <- ys

cord3 <- round(cor(d3)[c(1:7),c(1:7,13)], digits = 3)
cord3 <- cbind(cord3, "beta"= bts[1:7])


```
```{r corstructD3, echo = FALSE, fig.cap="Correlation structure of the first four variables in D3", results="asis"}
ggpairs(
  data =d3,
  columns = c(1:4)
)
```


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The linear correlation structure in $D_3$ is not as striking as in $D_1$. The two strongest linear relationships are between $\omega_2$ and $\omega_3$ with $\rho = -.534$ and between $Y$ and $\omega_2$ with $\rho = .700$. 


##Models and Comparisons

####CART: Regression Trees

```{r,echo=FALSE, message=FALSE,cache=TRUE}
library(tree)
t1 <- tree(y~., round(d1))
#t1 <- prune.tree(t1, k = 2)
#t2 <- tree(y~., round(d2))
#plot(cv.tree(t2))
#t2 <- prune.tree(t2, k = 4)
#t3 <- tree(y~., round(d1))
#plot(cv.tree(t3))
#t3 <- prune.tree(t3, k = 6)

t1$frame$yval <- round(t1$frame$yval)
#t2$frame$yval <- round(t2$frame$yval)
#t3$frame$yval <- round(t3$frame$yval)
```

```{r figcarts, echo = FALSE, fig.pos = 'H',fig.cap="CART representing Y~ V1,...,V12, from D1", results="asis"}
#par(mfrow=c(1,3))
draw.tree(t1, digits = 2, col = c(thesis, rep(thesis, 3)))
#draw.tree(t2, digits = 2, col = c(thesis, rep(thesis, 3)))
#draw.tree(t3, digits = 2, col = c(thesis, rep(thesis, 3)))
```

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A single CART tree representing the model $Y \sim X_1,...,X_{12}$ is easy enough to understand. Starting at the very top of the tree, predictions can be made based on the values of the leaves (or ending nodes) given the requirements of the path to get there. Trees can be quite variable, so to get a better idea of the differences between the methods let's run a simulation.

\begin{algorithm}
\caption{Simulation Scheme 2.1}
\label{sim2.1}
\begin{algorithmic}[1]
\For{$i \leq 1000$ }
\State Randomly sample $\frac 2 3$  of the observations in  $D_1$  to a training set,  $D_{1, train}^i$. The other observations,  $x \in D_1, x \notin D_{1, train}^i$ form the testing set $D_{1, test}^i$
\State Fit a tree, $T^i$, to the data under the model $Y \sim X_1,...,X_2$ using the observations in      $D_{1}^i$
\State Calculate the $MSE_{test}$ of the model using the equation:
    $MSE_{test} = \frac 1 n \sum (y_j - \hat{y_j})^2$
\EndFor
\end{algorithmic}
\end{algorithm}


Note that $n$ is the number of observations in  $D_{1, test}^i$, $y_j \in D_{1, test}^i, \hat{y_j} \in  T^i(D_{2, test}^i)$ for $1 \leq j \leq n$ This produces one  distribution of $MSE_{test}$ for CART. 


```{r fig10,echo=FALSE, message=FALSE,cache=TRUE}
testmseC <- rep(0,1000)
#testmseCt <- rep(0,1000)

for(i in 1:1000){
  train <- sample(1000, 666)
 #tCt <- ctree(y~., d1[train,])
  tC <- tree(y~., d1[train,])
  testmseC[i] <- mean((d1[-train,]$y - predict(tC, d1[-train,]))^2)
#  testmseCt[i] <- mean((d1[-train,]$y - predict(tCt, d1[-train,]))^2)
}

testmse <- (testmseC)
print(dim(testmse))
```


```{r fig11, echo=FALSE, message=FALSE,cache=TRUE}
mselm <- rep(0,1000)

for (i in 1:1000){
  train = sample(1000,666)
  b <- lm(y~., data = d1[train,])
  mselm[i] <- mean((d1$y[-train] - predict(b, d1[-train,]))^2)
}
print(length(mselm))
```


```{r fig12, echo=FALSE, message=FALSE,cache=TRUE}

mseRF <- rep(0,1000)

start <- Sys.time()
for(i in 1:1000){
  train = sample(1000,666)
  rf <- randomForest(x = d1[,1:12], y= d1[,13], subset = train, ntree = 100)
  mseRF[i] <- mean((d1$y[-train] - predict(rf, d1[-train,]))^2)
}

print(length(mseRF))

mserf <- data.frame("LinearModel" = mselm, "CART" = testmse, "RandomForest" = mseRF)

mserf<- melt(mserf)
```




```{r baggedvcartvforest, echo = FALSE, fig.pos= 'H', fig.height=3 ,fig.cap="Simulated MSEtest Distributions of CART, Random, and Bagged Forests", results="asis"}
ggplot(aes(x = value, fill = variable), data = mserf) + 
  geom_density(alpha = .5)+
  scale_fill_manual(values = thesis)+
  guides(fill=guide_legend(title=NULL))+
  scale_x_continuous(limits = c(0,40))+
  xlab(" ")+
  ylab(" ")
```


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The distribution of 100 CART trees' $MSE_{test}$ in the figure \ref{fig:baggedvcartvforest} is roughly normal with a variance of `var(testmseC)`. There is a fair amount of variability in a single tree, they are heavily dependent on fluctuations in the starting data set. The linear model is less flexible but it  


