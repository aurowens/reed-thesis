---
header-includes:
- \usepackage{amssymb,amsthm,amsmath}
- \usepackage{chemarr}
- \usepackage{algorithm}
- \usepackage{algpseudocode}
- \usepackage{pifont}
- \usepackage{float}


output: pdf_document
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = "H")
```
<!--
You can delete the header-includes (lines 3-5 above) if you like and also the chunk below since it is loaded in the skeleton.Rmd file.  They are included so that chap2.Rmd will compile by itself when you hit Knit PDF.
-->

```{r include_reedtemplates_2, include = FALSE}
# This chunk ensures that the reedtemplates package is installed and loaded
# This reedtemplates package includes the template files for the thesis and also
# two functions used for labeling and referencing
if(!require(devtools))
  install.packages("devtools", repos = "http://cran.rstudio.com")

if(!require(reedtemplates)){
  library(devtools)
  devtools::install_github("ismayc/reedtemplates")
  }
library(reedtemplates)
library(MASS)
library(ggplot2)
library(randomForest)
library(maptree)
library(knitr)
library(GGally)
library(reshape)
library(gridExtra)

thesis <- c("#245e67", "#90bd98", "#cfd0a0", "#c49e46", "#d28383")
```



#Simulations and Comparisons

##Simulated Data

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Tree-based methods shine in predictive situations with correlated predictors, although these situations can pose problems for inference. In a situation with correlated predictors $X_1$ and $X_2$, and the treemodel we are considering is $Y \sim X_1 + X_2$, it is difficult to say how much of the modeled effect on $Y$ is due to $X_1$ or $X_2$. To illustrate this idea, compare a few existing methods, and explore methods of inference on tree based models three datasets will be simulated with different correlation structures. We will be focused more on the correlation structure between the predictors than on their relationships with the response and this will be reflected in the simulations.  

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;To aid in comparisons between the methods, one of the simulated datasets considered in this paper will be generated from the same method as used in (Strobl et al, 2008b). Under this method, the 13 x 1000 data set, $D_1$, has 12 predictors, $V_1,..,V_{12}$, where $V_j \sim N(0,1)$. The first four are, however, block correlated to each other with $\rho = .9$. They are related to $Y$ by the linear equation: $$Y = 5 \cdot V_1 + 5 \cdot V_2 + 2 \cdot V_3 + 0 \cdot V_4 + -5 \cdot V_5 + -5\cdot V_6 + 0\cdot V_7 + 0 \cdot ..... + E, E \sim N(0,\frac 1 2 )$$ Note that the coefficients for $V_7,...,V_{12}$ are all zero. 

####Table 1: Correlation of $V_1,..., V_7$ and $Y$ 
```{r stroblSim, warning=FALSE, message=FALSE, echo=FALSE}
rep(0, 12) -> mu #mean of each variable

diag(12) -> sigma #creates a 12 by 12 diagonal matrix with 1's down the diagonal

sigma -> sigma1
.9 -> sigma1[1,2]
.9 -> sigma1[2,1] #adding the block correlation between the first four variables
.9 -> sigma1[1,3]
.9 -> sigma1[3,1]
.9 -> sigma1[3,2]
.9 -> sigma1[2,3]
.9 -> sigma1[1,4]
.9 -> sigma1[4,1]
.9 -> sigma1[2,4]
.9 -> sigma1[4,2]
.9 -> sigma1[4,3]
.9 -> sigma1[3,4]

1000 -> n #number of observations
set.seed(1)
mvrnorm(n =n, mu = mu, Sigma = sigma1) -> sim1000 #sampling from the multivariate normal 
c(5,5,2,0,-5,-5,-2,0,0,0,0,0) -> bts #these are the betas 

rnorm(1000, mean = 0, sd = .5) -> e #error terms

rep(0, 1000) -> ys #init a vector of zeros

for( i in 1:1000){ #go row by row and create the ys based on the function of e, bts, and sim1000
ys[i] <- sim1000[i,1]*bts[1]+
    sim1000[i,2]*bts[2]+ 
    sim1000[i,3]*bts[3]+ 
    sim1000[i,4]*bts[4]+ 
    sim1000[i,5]*bts[5]+ 
    sim1000[i,6]*bts[6]+
    sim1000[i,7]*bts[7]+ 
    sim1000[i,8]*bts[8]+ 
    sim1000[i,9]*bts[9]+ 
    sim1000[i,10]*bts[10]+ 
    sim1000[i,11]*bts[11]+ 
    sim1000[i,12]*bts[12] +
    e[i]
}

d1 <- as.data.frame(sim1000)
d1$y <- ys

cord1 <- round(cor(d1)[c(1:7),c(1:7,13)], digits = 3)
cord1 <- cbind(cord1, "beta"= bts[1:7])

kable(cord1)
```

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;As can be seen from the last column in the table, "beta", although $V4$ was not included in the model $Y \sim V1,..V_{12}$, it has a strong correlation with more influential predictors $V_1,...,V_3$ insures that it still shows a strong, empirical linear correlation with $Y$. A linear model would likely *overstate* the effect of $V_4$ on $Y$. [^1] [^2]

```{r, warning=FALSE, message=FALSE, echo=FALSE}
t <-  melt(d1[,1:5])
#levels(t$variable) <- rev(levels(t$variable))
a <- ggplot(data = t, aes(x = value, fill = variable)) + 
  geom_density(alpha = .8)+
  scale_fill_manual(values = thesis)+
  xlab(" ")+
  ylab(" ")
b <- ggplot(data = d1, aes(x = V4, y = y)) + 
  geom_point(color = thesis[4], alpha = .8) 
```


```{r figdenv1v5, echo = FALSE, fig.cap="Density Graphs for V1 through V5 and a Plot of Y ~ V4, Correlation = .789", results="asis"}
grid.arrange(a,b, ncol=2)
```


[^1]: A brief note on uncertainty is needed here. It's true that in this setting we can say that $V_4$ is actually unimportant to understanding $Y$, but in situations with real data this is profoundly more difficult to parse. Often like in the social science situations that Morgan and Sonquist encountered, the real relationship between correlated predictors is complicated and often there is some theoretical backing or other insight that is gained to include variables that may not be important to the model. 

[^2]: Another point that could be said is that, no $V_4$ is not unimportant, $V_1, V_2,$ and $V_3$ are just stand ins for the real star, $V_4$, as they are nearly the same ($\rho \sim 1$). Then the real relationship represented here is $Y \sim (5 + 5 + 2) \cdot V_4 + -5 \cdot V_5 + -5 \cdot V_6 + -2 \cdot V_7$. This model is not unsuccessful in capturing the structure of the data, and this is typically the practice used to model data with highly correlated predictors. If this seems philosophically satisfying to you, the rest of this thesis may seem a bit inconsequential.





&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; As can be seen in Figure 1 the densities of $V_1,...,V_5$ are all very similar due to the way they were generated. 

##Models and Comparisons

####CART: Regression Trees

```{r fig9,echo=FALSE, message=FALSE,cache=TRUE}
library(tree)
t1 <- tree(y~., round(d1))
#t1 <- prune.tree(t1, k = 2)
#t2 <- tree(y~., round(d2))
#plot(cv.tree(t2))
#t2 <- prune.tree(t2, k = 4)
#t3 <- tree(y~., round(d1))
#plot(cv.tree(t3))
#t3 <- prune.tree(t3, k = 6)

t1$frame$yval <- round(t1$frame$yval)
#t2$frame$yval <- round(t2$frame$yval)
#t3$frame$yval <- round(t3$frame$yval)
```

```{r figcarts, echo = FALSE, fig.cap="CART for the Model Y~, from D1", results="asis"}
#par(mfrow=c(1,3))
draw.tree(t1, digits = 2, col = c(thesis, rep(thesis, 3)))
#draw.tree(t2, digits = 2, col = c(thesis, rep(thesis, 3)))
#draw.tree(t3, digits = 2, col = c(thesis, rep(thesis, 3)))
```

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A single CART tree representing the model $Y \sim X_1,...,X_{12}$ is easy enough to understand. Starting at the very top of the tree, predictions can be made based on the values of the leaves (or ending nodes) given the requirements of the path to get there. Trees can be quite variable, so to get a better idea of the differences between the methods let's run a simulation.

\begin{algorithm}
\caption{Simulation Scheme 2.1}
\label{sim2.1}
\begin{algorithmic}[1]
\For{$i \leq 1000$ }
\State Randomly sample $\frac 2 3$  of the observations in  $D_1$  to a training set,  $D_{1, train}^i$. The other observations,  $x \in D_1, x \notin D_{1, train}^i$ form the testing set $D_{1, test}^i$
\State Fit a tree, $T^i$, to the data under the model $Y \sim X_1,...,X_2$ using the observations in      $D_{1}^i$
\State Calculate the $MSE_{test}$ of the model using the equation:
    $MSE_{test} = \frac 1 n \sum (y_j - \hat{y_j})^2$
\EndFor
\end{algorithmic}
\end{algorithm}


Note that $n$ is the number of observations in  $D_{1, test}^i$, $y_j \in D_{1, test}^i, \hat{y_j} \in  T^i(D_{2, test}^i)$ for $1 \leq j \leq n$ This produces one  distribution of $MSE_{test}$ for CART. 


```{r fig10,echo=FALSE, message=FALSE,cache=TRUE}
testmseC <- rep(0,1000)
#testmseCt <- rep(0,1000)

for(i in 1:1000){
  train <- sample(1000, 666)
 #tCt <- ctree(y~., d1[train,])
  tC <- tree(y~., d1[train,])
  testmseC[i] <- mean((d1[-train,]$y - predict(tC, d1[-train,]))^2)
#  testmseCt[i] <- mean((d1[-train,]$y - predict(tCt, d1[-train,]))^2)
}

testmse <- data.frame( CART= testmseC)

testm <- melt(testmse)



```


```{r fig11, echo=FALSE, message=FALSE,cache=TRUE}
library(bagRboostR)

mse <- rep(0,1000)

time <- Sys.time()
for (i in 1:1000){
  train = sample(1000,666)
  b <- randomForest(y~., data = d1, subset = train, mtry = 5, ntree = 100)
  mse[i] <- mean((d1$y[-train] - predict(b, d1[-train,]))^2)
}

time <- Sys.time()-time

msebag <- data.frame(BaggedForest = mse, CART = sample(testmseC,100))

msebag <- melt(msebag)



```


```{r fig12, echo=FALSE, message=FALSE,cache=TRUE}

mseRF <- 0

start <- Sys.time()
while ((Sys.time() - start) <= time){
  train = sample(1000,666)
  rf <- randomForest(x = d1[,1:12], y= d1[,13], subset = train, ntree = 100)
  mseRF <- c(mseRF, mean((d1$y[-train] - predict(rf, d1[-train,]))^2))
}

mserf <- data.frame(BaggedForest = sample(mse, length(mseRF)), CART = sample(testmseC, length(mseRF)), RandomForest = mseRF)

mserf<- melt(mserf)


```




```{r baggedvcartvforest, echo = FALSE, fig.cap="Simulated MSEtest Distributions of CART, Random, and Bagged Forests", results="asis"}
ggplot(aes(x = value, fill = variable), data = mserf) + 
  geom_density(alpha = .5)+
  scale_fill_manual(values = thesis)+
  guides(fill=guide_legend(title=NULL))+
  xlab(" ")+
  ylab(" ")
```


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The distribution of 100 CART trees' $MSE_{test}$ in the above figure is roughly normal with a variance of `var(testmseC)`. There is a fair amount of variability in a single tree, they are heavily dependent on fluctuations in the starting data set. As mention briefly in the introduction, bagged forests present one solution to this problem. To create a bagged forest, as outlined in *An Introduction to Statistical Learning* by James, Witten, Hastie and Tibshirani, 2013, many bootstrapped samples are taken from the initial dataset and trees are fitted to them. The final predictions are, then, averaged over all of the trees. This ensures that while each tree has high variance, when they are aggregated the variance will decrease. As one can see, the values of $MSE_{test}$ for the bagged forest were entirely below the $MSE_test$ for the trees and the variance was much smaller. As random forests are unbiased, they can be much smaller than their bagged forest cousins without sacrificing accuracy. 


