% This is the Reed College LaTeX thesis template. Most of the work
% for the document class was done by Sam Noble (SN), as well as this
% template. Later comments etc. by Ben Salzberg (BTS). Additional
% restructuring and APA support by Jess Youngberg (JY).
% Your comments and suggestions are more than welcome; please email
% them to cus@reed.edu
%
% See http://web.reed.edu/cis/help/latex.html for help. There are a
% great bunch of help pages there, with notes on
% getting started, bibtex, etc. Go there and read it if you're not
% already familiar with LaTeX.
%
% Any line that starts with a percent symbol is a comment.
% They won't show up in the document, and are useful for notes
% to yourself and explaining commands.
% Commenting also removes a line from the document;
% very handy for troubleshooting problems. -BTS

% As far as I know, this follows the requirements laid out in
% the 2002-2003 Senior Handbook. Ask a librarian to check the
% document before binding. -SN

%%
%% Preamble
%%
% \documentclass{<something>} must begin each LaTeX document
\documentclass[12pt,twoside]{reedthesis}
% Packages are extensions to the basic LaTeX functions. Whatever you
% want to typeset, there is probably a package out there for it.
% Chemistry (chemtex), screenplays, you name it.
% Check out CTAN to see: http://www.ctan.org/
%%
\usepackage{graphicx,latexsym}
\usepackage{amsmath}
\usepackage{amssymb,amsthm}
\usepackage{longtable,booktabs,setspace}
\usepackage{chemarr} %% Useful for one reaction arrow, useless if you're not a chem major
\usepackage[hyphens]{url}
% Added by CII
\usepackage{hyperref}
\usepackage{lmodern}
\usepackage{float}
\floatplacement{figure}{H}
% End of CII addition
\usepackage{rotating}

% Next line commented out by CII
%%% \usepackage{natbib}
% Comment out the natbib line above and uncomment the following two lines to use the new
% biblatex-chicago style, for Chicago A. Also make some changes at the end where the
% bibliography is included.
%\usepackage{biblatex-chicago}
%\bibliography{thesis}


% Added by CII (Thanks, Hadley!)
% Use ref for internal links
\renewcommand{\hyperref}[2][???]{\autoref{#1}}
\def\chapterautorefname{Chapter}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Subsection}
% End of CII addition

% Added by CII
\usepackage{caption}
\captionsetup{width=5in}
% End of CII addition

% \usepackage{times} % other fonts are available like times, bookman, charter, palatino


% To pass between YAML and LaTeX the dollar signs are added by CII
\title{INFFOREST Variable Importance on Random Forests}
\author{Aurora Owens}
% The month and year that you submit your FINAL draft TO THE LIBRARY (May or December)
\date{May 2017}
\division{Mathematics and Natural Sciences}
\advisor{Andrew Bray}
%If you have two advisors for some reason, you can use the following
% Uncommented out by CII
% End of CII addition

%%% Remember to use the correct department!
\department{Mathematics}
% if you're writing a thesis in an interdisciplinary major,
% uncomment the line below and change the text as appropriate.
% check the Senior Handbook if unsure.
%\thedivisionof{The Established Interdisciplinary Committee for}
% if you want the approval page to say "Approved for the Committee",
% uncomment the next line
%\approvedforthe{Committee}

% Added by CII
%%% Copied from knitr
%% maxwidth is the original width if it's less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\renewcommand{\contentsname}{Table of Contents}
% End of CII addition

\setlength{\parskip}{0pt}

% Added by CII

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\Acknowledgements{
.
}

\Dedication{
.
}

\Preface{

}

\Abstract{
Random forests are powerful predictive models but they lack a built-in
method for statistical inference. This paper compares several of the
most common methods of performing inference on random forests while
presenting a new method, INFFOREST variable importance. On simulated
data with multicollinearity, the INFFOREST method is able to show
significance for four out of five predictors used to generate the
response. Existing methods are tested and performed similarly on the
simulated data set. INFFOREST variable importance allows claims to be
made about the relationship between a predictor and the response, in the
context of the rest of the variables in the model. This sets it apart
from the exiting methods and creates some interesting implications.\par
}

	\usepackage{float}
\usepackage{amssymb,amsthm,amsmath}
\usepackage{chemarr}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{pifont}
\usepackage{float}
\usepackage{tikz}

\let\origfigure\figure
\let\endorigfigure\endfigure
\renewenvironment{figure}[1][2] {
    \expandafter\origfigure\expandafter[H]
} {
    \endorigfigure
}
% End of CII addition
%%
%% End Preamble
%%
%

\begin{document}

% Everything below added by CII
      \maketitle
  
  \frontmatter % this stuff will be roman-numbered
  \pagestyle{empty} % this removes page numbers from the frontmatter

      \begin{acknowledgements}
      .
    \end{acknowledgements}
  
  
      \hypersetup{linkcolor=black}
    \setcounter{tocdepth}{2}
    \tableofcontents
  
      \listoftables
  
      \listoffigures
  
      \begin{abstract}
      Random forests are powerful predictive models but they lack a built-in
      method for statistical inference. This paper compares several of the
      most common methods of performing inference on random forests while
      presenting a new method, INFFOREST variable importance. On simulated
      data with multicollinearity, the INFFOREST method is able to show
      significance for four out of five predictors used to generate the
      response. Existing methods are tested and performed similarly on the
      simulated data set. INFFOREST variable importance allows claims to be
      made about the relationship between a predictor and the response, in the
      context of the rest of the variables in the model. This sets it apart
      from the exiting methods and creates some interesting implications.\par
    \end{abstract}
  
      \begin{dedication}
      .
    \end{dedication}
  
  \mainmatter % here the regular arabic numbering starts
  \pagestyle{fancyplain} % turns page numbering back on

  \chapter{Introduction}\label{introduction}
  
  \section{Trees and Random Forests}\label{trees-and-random-forests}
  
  To begin our discussion of trees and random forests, we will first
  consider the following example using data from a dendrologic study of
  five orange trees. This study measured two variables for each tree: the
  age of the tree (recorded in days) and the circumference of the trunk
  (in cm). These are called, in general terms, the variables recorded by
  the study. In the dataset below, each column represents a variable and
  each row represents one set of measurements. There are 2 columns (age
  and circumference) and 35 rows. The first six rows are displayed in
  table \ref{tab:taborange}.
  
  \begin{table}
  
  \caption{\label{tab:unnamed-chunk-2}\label{tab:taborange}The first six rows of the Orange data set}
  \centering
  \begin{tabular}[t]{r|r}
  \hline
  age & circumference\\
  \hline
  118 & 30\\
  \hline
  484 & 58\\
  \hline
  664 & 87\\
  \hline
  1004 & 115\\
  \hline
  1231 & 120\\
  \hline
  1372 & 142\\
  \hline
  \end{tabular}
  \end{table}
  
  ~~~~~Let's pretend we are interested in the following question: knowing
  only the circumference of the orange tree, can we predict the age of the
  tree? This question can be translated into a formula, or a guess at how
  the relation between the two variables functions. We often refer to
  formulas using the following notation:
  
  \[Age \sim Circumference \]
  
  ~~~~~In this formula, circumference is the predictor and age is the
  response. Suppose we expanded the study to include the height of the
  orange trees at various stages of development. Now we can consider both
  the circumference and the height of the tree when we make our
  predictions of the age. When we have multiple predictors, we add them to
  the notation in the following way:\footnote{Often the notation for
    multiple predictors is written \(Y \sim X+W\) but this assumes an
    additive, linear relationship between the predictors and the response.
    This assumption is unnecessary for tree-based models so the notation,
    \(Y\sim X,W\) is used.} \[Age \sim Circumference, Height\]
  
  ~~~~~Returning to the original orange tree data set, we can begin to
  assess the structure in the data set by plotting the data and observing
  the relationship between the variables in figure
  \ref{fig:figOrangeTree1}.
  
  \begin{figure}[H]
  
  {\centering \includegraphics{Thesis_files/figure-latex/figOrangeTree1-1} 
  
  }
  
  \caption{The relationship between age and circumference of the trunk of orange trees.}\label{fig:figOrangeTree1}
  \end{figure}
  
  ~~~~~As can be seen in figure \ref{fig:figOrangeTree1}, generally older
  trees have thicker trunks, and it seems like we are not wrong to suspect
  that circumference is a good predictor of age. As the data could be
  reasonably represented by a straight line, we can say that the
  relationship between trunk circumference and tree age is roughly linear.
  To create our predictions of age, we fit the formula
  \(Age \sim Circumference\) to a model. A predictive model is, put
  simply, a systematic way to make our predictions. The most common type
  of model is the linear model, which creates predictions from a line
  through the data.
  
  \begin{figure}[htbp]
  \centering
  \includegraphics{Thesis_files/figure-latex/unnamed-chunk-3-1.pdf}
  \caption{\label{fig:unnamed-chunk-3}\label{fig:OrangeLM}A linear model
  representing age \textasciitilde{} trunk circumference in orange trees.
  The shaded area represents a 95\% confidence interval around this line.}
  \end{figure}
  
  ~~~~~As can be guessed from figure \ref{fig:OrangeLM}, the linear model
  works better on certain data than others. The linear model necessitates
  several assumptions that may not always be appropriate. We'll return to
  a brief discussion of the assumptions of the linear model later in this
  chapter.
  
  ~~~~~This paper discusses at length tree-based models. A tree for the
  formula \(age \sim circumference\) is similar to the linear model in
  that it presents a systematic way to make predictions, but the two
  differ in that the tree is not linear in any fashion. In fact, we can
  compare the differences between the two models by comparing figures
  \ref{fig:OrangeLM} and \ref{fig:orangeTreePartition}.
  
  \begin{figure}[H]
  
  {\centering \includegraphics{Thesis_files/figure-latex/unnamed-chunk-6-1} 
  
  }
  
  \caption{\label{fig:orangeTreePartition}A tree modeling the formula age \(\sim\)  trunk circumference first creates partitions on the predictor, seen as vertical lines, and then predicts the value of the response within that partition, seen as text.}\label{fig:unnamed-chunk-6}
  \end{figure}
  
  ~~~~~When using the linear model, we make predictions in the following
  way: given a value of X (circumference) the corresponding value of Y
  (age) on the line is our prediction. The predictions from the tree are
  gathered similarly: given a value of X (circumference), our prediction
  is the average value of Y (age) within the partition that X falls
  into.\footnote{While any curve can be approximated in a step-wise manner
    when the number of steps approaches \(\infty\), a tree model does not
    converge to the linear model as the number of splits approaches the
    number of observations, even when the data is linear.}
  
  ~~~~~Tree methods get their name from a common way of representing them
  in higher dimensions, when there is more than one predictor. Figure
  \ref{fig:figorangeTree} shows this method. In this case, given a new
  value for circumference, one would start their predictions at the top of
  the tree and, depending on the value of circumference and the
  instructions at each intersection or split, one would fall down branch
  by branch before landing on a prediction for age. If the new value for
  circumference is less than the value stated at the split, we would move
  down to the branch on the left. If circumference was more than the value
  for circumference at the split, we would move down to the branch on the
  right.
  
  \begin{figure}[H]
  
  {\centering \includegraphics{Thesis_files/figure-latex/unnamed-chunk-7-1} 
  
  }
  
  \caption{\label{fig:figorangeTree}A tree representing age \(\sim\) trunk circumference in orange trees.}\label{fig:unnamed-chunk-7}
  \end{figure}
  
  \subsection{Mathematical notation for
  trees}\label{mathematical-notation-for-trees}
  
  Given a data set, \(D\), and a formula \(Y \sim X\), where \(Y\) and
  \(X\) are columns in \(D\), a tree, \(T\), is a stepwise function from
  \(X\) to \(Y\). \(X\) can also be defined a a set of columns in \(D\).
  Recall, that \(X\) is the set of our predictors and \(Y\) is our
  response. In example in section 1.1, the orange trees data is the data
  set, the ages of the trees make up our response, and the circumferences
  are our predictor.
  
  \[T: X \rightarrow Y\] \(T\) is created using some sample of the rows of
  \(D\). This sample is called the \emph{training} set. The predictions or
  the image of \(T\), are discrete values in the range of \(Y\). We test
  the prediction accuracy of \(T\) using the rows of \(D\) that were not
  in the training set. This is called the \emph{test} set. Often with
  trees, we sample \(D\) by \emph{bootstrapping} the data set.
  ``Bootstrapping'' means sampling with replacement. This allows us to
  have large test and training sets without starting with an extra large
  data set \(D\). This is valuable because it is more difficult to create
  good predictive models on small amounts of data relative to the entire
  population. The orange trees data set is an excelent candidate for
  bootstrapping, as there are only 35 records from 5 trees in the data set
  when there are presumably thousands of orange trees in the world at
  large. Table \ref{tab:tabbootorange} demonstrates a bootstrapped sample
  of the first six rows of the orange trees data set from table
  \ref{tab:taborange}.
  
  \begin{table}
  \caption{\label{tab:unnamed-chunk-9}\label{tab:tabbootorange}The first six rows of the orange trees data set are repeated here (left) with row numbers. The table on the right represent a bootstrapped sample of the original data (left)}
  
  \centering
  \begin{tabular}[t]{r|r|r}
  \hline
  age & circumference & row\\
  \hline
  118 & 30 & 1\\
  \hline
  484 & 58 & 2\\
  \hline
  664 & 87 & 3\\
  \hline
  1004 & 115 & 4\\
  \hline
  1231 & 120 & 5\\
  \hline
  1372 & 142 & 6\\
  \hline
  \end{tabular}
  \centering
  \begin{tabular}[t]{l}
  \hline
  \\
  \hline
  \\
  \hline
  \\
  \hline
  \\
  \hline
  \\
  \hline
  \\
  \hline
  \end{tabular}
  \centering
  \begin{tabular}[t]{l|r|r|r}
  \hline
    & age & circumference & row\\
  \hline
  2 & 484 & 58 & 2\\
  \hline
  3 & 664 & 87 & 3\\
  \hline
  4 & 1004 & 115 & 4\\
  \hline
  6 & 1372 & 142 & 6\\
  \hline
  2.1 & 484 & 58 & 2\\
  \hline
  6.1 & 1372 & 142 & 6\\
  \hline
  \end{tabular}
  \end{table}
  
  There are many ways to test prediction accuracy of a model, but we will
  be considering the following two in this paper: \(MSE\) and
  \(RSS\).{[}\^{}3{]} \(MSE\) is the mean squared error and is defined as:
  \[MSE(T,Y,X) = \frac{1}{n} \sum_1^n (Y - T(X))^2\] Where \(n\) is the
  number of rows in \(X\) and \(Y\). \(RSS\) is the residual sum of
  squares error and is defined as: \[RSS(T,X,Y) = \sum_1^n (Y - T(X))^2\]
  
  Often \(MSE\) and \(RSS\) are calculated on the test set to create an
  estimate of how well the model will be able to predict the outcome of a
  wholly new observation. Then they are refered to as \(MSE_{test}\) and
  \(RSS_{test}\), respectively.
  
  \subsection{A brief history of trees}\label{a-brief-history-of-trees}
  
  Trees are a convenient way to represent data and assist in decision
  making. Morgan and Sonquist (1963) derived a way for constructing trees
  motivated by the specific characteristics of data collected from
  interviews and surveys. The first difficulty in analyzing this data was
  that data collected from surveys is mostly categorical, where the
  observation is that the participant is a member of some discrete group.
  Some common categorical variables are gender, ethnicity, and education
  level. Numeric variables, like age, height, and weight are, in general,
  much easier to work with. On top of this, the data sets Morgan and
  Sonquist dealt with had few participants (rows) and many variables
  (columns). To add to their difficulties, there was reason to believe
  that there were lurking errors in the data that would be hard to
  identify and quantify. Lastly, many of the predictors were correlated.
  Morgan and Sonquist doubted that the additive assumptions of many models
  would be appropriate for this data. They noted that while many
  statistical methods would have difficulty accurately parsing this data,
  a clever researcher with quite a lot of time could create a suitable
  model simply by grouping values of the predictors and predicting that
  the response corresponding to these values would be an average of the
  observed responses given the grouped conditions. Their formalization of
  this procedure in terms of ``decision rules'' laid the groundwork for
  future research on decision trees. See figure
  \ref{fig:orangeTreePartition} for a visualization of this process.
  
  ~~~~~Later researchers proposed new methods for creating trees that
  improved upon the Morgan and Sonquist model. Leo Breiman et al (1984)
  proposed an algorithm called CART (Classification And Regression Trees)
  to fit trees on various types of data. Torsten Hothorn, Kurt Hornik and
  Achim Zeileis argue in their 2006 paper, \emph{Unbiased Recursive
  Partitioning: A Conditional Inference Framework}, CART has a selection
  bias toward variables with either missing values or a great number of
  possible splits. This bias can affect the interpretability of all tree
  models fit using this method. As an alternative to CART and other
  algorithms, Hothorn et al. propose a new method: conditional inference
  trees. The conditional inference trees algorithm is similar to CART in
  many ways, but a thourough description is beyond the scope of this
  paper.
  
  ~~~~\&nbspIn the 1984 textbook \emph{Classification and Regression
  Trees}, Breiman, Friedman, Olshen, and Stone described their method for
  creating, pruning, and testing regression trees. There are essentially
  three steps: one, decide on a variable to split over, two, partition
  that variable space in two distinct partitions, and three, set our
  initial predictions for each partition to be mean value of the response
  according to the observed responses corresponding to the values in the
  partitions. Recursively, this process is repeated for each new partition
  until some stopping condition is reached. This is a top down, greedy
  algorithm that functions by creating as large a tree as possible
  (Breiman, Friedman, Olshen, \& Stone, 1984).
  
  ~~~~~Random Forests are generated by fitting a large number of trees,
  each on a boosted sample of the data. The crucial difference, however,
  between the trees in CART and the trees in a random forest, is that at
  each node in a random forest, only a subset of the predictors are
  considered as candidates for possible splits. This decorrelates each
  tree from its neighbors, and limits variability of the whole forest
  (James, Witten, Hastie, \& Tibshirani, 2013).
  
  ~~~~~ There is a limit to the predictive capabilities of a single tree
  as they suffer from high variance. These are further explored in chapter
  2. To alleviate this, aggregate methods called forests are often used
  instead. They function by enlisting the help of many trees, and then by
  aggregating the responses over all of them. The two most common types of
  forests are bagged and random forests.
  
  \subsection{Mathematical notation for bagged and random
  forests}\label{mathematical-notation-for-bagged-and-random-forests}
  
  Given a data set, \(D\), and a formula \(Y \sim X\), where \(Y\) and
  \(X\) are columns in \(D\), a forest, \(R\), collection of trees from
  \(X\) to \(Y\). As \(R\) is a collection of trees from \(X\) to \(Y\),
  it is also a stepwise function from \(X\) to \(Y\).
  
  \[R:X \rightarrow Y\]
  
  The trees in a forest are fit using bootstrapped samples of the original
  data set \(D\). The bootstrapped training set for a tree \(T\) is
  \(B_t\) and the corresponding test set is \(\bar{B}_t\). These test and
  training sets are called the \emph{in bag} set and the \emph{out of bag}
  set. When the \(MSE\) or \(RSS\) is calculated for a tree in a forest,
  it is done on the out of bag set for that tree and called the OOB (out
  of bag) error. The predictive accuracy of the random forest is the
  average of the out of bag error across all the trees in the forest. The
  predictions for the random forest, \(R(X)\), are the average prediction
  of each tree on \(X\).
  
  These are \emph{bagged forests}. \emph{Random forests} are a variation
  on bagged forests, and the main focus of this paper. There is no
  difference between bagged and random forests when there is only one
  predictor in the data set, but, in cases with more than one predictor,
  the trees are generated in a slightly different way. Usually, to make a
  split all the predictors are considered. (Recall that splits are rules
  on certain predictors; rules like ``if circumference is less than 113.5
  go left, if not go right.'' The statement ``circumference \textless{}
  113.5'' from figure \ref{fig:figorangeTree} is a split at 113.5 on the
  variable circumference). If we had a larger version of the orange tree
  data set that included the heights of each tree, then the tree would
  always consider both height and circumference as possible candidates for
  splits. In a random forest on this expanded orange tree data set, it is
  possible that only one predictor, height or circumference, would be
  considered at a time. The splitting procedure is discussed at length in
  chapter 3 and this property of random forests is discussed in chapter 4.
  For now only a cursory understanding is needed.
  
  \subsection{Inferential vs Descriptive
  Statistics}\label{inferential-vs-descriptive-statistics}
  
  In the earlier sections, we focused on building predictive models, but
  this paper hopes to use tree-based methods beyond this context. The
  linear model is a mainstay in social science because it allows for easy
  and interpretable statistical inference. Return to the orange tree
  example from section 1.1. The linear model gives us a line with which we
  can make predictions, but it also gives estimated coefficients and
  conducts hypothesis tests on the values of the coefficients.
  
  \begin{table}
  
  \caption{\label{tab:unnamed-chunk-11}\label{tab:tablmcoef}Estimated linear coefficients, error, and p-values from the model fit in section 1.1 on the orange tree dataset}
  \centering
  \begin{tabular}[t]{l|r|r|r|r}
  \hline
    & Estimate & Std. Error & t value & Pr(>|t|)\\
  \hline
  (Intercept) & 16.603609 & 78.1406182 & 0.2124837 & 0.8330368\\
  \hline
  circumference & 7.815998 & 0.6058806 & 12.9002281 & 0.0000000\\
  \hline
  \end{tabular}
  \end{table}
  
  This table provides evidence that not only is trunk circumference a good
  predictor of age, the relationship between them is the equation of the
  line:
  
  \[Age = 7.81 \cdot Circumference + 16.6\]
  
  ~~~~~Roughly, for every 1 cm of trunk growth, we would expect the tree
  to be 7.81 days older. Not only are we provided with a way to describe
  the relationship between age and circumference, we have conducted
  statistical tests to make reasonably sure that our estimates, 7.81 and
  16.6 in the equation above, are not zero. Inferential claims about the
  nature of tree age and trunk growth are possible here.
  
  ~~~~~It is important to note the difference between inferential and
  descriptive statistics. Descriptive statistics describe the data at hand
  without making any reference to a larger data generating system that
  they come from. It follows that inferential statistics then make claims
  about the data generating system given the data. The model in figure
  \ref{fig:figorangeTree} could be used to make descriptive claims about
  the orange tree data. For example, given the data we have, we expect a
  sapling with a trunk circumference less than 41 cm to be 118 days old.
  However, trees are variable; they are very sensitive to changes in the
  data set. It's entirely possible that if we fit this tree on a new
  sample of the data, the predictions would change. See chapter 2 for more
  discussion on the variability of trees. Our claims about the relation
  between circumference and age in young orange trees can only be
  descriptive as we have not taken into account the variability in
  gathering them. This paper's aim is to describe a process of making
  inferential claims using trees and random forests that employs
  permutation tests.
  
  ~~~~~As stated in the introduction of the \emph{Chronicle of
  Permutations Statistical Methods} by KJ Berry et al. 2014, there are two
  models of statistical inference. One is the population model, where we
  assume that the data was randomly sampled from one (or more)
  populations. Under this model, we assume that the data generated follows
  some known distribution. ``Under the population model, the level of
  statistical significance that results from applying a statistical test
  to the results of an experiment or a survey corresponds to the frequency
  with which the null hypothesis would be rejected in repeated random
  samplings from the same specified population(s)'' (Berry et al, 2014).
  
  ~~~~~The permutation family of methods, on the other hand, only assumes
  that the observed result was caused by experimental variability. The
  test statistics are calculated for the observed data, then the data is
  permuted a number of times. The statistic is calculated after each
  permutation to derive a distribution of possible values under some null
  hypothesis. Then the original test statistic is tested against this
  distribution. If the observed value is exceptionally rare, then there is
  evidence that our observation did not come from that distribution.
  
  \section{Inference on Random Forests}\label{inference-on-random-forests}
  
  \subsection{The Problem}\label{the-problem}
  
  Random forests create models with great predictive, but poor inferential
  capabilities. After Morgan and Sonquist's initial development of
  decision trees, trees quickly moved to the domain of machine learning
  and away from statistics. Researchers focused on bettering predictions
  and improving run times and less on the statistics behind them. In a
  single tree, descriptive claims may be simple to make, but it is much
  more difficult to describe the behavior of the whole forest. Inferential
  statistics with random forests generally falls behind the predictions in
  importance. This has limited the applications of random forests in
  certain fields, as to many the question of ``why'' the data is the way
  it is is more important than building predictions. There are several
  means of performing descriptive statistics with random forests that
  could be interpreted incorrectly as attempting to answer this but
  without a statistically backed method for performing inference, the use
  of random forest is limited to prediction-only settings.
  
  \subsection{Proposed solutions to this
  problem}\label{proposed-solutions-to-this-problem}
  
  Variable importance could be the tree-based analogue to the coefficients
  of the linear model, in that the variable importance for the predictor
  \(X_i\) in the model for \(Y \sim X_1,...,X_p\) is the amount of
  predictive accuracy due to \(X_i\). Breiman proposed a method of
  permuted variable importance in his paper \emph{Statistical Modeling:
  The Two Cultures} to answer this problem. Their method compares the
  variable importance for each variable in a tree-wise manner. For each
  tree, the permuted variable importance of the variable \(X_j\) is:
  
  \[VI^t(X_j) = \frac{\sum_{i \in |\bar{B}_t|} ({y} - \hat{y})^2}{|\bar{B}_t|} - \frac{\sum_{i \in |\bar{B}_t^*} ({y} - \hat{y}^*)^2}{|\bar{B}_t^*|} \]
  
  ~~~~~ Where \(\bar{B}^t\) is the out of bag sample for tree t, \(|B|\)
  is the number of observations in that sample, \(\bar{B}_p^t\) is with
  \(X_j\) permuted, \(\hat{y}\) is the predicted outcome, and
  \(\hat{y}^*\) is the predicted outcomes after variable \(X_j\) has been
  permuted. This value is averaged over all the trees. It is important to
  note that if the variable \(X_j\) is not split on in the tree \(t\), the
  tree-wise variable importance will be 0.
  
  ~~~~~ Strobl et al from the University of Munich criticize this method
  in their 2008 technical report \emph{Danger: High Power! -- Exploring
  the Statistical Properties of a Test for Random Forest Variable
  Importance}. First, this method has the downside of increasing power
  with increasing numbers of trees in the forest. This is a more or less
  arbitrary parameter which we would hope would not affect our importance
  estimates. Second, the null hypothesis under Breiman and Cutler's
  strategy is that the variable importance \(VI\) for any variable \(X_j\)
  is not equal to zero given \(Y\), the response. Because random forests
  are most often used in situations with multicolinearity that would make
  other methods like the linear model difficult, Strobl argues that any
  variable importance measure worth its salt should not be misled by
  correlation within the predictors.
  
  ~~~~~ The researchers at the University of Munich published a fully
  fleshed response to the Breiman and Cutler method in 2008, titled
  \emph{Conditional Variable Importance for Random Forests} that addresses
  these issues. Strobl et al propose restructuring the Breiman and Cutler
  algorithm to account for conditional dependence among the predictors.
  The null hypothesis is that \(VI_{\beta}(X_j) = 0\) given the predictor
  \(Y\) \emph{and all other predictors} \(X_1,..X_n\). This accounts for
  interactions between \(X_j\) and the other predictors, while preserving
  the relationship between \(Y\) and the remaining predictors.
  
  ~~~~~This paper aims to provide a response to this method. The
  partitions are made from the random forest corresponding to the formula
  of \(Y \sim X_1,...,X_n\) instead of a model of
  \(X_j \sim X_1,...,X_n\). This ignores the common situation where if the
  predictors are correlated enough, then they act as stand ins for each
  other, so that if one variable is heavily influential in a certain tree
  at predicting \(Y\), the other variable will be forgotten altogether.
  
  \chapter{Simulations and Comparisons}\label{simulations-and-comparisons}
  
  Our goal for this chapter is to compare trees, random forests, and
  linear models. In this chapter, we will use simulated data instead of
  the orange data set. One reason for this is theoretical consistency. One
  hopes that one's results will not be rendered null and void by any
  misstep in the data collection that comes to light. This also ensures
  that these simulations can be repeated by later researchers, but,
  granted, it does not make for the most exciting analysis. For now,
  consider \(Y\) to be our response variable. In the first simulation,
  \(V\) will be the set of our predictors, and \(V_j\) to be a predictor
  in \(V\). The formula will be the same for each model: \(Y \sim V\). In
  our second simulation, our set of predictors will be denoted as \(X\)
  and \(X_j\) will be a member of \(X\). The formula in this case is
  \(Y \sim X\). A single row in \(D_1\) will be denoted as \(v\), and a
  single row in \(D_2\) as \(x\).
  
  \section{Simulated Data}\label{simulated-data}
  
  Random forests excel in predicting outcomes with correlated predictors,
  although these situations can make it difficult to perform intelligible
  inference. In a situation in which the correlated predictors are \(X_1\)
  and \(X_2\) and the formula we're estimating is \(Y \sim X_1 + X_2\), it
  can be difficult to say if \(X_1\) or \(X_2\) is truly the better
  predictor. To illustrate this idea, compare a few existing methods, and
  explore methods of inference on tree based models, we will simulate two
  data sets with different correlation structures. We will focus more on
  the correlation structure between the predictors than on their
  relationships with the response and this will be reflected in the
  simulations.
  
  ~~~~~The first simulated dataset is generated under the same scheme as
  in (Strobl, Boulesteix, Kneib, Augustin, \& Zeileis, 2008). Under this
  method, the 13 x 1000 data set, \(D_1\), has 12 predictors,
  \(V_1,..,V_{12}\), where \(V_j \sim N(0,1)\). The first four are block
  correlated to each other with \(\rho = .9\). They are related to \(Y\)
  by the linear equation:
  \[Y = 5 \cdot V_1 + 5 \cdot V_2 + 2 \cdot V_3 + 0 \cdot V_4 + -5 \cdot V_5 + -5\cdot V_6 + 0\cdot V_7 + 0 \cdot ..... + E, E \sim N(0,\frac 1 2 )\]
  Note in table \ref{tab:tabcorSim1}, the coefficients for
  \(V_7,...,V_{12}\) are all zero.
  
  \begin{table}
  
  \caption{\label{tab:unnamed-chunk-12}\label{tab:tabcorSim1}Empirical correlations and coefficients of the variables in the first simulated data set}
  \centering
  \begin{tabular}[t]{l|r|r|r|r|r|r|r|r|r}
  \hline
    & V1 & V2 & V3 & V4 & V5 & V6 & V7 & y & beta\\
  \hline
  V1 & 1.000 & 0.915 & 0.908 & 0.907 & -0.034 & 0.006 & 0.012 & 0.839 & 5\\
  \hline
  V2 & 0.915 & 1.000 & 0.914 & 0.914 & -0.020 & -0.001 & -0.001 & 0.838 & 5\\
  \hline
  V3 & 0.908 & 0.914 & 1.000 & 0.903 & -0.017 & -0.007 & 0.007 & 0.818 & 2\\
  \hline
  V4 & 0.907 & 0.914 & 0.903 & 1.000 & -0.002 & -0.015 & 0.023 & 0.800 & 0\\
  \hline
  V5 & -0.034 & -0.020 & -0.017 & -0.002 & 1.000 & 0.044 & 0.005 & -0.392 & -5\\
  \hline
  V6 & 0.006 & -0.001 & -0.007 & -0.015 & 0.044 & 1.000 & -0.005 & -0.368 & -5\\
  \hline
  V7 & 0.012 & -0.001 & 0.007 & 0.023 & 0.005 & -0.005 & 1.000 & 0.004 & 0\\
  \hline
  \end{tabular}
  \end{table}
  
  ~~~~~In the last column of table \ref{tab:tabcorSim1}, the coefficient
  ``beta'', refers to the function used to generate the \(Y\) values.
  Although \(V4\) was not included in the model \(Y \sim V1,..V_{12}\),
  its strong correlation with more influential predictors \(V_1,...,V_3\)
  ensures that it still shows a strong, empirical linear correlation with
  \(Y\). A linear model would likely \emph{overstate} the effect of
  \(V_4\) on \(Y\).\footnote{A brief note on uncertainty is needed here.
    It's true that in this setting we can say that \(V_4\) is actually
    unimportant to understanding \(Y\), but in situations with real data
    this is profoundly more difficult to parse. Often like in the social
    science situations that Morgan and Sonquist encountered, the real
    relationship between correlated predictors is complicated and often
    there is some theoretical backing or other insight that is gained to
    include variables that may not be important to the model.}\footnote{Another
    point that could be said is that, no \(V_4\) is not unimportant,
    \(V_1, V_2,\) and \(V_3\) are just stand ins for the real star,
    \(V_4\), as they are nearly the same (\(\rho \sim 1\)). Then the real
    relationship represented here is
    \(Y \sim (5 + 5 + 2) \cdot V_4 + -5 \cdot V_5 + -5 \cdot V_6 + -2 \cdot V_7\).
    This model is not unsuccessful in capturing the structure of the data,
    and this is typically the practice used to model data with highly
    correlated predictors. If this seems philosophically satisfying to
    you, the rest of this thesis may seem a bit inconsequential.}
  
  \begin{figure}[htbp]
  \centering
  \includegraphics{Thesis_files/figure-latex/figdenv4y-1.pdf}
  \caption{\label{fig:figdenv4y}Relation between V4 and Y. This relation has
  empirical linear correlation = .789}
  \end{figure}
  
  \begin{figure}[htbp]
  \centering
  \includegraphics{Thesis_files/figure-latex/unnamed-chunk-14-1.pdf}
  \caption{\label{fig:unnamed-chunk-14}\label{fig:figdenv1v5}Empirical
  densities for V1 through V4}
  \end{figure}
  
  ~~~~~The densities of \(V_1,...,V_4\) in figure \ref{fig:figdenv1v5} are
  all very similar due to the way they were generated. \(D_1\) represents
  the case where some of the predictors are linearly correlated with each
  other, but that is not the only possible correlation structure. The data
  set \(D_2\) is simulated similarly to \(D_2\) in that \(D_2\) contains
  twelve predictors and one response variable. The first four variables
  are generated in the following way: \[X_1 \sim N(0,1)\]
  \[X_2 = log(X_1) + E, E \sim N(0,1)\]
  \[X_3 = log(X_2) + E, E \sim N(0,1)\]
  \[X_4 = log(X_4) + E, E \sim N(0,1)\]
  
  This simulation scheme leads to the first four variables having an
  obvious relationship between each other, but relatively low linear
  correlations, as seen in figure \ref{fig:corstructd2}. Predictors are
  sampled by \(X_5,...,X_{12} \sim N(0,1)\). The \(Y\) values are
  generated according to the following formula:
  
  \[Y = 5 \cdot (X_1)^2 + 5 \cdot(X_2)^2 + 2 \cdot (X_3)^2 + 0 \cdot X_4 + -5 \cdot X_5 + -5\cdot X_6 + 0\cdot X_7 + 0 \cdot ..... + E, E \sim N(0,\frac 1 2 )\]
  
  \begin{figure}[htbp]
  \centering
  \includegraphics{Thesis_files/figure-latex/unnamed-chunk-15-1.pdf}
  \caption{\label{fig:unnamed-chunk-15}\label{fig:corstructd2}Correlation
  structure of the first four variables in D2}
  \end{figure}
  
  ~~~~~The correlation structure in \(D_2\) is much more difficult to
  capture with a single line. The relationships between the first four
  predictors form striking, symmetrical scatter plots in figure
  \ref{fig:corstructd2}. This information is considered again in table
  \ref{tab:tabcorSim2}, where the empirical correlations of the first
  seven variables are presented along with their observed correlations
  with \(Y\) and their simulation coefficients.
  
  \begin{table}
  
  \caption{\label{tab:unnamed-chunk-17}\label{tab:tabcorSim2}Empirical correlations and coefficients of the first seven predictors and the response using the second simulated dataset}
  \centering
  \begin{tabular}[t]{l|r|r|r|r|r|r|r|r|r}
  \hline
    & X1 & X2 & X3 & X4 & X5 & X6 & X7 & y & beta\\
  \hline
  X1 & 1.000 & 0.013 & 0.044 & 0.002 & -0.012 & -0.012 & -0.044 & -0.003 & 5\\
  \hline
  X2 & 0.013 & 1.000 & -0.520 & -0.272 & 0.073 & 0.004 & 0.006 & -0.697 & 5\\
  \hline
  X3 & 0.044 & -0.520 & 1.000 & -0.004 & 0.030 & -0.013 & 0.023 & 0.271 & 2\\
  \hline
  X4 & 0.002 & -0.272 & -0.004 & 1.000 & -0.031 & 0.022 & -0.002 & 0.352 & 0\\
  \hline
  X5 & -0.012 & 0.073 & 0.030 & -0.031 & 1.000 & 0.010 & -0.030 & -0.099 & -5\\
  \hline
  X6 & -0.012 & 0.004 & -0.013 & 0.022 & 0.010 & 1.000 & -0.088 & -0.022 & -5\\
  \hline
  X7 & -0.044 & 0.006 & 0.023 & -0.002 & -0.030 & -0.088 & 1.000 & 0.013 & 0\\
  \hline
  \end{tabular}
  \end{table}
  
  \section{Models and Comparisons}\label{models-and-comparisons}
  
  \subsubsection{CART: Regression Trees}\label{cart-regression-trees}
  
  \begin{figure}[htbp]
  \centering
  \includegraphics{Thesis_files/figure-latex/unnamed-chunk-19-1.pdf}
  \caption{\label{fig:unnamed-chunk-19}\label{fig:figcarts}CART representing
  Y\textasciitilde{} X, from D2}
  \end{figure}
  
  The CART tree representing the model \(Y \sim X\) in figure
  \ref{fig:figcarts} is easy enough to understand. Starting at the very
  top of the tree, predictions can be made based on the values of the
  leaves (or ending nodes) given the requirements of the path to get
  there. Trees can be quite variable, so to get a better idea of the
  differences between the methods let's run a simulation. This simulation
  scheme will take advantage of the non linearity present in \(D_2\).
  
  \begin{algorithm}
  \caption{Simulation Scheme 2.1}
  \label{sim2.1}
  \begin{algorithmic}[1]
  \For{$i \leq 1000$ }
  \State Randomly sample $\frac 2 3$  of the observations in  $D_2$  to a training set,  $D_{2, train}^i$. The other observations,  $x \in D_2, x \notin D_{2, train}^i$ form the testing set $D_{2, test}^i$
  \State Fit a tree, $T^i$, to the data under the model $Y \sim X_1,...,X_2$ using the observations in $D_{2}^i$
  \State Calculate the $MSE_{test}$ of the model using the equation:
      $MSE_{test} = \frac 1 n \sum (y_j - \hat{y_j})^2$
  \EndFor
  \end{algorithmic}
  \end{algorithm}
  
  Note that \(n\) is the number of observations in \(D_{1, test}^i\),
  \(y_j \in D_{2, test}^i, \hat{y_j} \in T^i(D_{2, test}^i)\) for
  \(1 \leq j \leq n\) This produces one distribution of \(MSE_{test}\) for
  CART. This simulation scheme will be repeated for the linear model and
  the random forest and the \(MSE_{test}\) distributions are compared in
  figure \ref{fig:baggedvcartvforest}.\footnote{Usually, of the procedure
    described in Simulation Scheme 2.1, the random forest test error is
    estimated using the out of bag error for the forest. We use the same
    method for estimating prediction error for random forests as we do for
    CART and the linear model so that the results are as comparable as
    possible.} Note that the scales on the x-axis are drastically
  different for each of these models. The \(MSE_{test}\) distribution for
  the random forest has such low variance, and low \(MSE_{test}\) it would
  be difficult to display it on the same plot as the other two.
  
  \begin{figure}[htbp]
  \centering
  \includegraphics{Thesis_files/figure-latex/unnamed-chunk-20-1.pdf}
  \caption{\label{fig:unnamed-chunk-20}\label{fig:baggedvcartvforest}The
  simulated MSE distributions of CART, linear model, and the random forest
  on D2}
  \end{figure}
  
  ~~~~~The linear model is characteristically less flexible and less prone
  to over-fitting than either of the tree-based methods, CART and random
  forests, and has a \(MSE_{test}\) distribution that is quite peaked.
  CART is flexible and suffers from high variance. The random forest
  models perform much better on average than either the CART or the linear
  model, due to both the non-linear relationships between \(Y\) and the
  predictors and the random forest's ability to decorrelate each of the
  trees by restricting the variables available on each split. See chapter
  3 for more discussion on the enforced heterogeneity of trees in the
  random forest. As \(MSE_{test}\) is a test of \emph{predictive} accuracy
  it is not surprising that the random forest performed admirably. On a
  certain level, that is what they are designed to do. The linear model is
  not always popular in predictive situations but it is ubiquitous in
  inferential ones.
  
  \backmatter
  
  \chapter{References}\label{references}
  
  \noindent
  
  \setlength{\parindent}{-0.20in} \setlength{\leftskip}{0.20in}
  \setlength{\parskip}{8pt}
  
  Berry (2014), Morgan \& Sonquist (1963)
  
  \hypertarget{refs}{}
  \hypertarget{ref-berry}{}
  Berry, K. (2014). \emph{A chronicle of permutation statistical methods}
  (pp. 1--17). Springer International Publishing Switzerland.
  
  \hypertarget{ref-bibCART}{}
  Breiman, Friedman, Olshen, \& Stone. (1984). \emph{Classification and
  regression trees}. Belmont, CA: Wadsworth International Group.
  
  \hypertarget{ref-bibISL}{}
  James, G., Witten, D., Hastie, T., \& Tibshirani, R. (2013). \emph{An
  introduction to statistical learning} (1st ed.). Springer Verlag New
  York.
  
  \hypertarget{ref-morganSonquist}{}
  Morgan, J., \& Sonquist, J. (1963). Problems in the analysis of survey
  data, and a proposal. \emph{Journal of the American Statistical
  Association}, 415--434.
  
  \hypertarget{ref-bibstrobl2008}{}
  Strobl, C., Boulesteix, A.-L., Kneib, T., Augustin, T., \& Zeileis, A.
  (2008). \emph{Conditional variable importance for random forests}
  (Technical Report Number 23). University of Munich.


  % Index?

\end{document}
