% This is the Reed College LaTeX thesis template. Most of the work
% for the document class was done by Sam Noble (SN), as well as this
% template. Later comments etc. by Ben Salzberg (BTS). Additional
% restructuring and APA support by Jess Youngberg (JY).
% Your comments and suggestions are more than welcome; please email
% them to cus@reed.edu
%
% See http://web.reed.edu/cis/help/latex.html for help. There are a
% great bunch of help pages there, with notes on
% getting started, bibtex, etc. Go there and read it if you're not
% already familiar with LaTeX.
%
% Any line that starts with a percent symbol is a comment.
% They won't show up in the document, and are useful for notes
% to yourself and explaining commands.
% Commenting also removes a line from the document;
% very handy for troubleshooting problems. -BTS

% As far as I know, this follows the requirements laid out in
% the 2002-2003 Senior Handbook. Ask a librarian to check the
% document before binding. -SN

%%
%% Preamble
%%
% \documentclass{<something>} must begin each LaTeX document
\documentclass[12pt,twoside]{reedthesis}
% Packages are extensions to the basic LaTeX functions. Whatever you
% want to typeset, there is probably a package out there for it.
% Chemistry (chemtex), screenplays, you name it.
% Check out CTAN to see: http://www.ctan.org/
%%
\usepackage{graphicx,latexsym}
\usepackage{amsmath}
\usepackage{amssymb,amsthm}
\usepackage{longtable,booktabs,setspace}
\usepackage{chemarr} %% Useful for one reaction arrow, useless if you're not a chem major
\usepackage[hyphens]{url}
% Added by CII
\usepackage{hyperref}
\usepackage{lmodern}
% End of CII addition
\usepackage{rotating}

% Next line commented out by CII
%%% \usepackage{natbib}
% Comment out the natbib line above and uncomment the following two lines to use the new 
% biblatex-chicago style, for Chicago A. Also make some changes at the end where the 
% bibliography is included. 
%\usepackage{biblatex-chicago}
%\bibliography{thesis}


% Added by CII (Thanks, Hadley!)
% Use ref for internal links
\renewcommand{\hyperref}[2][???]{\autoref{#1}}
\def\chapterautorefname{Chapter}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Subsection}
% End of CII addition

% Added by CII 
\usepackage{caption}
\captionsetup{width=5in}
% End of CII addition

% \usepackage{times} % other fonts are available like times, bookman, charter, palatino


% To pass between YAML and LaTeX the dollar signs are added by CII
\title{Statistical Inference on Random Forests}
\author{Aurora Owens}
% The month and year that you submit your FINAL draft TO THE LIBRARY (May or December)
\date{May 2017}
\division{Mathematics and Natural Sciences}
\advisor{Andrew Bray}
%If you have two advisors for some reason, you can use the following
% Uncommented out by CII
% End of CII addition

%%% Remember to use the correct department!
\department{Mathematics}
% if you're writing a thesis in an interdisciplinary major,
% uncomment the line below and change the text as appropriate.
% check the Senior Handbook if unsure.
%\thedivisionof{The Established Interdisciplinary Committee for}
% if you want the approval page to say "Approved for the Committee",
% uncomment the next line
%\approvedforthe{Committee}

% Added by CII
%%% Copied from knitr
%% maxwidth is the original width if it's less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\renewcommand{\contentsname}{Table of Contents}
% End of CII addition

\setlength{\parskip}{0pt}

% Added by CII

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\Acknowledgements{
I want to thank a few people.
}

\Dedication{
You can have a dedication here if you wish.
}

\Preface{
This is an example of a thesis setup to use the reed thesis document
class.
}

\Abstract{
The preface pretty much says it all. \par  Second paragraph of abstract
starts here.
}

	\usepackage{algorithm}
	\usepackage{algpseudocode}
% End of CII addition
%%
%% End Preamble
%%
%

\begin{document}

% Everything below added by CII
      \maketitle
  
  \frontmatter % this stuff will be roman-numbered
  \pagestyle{empty} % this removes page numbers from the frontmatter

      \begin{acknowledgements}
      I want to thank a few people.
    \end{acknowledgements}
  
      \begin{preface}
      This is an example of a thesis setup to use the reed thesis document
      class.
    \end{preface}
  
      \hypersetup{linkcolor=black}
    \setcounter{tocdepth}{2}
    \tableofcontents
  
      \listoftables
  
      \listoffigures
  
      \begin{abstract}
      The preface pretty much says it all. \par  Second paragraph of abstract
      starts here.
    \end{abstract}
  
      \begin{dedication}
      You can have a dedication here if you wish.
    \end{dedication}
  
  \mainmatter % here the regular arabic numbering starts
  \pagestyle{fancyplain} % turns page numbering back on

  \chapter{Introduction}\label{introduction}
  
  \section{Trees and Random Forests}\label{trees-and-random-forests}
  
  \subsection{Trees}\label{trees}
  
  ~~~~~Decision trees may be familiar to many with a background in the
  social or medical sciences as convenient ways to represent data and can
  assist in decision making. Morgan and Sonquist (1963) derived a way for
  constructing trees motivated by the specific feature space of data
  collected from interviews and surveys. Unlike, say agricultural data
  which involves mostly numerical variables like rainfall, the data
  collected from interviews is mostly categorical. On top of this issue,
  the datasets Morgan and Sonquist dealt with had few participants,
  \emph{n}, and much data collected on them, \emph{p}. To continue with
  their list of difficulties, there was reason to believe that there were
  lurking errors in the variables that would be hard identify and
  quantify. Lastly, many of the predictors were correlated and Morgan and
  Sonquist doubted that the additive assumptions of many models would be
  appropriate for this data. Morgan and Sonquist noted that while many
  statistical methods would have difficulty accurately parsing this data,
  a clever researcher with quite a lot of time could create a suitable
  model simply by grouping values in the feature space and predicting that
  the response corresponding to these values would be the mean of the
  observed responses given the grouped conditions. Their formalization of
  this procedure in terms of ``decision rules'' laid the ground work for
  future research on decision trees.
  
  ~~~~~Later researchers proposed new methods for creating trees that
  improved upon the Morgan and Sonquist model. Leo Breiman et al 1984
  proposed an algorithm called CART, \emph{classification and regression
  trees}, that would allow trees to be fit on various types of data. An
  alternative to this method is conditional inference trees. Torsten
  Hothorn, Kurt Hornik, Achim Zeileis argue in their 2006 paper
  \emph{Unbiased Recursive Partitioning: A Conditional Inference
  Framework}, CART has a selection bias toward variables with either
  missing values or a great number of possible splits. This bias can
  effect the interpretability of all tree models fit using this method. As
  an alternative to CART and other algorithms, Hothorn et al propose a new
  method, conditional inference trees.
  
  ~~~~~ There is a limit to the predictive capabilities of a single tree
  as they suffer from high variance. To alleviate this, random forests are
  often used instead. They function by enlisting the help of many trees,
  and then by aggregating the responses over all of them but with a subtle
  trick that ensures the trees will be independent of each other. At each
  split only \emph{m} variables are considered as possible candidates.
  Random forests and their algorithms will be discussed at length in
  Chapter 2.
  
  \section{What We Mean When We Talk About
  Inference}\label{what-we-mean-when-we-talk-about-inference}
  
  \subsection{Inferential vs Descriptive
  Statistics}\label{inferential-vs-descriptive-statistics}
  
  ~~~~~ A note should be made of the difference between inferential and
  descriptive statistics. This paper's aim is to describe a process of
  making inferential claims using random forests, not descriptive ones.
  Descriptive statistics describe the data at hand without making any
  reference to a larger data generating system that they come from. It
  follows that inferential statistics then make claims about the data
  generating system given the data at hand.
  
  ---Frequentist vs Bayesian---
  
  ~~~~~ ---There is some debate about interpreting inferential statistics.
  On one hand, we have the Bayesian model---
  
  \emph{Need a better way to discuss inference than Bayes/frequentist}
  
  \section{Permutations and
  Populations}\label{permutations-and-populations}
  
  ~~~~~ As stated in the introduction of the \emph{Chronical of
  Permutations Statistical Methods} by KJ Berry et al, 2014, there are two
  models of statistical inference. One is the population model, where we
  assume that the data was randomly sampled from one (or more)
  populations. Under this model, we assume that the data generated follows
  some known distribution. ``Under the population model, the level of
  statistical significance that results from applying a statistical test
  to the results of an experiment or a survey corresponds to the frequency
  with which the null hypothesis would be rejected in repeated random
  samplings from the same specified population(s)'', (Berry et al, 2014).
  
  ~~~~~The permutation family of methods, on the other hand, only assumes
  that the observed result was caused by experimental variability. The
  test statistics is first calculated for the observed data, then the data
  is permuted a number of times. The statistic is calculated after each
  permutation to dervive a distribution of possible values. Then the
  original test statistic is tested against this distribution. If it is
  exceptionally rare, then there is evidence that our observation was not
  simply experimental variability.
  
  \section{Inference on Random Forests}\label{inference-on-random-forests}
  
  \subsection{The Problem}\label{the-problem}
  
  ~~~~~Random forests create models with great predictive-, but poor
  inferential capabilities. After Morgan and Sonquist initial development
  of decision trees, they quickly moved to the domain of machine learning
  and away from statistics, thus, researchers focused on bettering
  predictions and improving run times and less on the statistics behind
  them. Inferential statistics with random forests is usually treated as a
  variable selection problem, and generally falls behind the predictions
  in importance. This has limited the applications of random forests in
  certain fields, as to many the question of ``why'' the data is the way
  it is, is just, if not more, important as the predictions. There are
  several means of performing descriptive statistics with random forests
  that could be interpreted incorrectly as attempting to answer this,
  namely base variable importance, but without a statistically backed
  method for performing variable importance, the use of random forest is
  limited to prediction-only settings.
  
  \subsection{Proposed solutions to this
  problem}\label{proposed-solutions-to-this-problem}
  
  ~~~~~Statisticians Breiman and Cutler proposed a method of permuted
  variable importance to answer this problem. Their method compares the
  variable importance for each variable in a tree-wise manner. For each
  tree, the permuted variable importance of the variable \(X_j\) is:
  
  \[PV^t(x_j) = \frac{\sum_{i \in |B|} {y} - \hat{y}^t}{|B|} - \frac{\sum_{i \in |*B|} {y} - \hat{*y}^t}{|*B|} \]
  
  ~~~~~ Where \(B\) is the matrix representing the feature space, \(|B|\)
  is the number of observations, \(*B\) is the matrix of predictors but
  with \(X_j\) permuted, \(\hat{y}\) is the predicted outcome, and
  \(\hat{*y}^t\) is the predicted outcomes after variable \(X_j\) has been
  permuted. This value is averaged over all the trees. It's important to
  note that if the variable \(X_j\) is not split on in the tree \(t\), the
  tree-wise variable importance will be 0.
  
  ~~~~~ Creating a permutation-based method is certainly an attractive
  solution to our problem. One, it allows us to estimate the distribution
  of variable importance and generate a Z score under the null hypothesis
  that \(PV = 0\).
  
  \[PV(x_j) = \frac{\sum_1^ntree PV^t(x_j)}{\frac{\hat{\sigma}}{\sqrt{ntree}}}\]
  
  ~~~~~ Strobl et al from the University of Munich criticize this method
  in their 2008 technical report, \textbf{Danger: High Power! -- Exploring
  the Statistical Properties of a Test for Random Forest Variable
  Importance}. One, this method has the downside of increasing power with
  increasing numbers of trees in the forest. This is a more or less
  arbitrary parameter which we would hope would not affect our importance
  estimates. Secondly, the null hypothesis under Breiman and Cutler's
  strategy is that the variable importance \(V\) for any variable \(X_j\)
  is not equal to zero given \(Y\), the response. Because random forests
  are most often used in situations with multicolinearity that would make
  other methods like the linear model difficult, Strobl argues that any
  variable importance measure worth its salt should not be mislead by
  correlation within the predictors.
  
  ~~~~~ The researchers at the University of Munich published a fully
  fleshed response to the Breiman and Cutler method in 2008, titled
  \emph{Conditional Variable Importance for Random Forests} that address
  these issues. Strobl et al propose restructuring the Breiman and Cutler
  algorithm to account for conditional dependence among the predictors.
  Their algorithm looks like this:
  
  \begin{algorithm}
  \caption{Conditional Variable Importance for Random Forests}
  \label{cviRF}
  \begin{algorithmic}[1]
  \State Fit a random forest to the model, $R_0$, and calculate base variable importance for each variable $V$ 
  \For{every predictor $X_j \in X_1,...,X_n$}
  \State Conditionally permute $X_j$ given the splits found in $R_0$
  \State Fit a new random forest $R_j$ with the permuted data
  \State Calculate a new variable importance $\hat{V}_j$
  \EndFor
  \State For every variable $X_1,..., X_n$, $$CV(X_j) = \hat{V}_j - V_j$$
  \end{algorithmic}
  \end{algorithm}
  
  ~~~~~ The null hypothesis is that \(CV(X_j) = 0\) given the predictor
  \(Y\) \emph{and all other predictors} \(X_1,..X_n\).This accounts for
  interactions between \(X_j\) and the other predictors. Using the
  simulated data from the previous example, here's an implementation of
  the algorithm outlined here as it is in the \texttt{party} package.
  
  ~~~~~This paper aims to provide a response to this method. One the
  conditional permutation algorithm is notoriously slow with any dataset
  of a size that is appropriate for a random forest. Two, the partitions
  are made from the random forest corresponding to the formula of
  \(Y~X_1,...,X_n\) instead of a model of \(X_j~X_1,...,X_n\).
  
  \chapter{Simulations and Comparisons}\label{simulations-and-comparisons}
  
  \section{Simulated Data}\label{simulated-data}
  
  ~~~~~Tree-based methods shine in situations with correlated predictors,
  although these situations can pose problems for inference. In a
  situation with correlated predictors \(X_1\) and \(X_2\), and the model
  we are considering is \(Y \sim X_1 + X_2\), it is difficult to say how
  much of the modeled effect on \(Y\) is due to \(X_1\) or \(X_2\). To
  illustrate this idea, compare a few existing methods, and explore
  methods of inference on tree based models three datasets will be
  simulated with different correlation structures. We will be focused more
  on the correlation structure between the predictors than on their
  relationships with the response and this will be reflected in the
  simulations.
  
  ~~~~~To aid in comparisons between the methods, one of the simulated
  datasets considered in this paper will be generated from the same method
  as used in (Strobl et al, 2008???). Under this method, the 13 x 1000
  data set, \(D_1\), has 12 predictors, \(V_1,..,V_{12}\), where
  \(V_j \sim N(0,1)\). The first four are, however, block correlated to
  each other with \(\rho = .9\). They are related to \(Y\) by the linear
  equation:
  \[Y = 5 \cdot V_1 + 5 \cdot V_2 + 2 \cdot V_3 + 0 \cdot V_4 + -5 \cdot V_5 + -5\cdot V_6 + 0\cdot V_7 + 0 \cdot ..... + E, E \sim N(0,\frac 1 2 )\]
  Note that the coefficients for \(V_7,...,V_{12}\) are all zero.
  
  \subsubsection{\texorpdfstring{Table 1: Correlation of \(V_1,..., V_7\)
  and
  \(Y\)}{Table 1: Correlation of V\_1,..., V\_7 and Y}}\label{table-1-correlation-of-v_1...-v_7-and-y}
  
  \begin{longtable}[]{@{}lrrrrrrrrr@{}}
  \toprule
  & V1 & V2 & V3 & V4 & V5 & V6 & V7 & y & beta\tabularnewline
  \midrule
  \endhead
  V1 & 1.000 & 0.915 & 0.908 & 0.907 & -0.034 & 0.006 & 0.012 & 0.829 &
  5\tabularnewline
  V2 & 0.915 & 1.000 & 0.914 & 0.914 & -0.020 & -0.001 & -0.001 & 0.830 &
  5\tabularnewline
  V3 & 0.908 & 0.914 & 1.000 & 0.903 & -0.017 & -0.007 & 0.007 & 0.808 &
  2\tabularnewline
  V4 & 0.907 & 0.914 & 0.903 & 1.000 & -0.002 & -0.015 & 0.023 & 0.789 &
  0\tabularnewline
  V5 & -0.034 & -0.020 & -0.017 & -0.002 & 1.000 & 0.044 & 0.005 & -0.388
  & -5\tabularnewline
  V6 & 0.006 & -0.001 & -0.007 & -0.015 & 0.044 & 1.000 & -0.005 & -0.364
  & -5\tabularnewline
  V7 & 0.012 & -0.001 & 0.007 & 0.023 & 0.005 & -0.005 & 1.000 & -0.141 &
  -2\tabularnewline
  \bottomrule
  \end{longtable}
  
  ~~~~~As can be seen from the last column in the table, ``beta'',
  although \(V4\) was not included in the model \(Y \sim V1,..V_{12}\),
  its' strong correlation with more influential predictors \(V_1,...,V_3\)
  insures that it still shows a strong linear correlation with \(Y\). A
  linear model would likely \emph{overstate} the effect of \(V_4\) on
  \(Y\).\footnote{A brief note on uncertainty is needed here. It's true
    that in this setting we can say that \(V_4\) is actually unimportant
    to understanding \(Y\), but in situations with real data this is
    profoundly more difficult to parse. Often like in the social science
    situations that Morgan and Sonquist encountered, the real relationship
    between correlated predictors is complicated and often there is some
    theoretical backing or other insight that is gained to include
    variables that may not be important to the model.}\footnote{Another
    point that could be said is that, no \(V_4\) is not unimportant,
    \(V_1, V_2,\) and \(V_3\) are just stand ins for the real star,
    \(V_4\), as they are nearly the same (\(\rho \sim 1\)). Then the real
    relationship represented here is
    \(Y \sim (5 + 5 + 2) \cdot V_4 + -5 \cdot V_5 + -5 \cdot V_6 + -2 \cdot V_7\).
    This model is not unsuccessful in capturing the structure of the data,
    and this is typically the practice used to model data with highly
    correlated predictors. If this seems philosophically satisfying to
    you, the rest of this thesis may seem a bit inconsequential. I
    apologize.}
  
  \subsubsection{Figure 1:}\label{figure-1}
  
  \begin{center}\includegraphics{Thesis_files/figure-latex/fig1-1} \end{center}
  
  ~~~~~ As can be seen above in Figure 1 the densities of \(V_1,...,V_5\)
  are all very similar due to the way they were generated.
  
  \subsubsection{Figure 2:}\label{figure-2}
  
  \begin{center}\includegraphics{Thesis_files/figure-latex/fig2-1} \end{center}
  
  ~~~~~ Figure 2 is an illustration of the relationship between
  \(Y\sim V_4\) with linear correlation of .789.
  
  ~~~~~While \(D_1\) represents a situation with linear correlation
  between the predictors, \(D_2\) does not. Here, the model is the same,
  \(Y~X_1,...,X_12\) where \(Y\) is generated according to the equation:
  \[Y = 5 \cdot X_1 + 5 \cdot X_2 + 2 \cdot X_3 + 0 \cdot X_4 + -5 \cdot X_5 + -5\cdot X_6 + 0\cdot X_7 + 0 \cdot ..... + E, E \sim N(0,\frac 1 2 )\]
  
  However, instead of block correlation with \(\rho = .9\), four variables
  are related to each other by the equations below. Note that
  \(X_1, X_5,...,X_{12} ~ N(0,1)\)
  \[X_2 = X_1 + E, E \sim Exponential(1)\]
  \[X_3 = X_2 + E, E \sim Exponential(1)\]
  \[X_4 = X_3 + E, E \sim Exponential(1)\]
  
  \subsubsection{\texorpdfstring{Table 2: Correlation of \(X_1,..., X_7\)
  and
  \(Y\)}{Table 2: Correlation of X\_1,..., X\_7 and Y}}\label{table-2-correlation-of-x_1...-x_7-and-y}
  
  \begin{longtable}[]{@{}lrrrrrrrrr@{}}
  \toprule
  & X1 & X2 & X3 & X4 & X5 & X6 & X7 & y & beta\tabularnewline
  \midrule
  \endhead
  X1 & 1.000 & 0.693 & 0.605 & 0.552 & -0.043 & 0.009 & -0.006 & 0.760 &
  5\tabularnewline
  X2 & 0.693 & 1.000 & 0.847 & 0.745 & 0.004 & 0.006 & -0.018 & 0.845 &
  5\tabularnewline
  X3 & 0.605 & 0.847 & 1.000 & 0.877 & 0.007 & 0.005 & -0.024 & 0.785 &
  2\tabularnewline
  X4 & 0.552 & 0.745 & 0.877 & 1.000 & 0.011 & 0.006 & -0.032 & 0.696 &
  0\tabularnewline
  X5 & -0.043 & 0.004 & 0.007 & 0.011 & 1.000 & -0.008 & 0.020 & -0.318 &
  -5\tabularnewline
  X6 & 0.009 & 0.006 & 0.005 & 0.006 & -0.008 & 1.000 & -0.046 & -0.310 &
  -5\tabularnewline
  X7 & -0.006 & -0.018 & -0.024 & -0.032 & 0.020 & -0.046 & 1.000 & -0.133
  & -2\tabularnewline
  \bottomrule
  \end{longtable}
  
  ~~~~~As one can see, Table 2 mirrors Table 1. For this dataset, however,
  the correlation structure is more complicated. \(X_1\) and \(X_2\) are
  highly correlated with \(\rho = .7\).
  
  \subsubsection{Figure 3:}\label{figure-3}
  
  \begin{center}\includegraphics{Thesis_files/figure-latex/fig3-1} \end{center}
  
  \subsubsection{Figure 4:}\label{figure-4}
  
  \begin{center}\includegraphics{Thesis_files/figure-latex/fig4-1} \end{center}
  
  ~~~~~As seen in Figure 4, the pattern observed between \(X_1\) and
  \(X_2\) does not carry over to the other correlated predictors.
  
  \subsubsection{Figure 5:}\label{figure-5}
  
  \begin{center}\includegraphics{Thesis_files/figure-latex/fig5-1} \end{center}
  
  ~~~~~Figure 5 demonstrate how the correlation between a few of the
  predictors and \(Y\) may be effected by slope. Scale is much more a
  factor in this dataset, with some variables like \(X_3\) having a larger
  range than the variables \(X_1 \sim N(0,1)\) or
  \(X_5,...,X_{12} \sim MVN()\).
  
  ~~~~~The last dataset we'll consider is \(D_3\), a data set with even
  more non-linear relationships between the first four variables.
  Otherwise it is very similar to both \(D_1\) and \(D_2\). The first four
  variables are generated as follows:
  
  \[\omega_1 \sim N(1,0)\] \[\omega_2 = log(\omega_1) + E, E \sim N(1,0)\]
  \[\omega_3 = log(\omega_2) + E, E \sim N(1,0)\]
  \[\omega_4 = log(\omega_4) + E, E \sim N(1,0)\]
  
  \subsubsection{\texorpdfstring{Table 3:Correlation of
  \(\omega_1,..., \omega_7\) and
  \(Y\)}{Table 3:Correlation of \textbackslash{}omega\_1,..., \textbackslash{}omega\_7 and Y}}\label{table-3correlation-of-omega_1...-omega_7-and-y}
  
  \begin{longtable}[]{@{}lrrrrrrrrr@{}}
  \toprule
  & W1 & W2 & W3 & W4 & W5 & W6 & W7 & y & beta\tabularnewline
  \midrule
  \endhead
  W1 & 1.000 & -0.056 & -0.040 & 0.041 & 0.002 & -0.034 & -0.028 & 0.322 &
  5\tabularnewline
  W2 & -0.056 & 1.000 & -0.533 & -0.279 & -0.002 & 0.049 & -0.003 & 0.668
  & 5\tabularnewline
  W3 & -0.040 & -0.533 & 1.000 & -0.002 & -0.019 & -0.031 & -0.010 &
  -0.096 & 2\tabularnewline
  W4 & 0.041 & -0.279 & -0.002 & 1.000 & -0.007 & -0.008 & -0.079 & -0.223
  & 0\tabularnewline
  W5 & 0.002 & -0.002 & -0.019 & -0.007 & 1.000 & -0.012 & -0.019 & -0.382
  & -5\tabularnewline
  W6 & -0.034 & 0.049 & -0.031 & -0.008 & -0.012 & 1.000 & 0.004 & -0.358
  & -5\tabularnewline
  W7 & -0.028 & -0.003 & -0.010 & -0.079 & -0.019 & 0.004 & 1.000 & -0.159
  & -2\tabularnewline
  \bottomrule
  \end{longtable}
  
  ~~~~~The linear correlation structure in \(D_3\) is not as striking as
  in \(D_1\). The two strongest linear relationships are between
  \(\omega_2\) and \(\omega_3\) with \(\rho = -.534\) and between \(Y\)
  and \(\omega_2\) with \(\rho = .700\).
  
  \subsubsection{Figure 6:}\label{figure-6}
  
  \begin{center}\includegraphics{Thesis_files/figure-latex/fig6-1} \end{center}
  
  ~~~~~Figure 6 provides another way of visualizing some of the
  information given in Table 3. Here we can see the densities as well as
  the paired correlations of the first four variables in \(D_3\).
  
  \subsubsection{Figure 7:}\label{figure-7}
  
  \begin{center}\includegraphics{Thesis_files/figure-latex/fig7-1} \end{center}
  
  ~~~~~There is more variation between the densities of
  \(\omega_1,...,\omega_5\) then we have seen in the other data sets.
  \(\omega_2,\omega_3,\) and \(\omega_4\) have greater spread than their
  counterparts that are generated under the normal distribution.
  
  \subsection{Figure 8:}\label{figure-8}
  
  \begin{center}\includegraphics{Thesis_files/figure-latex/fig8-1} \end{center}
  
  ~~~~~ As the relationship between \(Y\) and \(\omega_2\) was so
  striking, it is nice to see a scatter plot that represents it.
  
  \section{Models and Comparisons}\label{models-and-comparisons}
  
  \subsubsection{CART: Regression Trees}\label{cart-regression-trees}
  
  ~~~~~As outlined in the 1984 textbook, \emph{Classification and
  Regression Trees}, Brieman, Friedman, Olshen, and Stone described their
  method for creating, pruning, and testing regression trees. There are
  essentially three steps: one, decide on a variable to split over, two,
  partition that variable space in two distinct partitions, and three, set
  our initial predictions for each partition to be mean value of the
  response according to the observed responses corresponding to the values
  in the partitions. Recursively, this process is repeated for each new
  partition until some stopping condition is reached.This is a top down,
  greedy algorithm that functions by creating as large a tree as possible
  and then is pruned down to prevent over fitting.
  
  \subsection{Figure 9:}\label{figure-9}
  
  \textbf{CART Representing Y\textasciitilde{}, from datasets D1, D2, and
  D3}
  
  \begin{center}\includegraphics{Thesis_files/figure-latex/fig9-1} \end{center}
  
  ~~~~~Trees can be quite variable, so to get a better idea of the
  differences between the methods let's run a simulation.
  
  \begin{algorithm}
  \caption{Simulation Scheme 2.1}
  \label{sim2.1}
  \begin{algorithmic}[1]
  \For{$i \leq 1000$ }
  \State Randomly sample $\frac 2 3$  of the observations in  $D_2$  to a training set,  $D_{2, train}^i$. The other observations,  $x \in D_2, x \notin D_{2, train}^i$ form the testing set $D_{2, test}^i$
  \State Fit a tree, $T^i$, to the data under the model $Y \sim X_1,...,X_2$ using the observations in      $D_{2}^i$
  \State Calculate the $MSE_test$ of the model using the equation:
      $MSE_{test} = \frac 1 n \sum (y_j - \hat{y_j})^2$
  \EndFor
  \end{algorithmic}
  \end{algorithm}
  
  ~~~~~Where \(n\) is the number of observations in \(D_{2, test}^i\),
  \(y_j \in D_{2, test}^i, \hat{y_j} \in T^i(D_{2, test}^i)\) for
  \(1 \leq j \leq n\) This produces two distributions of \(MSE_{test}\),
  one for CART and one for CTree, conditional inference trees.
  
  \subsection{Figure 10:}\label{figure-10}
  
  \begin{center}\includegraphics{Thesis_files/figure-latex/fig10-1} \end{center}
  
  ~~~~~The distribution of 1000 CART trees' \(MSE_{test}\) is roughly
  normal with a variance of \texttt{var(testmseC)}.
  
  \section{Bagged Forests}\label{bagged-forests}
  
  ~~~~~As one can see in the Figure 10, there is a fair amount of
  variability in a single tree, they are heavily dependent on fluctuations
  in the starting data set. As mention briefly in the introduction, bagged
  forests present one solution to this problem. To create a bagged forest,
  as outlined in \emph{An Introduction to Statistical Learning} by James,
  Witten, Hastie and Tibshirani, 2013, many bootstrapped samples are taken
  from the initial dataset and trees are fitted to them. The final
  predictions are, then, averaged over all of the trees. This ensures that
  while each tree has high variance, when they are aggregated the variance
  will decrease.
  
  ~~~~~Let's put that to the test here using our dataset \(D3\) again.
  We'll build 100 forests of 100 trees each and compare the variability of
  the \(MSE\) distributions.
  
  \subsection{Figure 11:}\label{figure-11}
  
  \begin{center}\includegraphics{Thesis_files/figure-latex/fig11-1} \end{center}
  
  ~~~~~As one can see, the values of \(MSE_{test}\) for the bagged forest
  were entirely below the \(MSE_test\) for the trees and the variance was
  much smaller.
  
  \section{Random Forests}\label{random-forests}
  
  As the number of trees grown in each forest increases, the
  \(MSE_{test}\) decreases (cite). Still, this can become computationally
  intensive on larger data sets where we would like very accurate models.
  Random forests are often seen as a solution to this problem. In a bagged
  forest, every variable is considered when each split is made but in a
  random forest only \(mtry, mtry \leq p\) are considered. This allows us
  to assume that the trees have a level of independence not found in
  bagged forests, and that a small random forest will often out perform
  the bagged forest.
  
  For an illustration, let's build a random forest on \(D3\) and compare
  the \(MSE\).
  
  \subsection{Figure 12:}\label{figure-12}
  
  \begin{center}\includegraphics{Thesis_files/figure-latex/fig12-1} \end{center}
  
  \subsection{TO DO}\label{to-do}
  
  \begin{itemize}
  \tightlist
  \item
    Fix the algorithms to work for regression
  \item
    Make the OOB references more clear
  \item
    Expand around viz created today 3/8
  \end{itemize}
  
  \chapter{Random Forest Variable
  Importance}\label{random-forest-variable-importance}
  
  \section{Breiman et al Introduce Permuted Variable Importance
  (1984)}\label{breiman-et-al-introduce-permuted-variable-importance-1984}
  
  \subsection{Variable Importance on a Single
  Tree}\label{variable-importance-on-a-single-tree}
  
  Breiman et al in \emph{Classification and Regression Trees} (1984)
  propose a method for variable importance for individual trees that stems
  from their definition of \(\tilde{s}\), a surrogate split. Surrogate
  splits help Brieman et al deal with several common problems one may
  have: modeling with missing data, diagnosing masking, and variable
  importance. They are defined using logic that resembles that behind
  random forests.
  
  Definitions
  
  Assume the standard structure for tree models. Let \(D\) be the dataset
  composed of \(D = {Y, X_1,...X_p}\), where the model we would like to
  estimate is of the form \(T: Y \sim X_1,...X_p\). For any node
  \(t \in T(D)\), \(s*\) is the best split of the node into daughters
  \(t_r\) and \(t_l\). Take \(X_i \in D\) and let \(S_i\) be the set of
  all of the splits on \(X_i\) in \(T\). Then set \(\bar{S_i}\) equal to
  the complement of \(S_i\), \(\bar{S_i} = S_i^c\). For any possible split
  \(s_i \in S_i \cup \bar{S_i}\), \(s_i\) will split the node \(t\) into
  two daughters, \(t_{i,l}\) and \(t_{i,r}\). Count the number of times
  that \(s*\) and \(s_i\), while splitting differently, generate the same
  left daughter \(t_{l}\) as \(N_{LL}\) and the number of times they
  generate the same right daughter as \(N_{RR}\). Then the probability
  that a case falls within \(t_L \cap t'_L\) is
  \(P(t_L \cap t'_L) = \sum_j \frac{\pi(j) N_j(LL)}{N_j}\) and the
  probability that a case falls within \(t_R \cap t'_R\) is
  \(P(t_R \cap t'_R) = \sum_j \frac{\pi(j) N_j(RR)}{N_j}\). Where
  \(\pi(j)\) is the prior assumed for the the jth variable.Finally, the
  probability that a surrogate split predicts \(s*\) is
  \(P(s*, s_M) = (t_R \cap t'_R) + P(t_L \cap t'_L)\). Then the surrogate
  split is the value of \(s*\) that maximizes this probability. It is
  denoted \(\tilde{s}\)
  
  A surrogate split \(\tilde{s}\),is one that estimates the best possible
  univariate split \(s*\) on node \(t\).
  
  \textbf{Defintion: Variable Importance, Single Tree}
  
  \emph{oob sample: test yi vs yhat trees that did not include xi in their
  training sample}
  
  \[VI_{tree}(X_i, T) = \sum_{t \in T} \Delta RSS(\tilde{s_i}, t)\] Or the
  decrease of RSS attributable to \(X_i\) across the tree \(T\). In
  \emph{Classification and Regression Trees}, Brieman et al, outline
  several potential problems with this method that the do not attempt to
  solve. First, that this is only one of a number of reasonable ways to
  define variable importance. Second, the variable importances for
  variables \(X_1,..,X_p\) can be effected by outliers or random
  fluctuations within the data. (Ch 5.3)
  
  \subsection{Variable Importance for a Random
  Forest}\label{variable-importance-for-a-random-forest}
  
  One way to define variable importance for a random forest follows
  directly from Breiman et al's definition for a single tree. Now,
  however, the measure of decrease in RSS is replaced with the
  \(\bar{B}^{T}\) or the out-of-the-bag error estimate. Recall that each
  tree in a random forest is fit to a bootstrapped sample of the original
  observations. To estimate \(MSE_test\), therefor, no cross validation is
  needed - each tree is simply tested against the test set of observations
  that were not in that tree's initial training set. The OOB error
  estimate is then averaged over all the trees in the random forest.
  
  Then the method for variable importance is as follows:
  
  \[VI_{base}(X_i) = \sum_{T \in RF}  \frac{\sum_{i \in \bar{B}^{T}} \Delta RSS(x_i,y_i)} {|\bar{B}^{T}|}\]
  However, this method poses some problems. Namely, while variable
  importance for random forests is more stable than for the variable
  importance values for CART, (this is because the model is less variable
  in general), it is lacking the traditional inferential capabilities of
  other regression models. In an effort to derive a p-value for variable
  importance values, Breiman 2001b, describes a \emph{permuted variable
  importance}.
  
  \begin{algorithm}
  \caption{Permuted Variable Importance for Random Forests}
  \label{breiman}
  \begin{algorithmic}[1]
  \State Fit a random forest, $RF$ on the dataset $D$ fitting the model $Y \sim X_1,...,X_p$
  \For{each $T_i \in RF$}
  \State Calculate the oob error before permutation: $$\frac{\sum_{i \in \bar{B}^{T}} I(y = \hat{y})} {|\bar{B}^{T}|}$$
  \For{each $X_i \in {{X_1,...,X_p}}$}
  \State Permute $X_i$
  \State Calculate the oob error again and compare with the estimate before permutation $$VI_{perm1}(X_i, T_i) = \frac{\sum_{i \in \bar{B}^{T}} I(y = \hat{y})} {|\bar{B}^{T}|} -\frac{\sum_{i \in \bar{B}^{T}} I(y = \hat{y^p})} {|\bar{B}^{T}|}$$
  \EndFor
  \EndFor
  \end{algorithmic}
  \end{algorithm}
  
  The permuted variable importance for each variable
  \(X_i \in {{X_1,..,X_p}}\) is the sum of \(VI_{perm1}(X_i, T_i)\) over
  all \(T_i \in RF\).
  
  \section{Strobl et al Respond (2008)}\label{strobl-et-al-respond-2008}
  
  Strobl et al (2008) respond to Brieman's method with one main argument:
  the null hypothesis implied by the permutation distribution utilized in
  permuted variable importance is that \(X_i\) is independent of \(Y\)
  \textbf{and} \({X_j \notin X_1,...,X_p}\) so the null hypothesis will be
  rejected in the case where \(X_j\) is independent of \(Y\) but not some
  subset of the other predictors. As correlation among the predictors is
  very common in data sets that are used for random forests, this is a
  large problem for Breiman's method.
  
  To alleiviate this difficulty, Strobl et al propose a permutation scheme
  under the null hypothesis that \(X_j\) given it's relationship with the
  other predictors is independent of \(Y\).
  
  \begin{algorithm}
  \caption{Conditional Variable Importance for Random Forests}
  \label{strobl}
  \begin{algorithmic}[1]
  \State Fit a random forest, $RF$ on the dataset $D$ fitting the model $Y \sim X_1,...,X_p$
  \For{each $T_i \in RF$}
  \State Calculate the oob error before permutation: $$\frac{\sum_{i \in \bar{B}^{T}} I(y = \hat{y})} {|\bar{B}^{T}|}$$
  \For{each $X_i \in {{X_1,...,X_p}}$}
  \State Permute the dataset, $D$, conditional on the grid created by the splits on $X_i$ in $T_i$
  \State Calculate the oob error again and compare with the estimate before permutation $$VI_{perm2}(X_i, T_i) = \frac{\sum_{i \in \bar{B}^{T}} I(y = \hat{y})} {|\bar{B}^{T}|} -\frac{\sum_{i \in \bar{B}^{T}} I(y = \hat{y^p})} {|\bar{B}^{T}|}$$
  \EndFor
  \EndFor
  \end{algorithmic}
  \end{algorithm}
  
  Finally, as above, the permuted variable importance for each variable
  \(X_i \in {{X_1,..,X_p}}\) is the sum of \(VI_{perm2}(X_i, T_i)\) over
  all \(T_i \in RF\).
  
  AN EXAMPLE:
  
  Fit a simple tree:
  
  \begin{center}\includegraphics{Thesis_files/figure-latex/unnamed-chunk-2-1} \end{center}
  
  \begin{center}\includegraphics{Thesis_files/figure-latex/unnamed-chunk-2-2} \end{center}
  
  \begin{longtable}[]{@{}llllll@{}}
  \toprule
  Group & Predicted Value of Y & Min(W2) & Max(W2) & Min(W1) &
  Max(W1)\tabularnewline
  \midrule
  \endhead
  A & -25.9369010231033 & -7.3 & -5.24 & -0.38 & 0.14\tabularnewline
  B & -10.1521754868715 & -4.99 & 0.19 & -1.68 & 1.75\tabularnewline
  C & 0.791053130734821 & 0.3 & 3.1 & -2.84 & 0.8\tabularnewline
  D & 8.40580623141462 & 0.2 & 1.29 & 0.96 & 2.11\tabularnewline
  E & 30.7209296380884 & 1.86 & 3.82 & 0.82 & 2.7\tabularnewline
  \bottomrule
  \end{longtable}
  
  If we permute the \(\omega_1\) values in group \(B\), this is what that
  plot looks like:
  
  \begin{center}\includegraphics{Thesis_files/figure-latex/unnamed-chunk-3-1} \end{center}
  
  \section{Inferential Variable
  Importance}\label{inferential-variable-importance}
  
  This thesis hopes to be a reponse to conditional variable importance as
  outlined by Strobl et al 2008. First is that the practice of permuting
  given the partitions from the model \(Y \sim X_1,...,X_p\) instead of
  \(X_j \sim X_1,..,X_p\)
  
  \chapter{INFTrees and INFforests Variable
  Importance}\label{inftrees-and-infforests-variable-importance}
  
  \section{Theory}\label{theory}
  
  \section{Implementation}\label{implementation}
  
  \chapter*{Conclusion}\label{conclusion}
  \addcontentsline{toc}{chapter}{Conclusion}
  
  \setcounter{chapter}{4} \setcounter{section}{0}
  
  If we don't want Conclusion to have a chapter number next to it, we can
  add the \texttt{\{.unnumbered\}} attribute. This has an unintended
  consequence of the sections being labeled as 3.6 for example though
  instead of 4.1. The \LaTeX~commands immediately following the Conclusion
  declaration get things back on track.
  
  \subsubsection{More info}\label{more-info}
  
  And here's some other random info: the first paragraph after a chapter
  title or section head \emph{shouldn't be} indented, because indents are
  to tell the reader that you're starting a new paragraph. Since that's
  obvious after a chapter or section title, proper typesetting doesn't add
  an indent there.
  
  \appendix
  
  \chapter{The First Appendix}\label{the-first-appendix}
  
  This first appendix includes all of the R chunks of code that were
  hidden throughout the document (using the \texttt{include\ =\ FALSE}
  chunk tag) to help with readibility and/or setup.
  
  \subsubsection{In the main Rmd file:}\label{in-the-main-rmd-file}
  
  \begin{Shaded}
  \begin{Highlighting}[]
  \CommentTok{# This chunk ensures that the reedtemplates package is}
  \CommentTok{# installed and loaded. This reedtemplates package includes}
  \CommentTok{# the template files for the thesis and also two functions}
  \CommentTok{# used for labeling and referencing}
  \NormalTok{if(!}\KeywordTok{require}\NormalTok{(devtools))}
    \KeywordTok{install.packages}\NormalTok{(}\StringTok{"devtools"}\NormalTok{, }\DataTypeTok{repos =} \StringTok{"http://cran.rstudio.com"}\NormalTok{)}
  \NormalTok{if(!}\KeywordTok{require}\NormalTok{(reedtemplates))\{}
    \KeywordTok{library}\NormalTok{(devtools)}
    \NormalTok{devtools::}\KeywordTok{install_github}\NormalTok{(}\StringTok{"ismayc/reedtemplates"}\NormalTok{)}
  \NormalTok{\}}
  \KeywordTok{library}\NormalTok{(reedtemplates)}
  \end{Highlighting}
  \end{Shaded}
  
  \subsubsection{\texorpdfstring{In
  \protect\hyperlink{ref_labels}{}:}{In :}}\label{in}
  
  \begin{Shaded}
  \begin{Highlighting}[]
  \CommentTok{# This chunk ensures that the reedtemplates package is}
  \CommentTok{# installed and loaded. This reedtemplates package includes}
  \CommentTok{# the template files for the thesis and also two functions}
  \CommentTok{# used for labeling and referencing}
  \NormalTok{if(!}\KeywordTok{require}\NormalTok{(devtools))}
    \KeywordTok{install.packages}\NormalTok{(}\StringTok{"devtools"}\NormalTok{, }\DataTypeTok{repos =} \StringTok{"http://cran.rstudio.com"}\NormalTok{)}
  \NormalTok{if(!}\KeywordTok{require}\NormalTok{(dplyr))}
      \KeywordTok{install.packages}\NormalTok{(}\StringTok{"dplyr"}\NormalTok{, }\DataTypeTok{repos =} \StringTok{"http://cran.rstudio.com"}\NormalTok{)}
  \NormalTok{if(!}\KeywordTok{require}\NormalTok{(ggplot2))}
      \KeywordTok{install.packages}\NormalTok{(}\StringTok{"ggplot2"}\NormalTok{, }\DataTypeTok{repos =} \StringTok{"http://cran.rstudio.com"}\NormalTok{)}
  \NormalTok{if(!}\KeywordTok{require}\NormalTok{(reedtemplates))\{}
    \KeywordTok{library}\NormalTok{(devtools)}
    \NormalTok{devtools::}\KeywordTok{install_github}\NormalTok{(}\StringTok{"ismayc/reedtemplates"}\NormalTok{)}
    \NormalTok{\}}
  \KeywordTok{library}\NormalTok{(reedtemplates)}
  \KeywordTok{library}\NormalTok{(tree)}
  \KeywordTok{library}\NormalTok{(reedtemplates)}
  \KeywordTok{library}\NormalTok{(MASS)}
  \KeywordTok{library}\NormalTok{(ggplot2)}
  \KeywordTok{library}\NormalTok{(randomForest)}
  \KeywordTok{library}\NormalTok{(maptree)}
  \KeywordTok{library}\NormalTok{(knitr)}
  \KeywordTok{library}\NormalTok{(GGally)}
  \KeywordTok{library}\NormalTok{(reshape)}
  \CommentTok{#flights <- read.csv("data/flights.csv")}
  \NormalTok{thesis <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"#245e67"}\NormalTok{, }\StringTok{"#90bd98"}\NormalTok{, }\StringTok{"#cfd0a0"}\NormalTok{, }\StringTok{"#c49e46"}\NormalTok{, }\StringTok{"#d28383"}\NormalTok{)}
  \end{Highlighting}
  \end{Shaded}
  
  \chapter{The Second Appendix: CTree}\label{the-second-appendix-ctree}
  
  \subsection{Conditional Inference
  Trees}\label{conditional-inference-trees}
  
  ~~~~~As mentioned in the introduction, CART has the tendency to bias
  towards variables with the most possible splits and overfitting. There
  is little head paid to statistical significance or general statistical
  theory. \emph{Conditional Inference Trees} are a method proposed by
  Horthon et al, 2006, that utilizes permutation theory to create and
  algorithm that is sensitive to these issues. A crutial difference
  between CTree and CART is that while CART is a top down algorithm, CTree
  initially assumes each row of the dataset is a node and then gradually
  prunes them.
  
  \begin{algorithm}
  \caption{Conditional Inference Trees}
  \label{ctree}
  \begin{algorithmic}[1]
  \For{$w_i, i \in \{{w_1,...,w_n}\}$}
  \State Test the global null hypothesis of independence between any of the $m$ covariates and the response. 
  \If{$H_O$ cannot be rejected} 
  \State Stop 
  \Else \State Select predictor $X_j$ with the strongest linear association to $Y$ 
  \EndIf
  \State Choose a set $A \in X_j$ such that $A \cup X_j \ A = A$ 
  \State The case weights, $w_{left}$ and $w_{right}$ are then defined as $w_{left,i} = w_i I (x_j \in X_j, \in A)$ and $w_{right,i} = w_i I(x_j \in X_j, x_j \notin A)$
  \EndFor
  \end{algorithmic}
  \end{algorithm}
  
  ~~~~~The case weights, \(w_i \in {w_1,..., w_n}\), correspond to nodes
  are defined as:\[w_i = I(x_i \in {N_t})\] ~~~~~Where \(x_i\) is a vector
  of observations and \(N_t\) is a node in the tree.
  
  \textbf{CTree fitted to \(D_3\)}
  
  \begin{center}\includegraphics{Thesis_files/figure-latex/ctree-1} \end{center}
  
  \begin{center}\includegraphics{Thesis_files/figure-latex/ctree-2} \end{center}
  
  \backmatter
  
  \chapter{References}\label{references}
  
  \noindent
  
  \setlength{\parindent}{-0.20in} \setlength{\leftskip}{0.20in}
  \setlength{\parskip}{8pt}
  
  \hypertarget{refs}{}
  \hypertarget{ref-angel2000}{}
  Angel, E. (2000). \emph{Interactive computer graphics : A top-down
  approach with opengl}. Boston, MA: Addison Wesley Longman.
  
  \hypertarget{ref-angel2001}{}
  Angel, E. (2001a). \emph{Batch-file computer graphics : A bottom-up
  approach with quicktime}. Boston, MA: Wesley Addison Longman.
  
  \hypertarget{ref-angel2002a}{}
  Angel, E. (2001b). \emph{Test second book by angel}. Boston, MA: Wesley
  Addison Longman.


  % Index?

\end{document}

