% This is the Reed College LaTeX thesis template. Most of the work
% for the document class was done by Sam Noble (SN), as well as this
% template. Later comments etc. by Ben Salzberg (BTS). Additional
% restructuring and APA support by Jess Youngberg (JY).
% Your comments and suggestions are more than welcome; please email
% them to cus@reed.edu
%
% See http://web.reed.edu/cis/help/latex.html for help. There are a
% great bunch of help pages there, with notes on
% getting started, bibtex, etc. Go there and read it if you're not
% already familiar with LaTeX.
%
% Any line that starts with a percent symbol is a comment.
% They won't show up in the document, and are useful for notes
% to yourself and explaining commands.
% Commenting also removes a line from the document;
% very handy for troubleshooting problems. -BTS

% As far as I know, this follows the requirements laid out in
% the 2002-2003 Senior Handbook. Ask a librarian to check the
% document before binding. -SN

%%
%% Preamble
%%
% \documentclass{<something>} must begin each LaTeX document
\documentclass[12pt,twoside]{reedthesis}
% Packages are extensions to the basic LaTeX functions. Whatever you
% want to typeset, there is probably a package out there for it.
% Chemistry (chemtex), screenplays, you name it.
% Check out CTAN to see: http://www.ctan.org/
%%
\usepackage{graphicx,latexsym}
\usepackage{amsmath}
\usepackage{amssymb,amsthm}
\usepackage{longtable,booktabs,setspace}
\usepackage{chemarr} %% Useful for one reaction arrow, useless if you're not a chem major
\usepackage[hyphens]{url}
% Added by CII
\usepackage{hyperref}
\usepackage{lmodern}
\usepackage{float}
\floatplacement{figure}{H}
% End of CII addition
\usepackage{rotating}

% Next line commented out by CII
%%% \usepackage{natbib}
% Comment out the natbib line above and uncomment the following two lines to use the new
% biblatex-chicago style, for Chicago A. Also make some changes at the end where the
% bibliography is included.
%\usepackage{biblatex-chicago}
%\bibliography{thesis}


% Added by CII (Thanks, Hadley!)
% Use ref for internal links
\renewcommand{\hyperref}[2][???]{\autoref{#1}}
\def\chapterautorefname{Chapter}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Subsection}
% End of CII addition

% Added by CII
\usepackage{caption}
\captionsetup{width=5in}
% End of CII addition

% \usepackage{times} % other fonts are available like times, bookman, charter, palatino


% To pass between YAML and LaTeX the dollar signs are added by CII
\title{Statistical Inference on Random Forests}
\author{Aurora Owens}
% The month and year that you submit your FINAL draft TO THE LIBRARY (May or December)
\date{May 2017}
\division{Mathematics and Natural Sciences}
\advisor{Andrew Bray}
%If you have two advisors for some reason, you can use the following
% Uncommented out by CII
% End of CII addition

%%% Remember to use the correct department!
\department{Mathematics}
% if you're writing a thesis in an interdisciplinary major,
% uncomment the line below and change the text as appropriate.
% check the Senior Handbook if unsure.
%\thedivisionof{The Established Interdisciplinary Committee for}
% if you want the approval page to say "Approved for the Committee",
% uncomment the next line
%\approvedforthe{Committee}

% Added by CII
%%% Copied from knitr
%% maxwidth is the original width if it's less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\renewcommand{\contentsname}{Table of Contents}
% End of CII addition

\setlength{\parskip}{0pt}

% Added by CII

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\Acknowledgements{
I want to thank a few people.
}

\Dedication{
You can have a dedication here if you wish.
}

\Preface{
This is an example of a thesis setup to use the reed thesis document
class.
}

\Abstract{
The preface pretty much says it all. \par  Second paragraph of abstract
starts here.
}

	\usepackage{algorithm}
	\usepackage{algpseudocode}
% End of CII addition
%%
%% End Preamble
%%
%

\begin{document}

% Everything below added by CII
      \maketitle
  
  \frontmatter % this stuff will be roman-numbered
  \pagestyle{empty} % this removes page numbers from the frontmatter

      \begin{acknowledgements}
      I want to thank a few people.
    \end{acknowledgements}
  
      \begin{preface}
      This is an example of a thesis setup to use the reed thesis document
      class.
    \end{preface}
  
      \hypersetup{linkcolor=black}
    \setcounter{tocdepth}{2}
    \tableofcontents
  
      \listoftables
  
      \listoffigures
  
      \begin{abstract}
      The preface pretty much says it all. \par  Second paragraph of abstract
      starts here.
    \end{abstract}
  
      \begin{dedication}
      You can have a dedication here if you wish.
    \end{dedication}
  
  \mainmatter % here the regular arabic numbering starts
  \pagestyle{fancyplain} % turns page numbering back on

  \chapter{Introduction}\label{introduction}
  
  \section{Trees and Random Forests}\label{trees-and-random-forests}
  
  \subsection{Trees}\label{trees}
  
  ~~~~~Decision trees may be familiar to many with a background in the
  social or medical sciences as convenient ways to represent data and can
  assist in decision making. Morgan and Sonquist (1963) derived a way for
  constructing trees motivated by the specific feature space of data
  collected from interviews and surveys. Unlike, say agricultural data
  which involves mostly numerical variables like rainfall, the data
  collected from interviews is mostly categorical. On top of this issue,
  the datasets Morgan and Sonquist dealt with had few participants,
  \emph{n}, and much data collected on them, \emph{p}. To continue with
  their list of difficulties, there was reason to believe that there were
  lurking errors in the variables that would be hard identify and
  quantify. Lastly, many of the predictors were correlated and Morgan and
  Sonquist doubted that the additive assumptions of many models would be
  appropriate for this data. Morgan and Sonquist noted that while many
  statistical methods would have difficulty accurately parsing this data,
  a clever researcher with quite a lot of time could create a suitable
  model simply by grouping values in the feature space and predicting that
  the response corresponding to these values would be the mean of the
  observed responses given the grouped conditions. Their formalization of
  this procedure in terms of ``decision rules'' laid the ground work for
  future research on decision trees.
  
  ~~~~~Later researchers proposed new methods for creating trees that
  improved upon the Morgan and Sonquist model. Leo Breiman et al 1984
  proposed an algorithm called CART, \emph{classification and regression
  trees}, that would allow trees to be fit on various types of data. An
  alternative to this method is conditional inference trees. Torsten
  Hothorn, Kurt Hornik, Achim Zeileis argue in their 2006 paper
  \emph{Unbiased Recursive Partitioning: A Conditional Inference
  Framework}, CART has a selection bias toward variables with either
  missing values or a great number of possible splits. This bias can
  effect the interpretability of all tree models fit using this method. As
  an alternative to CART and other algorithms, Hothorn et al propose a new
  method, conditional inference trees.
  
  ~~~~~ There is a limit to the predictive capabilities of a single tree
  as they suffer from high variance. To alleviate this, random forests are
  often used instead. They function by enlisting the help of many trees,
  and then by aggregating the responses over all of them but with a subtle
  trick that ensures the trees will be independent of each other. At each
  split only \emph{m} variables are considered as possible candidates.
  Random forests and their algorithms will be discussed at length in
  Chapter 2.
  
  \section{What We Mean When We Talk About
  Inference}\label{what-we-mean-when-we-talk-about-inference}
  
  \subsection{Inferential vs Descriptive
  Statistics}\label{inferential-vs-descriptive-statistics}
  
  ~~~~~ A note should be made of the difference between inferential and
  descriptive statistics. This paper's aim is to describe a process of
  making inferential claims using random forests, not descriptive ones.
  Descriptive statistics describe the data at hand without making any
  reference to a larger data generating system that they come from. It
  follows that inferential statistics then make claims about the data
  generating system given the data at hand.
  
  ---Frequentist vs Bayesian---
  
  ~~~~~ ---There is some debate about interpreting inferential statistics.
  On one hand, we have the Bayesian model---
  
  \emph{Need a better way to discuss inference than Bayes/frequentist}
  
  \section{Permutations and
  Populations}\label{permutations-and-populations}
  
  ~~~~~ As stated in the introduction of the \emph{Chronical of
  Permutations Statistical Methods} by KJ Berry et al, 2014, there are two
  models of statistical inference. One is the population model, where we
  assume that the data was randomly sampled from one (or more)
  populations. Under this model, we assume that the data generated follows
  some known distribution. ``Under the population model, the level of
  statistical significance that results from applying a statistical test
  to the results of an experiment or a survey corresponds to the frequency
  with which the null hypothesis would be rejected in repeated random
  samplings from the same specified population(s)'', (Berry et al, 2014).
  
  ~~~~~The permutation family of methods, on the other hand, only assumes
  that the observed result was caused by experimental variability. The
  test statistics is first calculated for the observed data, then the data
  is permuted a number of times. The statistic is calculated after each
  permutation to dervive a distribution of possible values. Then the
  original test statistic is tested against this distribution. If it is
  exceptionally rare, then there is evidence that our observation was not
  simply experimental variability.
  
  \section{Inference on Random Forests}\label{inference-on-random-forests}
  
  \subsection{The Problem}\label{the-problem}
  
  ~~~~~Random forests create models with great predictive-, but poor
  inferential capabilities. After Morgan and Sonquist initial development
  of decision trees, they quickly moved to the domain of machine learning
  and away from statistics, thus, researchers focused on bettering
  predictions and improving run times and less on the statistics behind
  them. Inferential statistics with random forests is usually treated as a
  variable selection problem, and generally falls behind the predictions
  in importance. This has limited the applications of random forests in
  certain fields, as to many the question of ``why'' the data is the way
  it is, is just, if not more, important as the predictions. There are
  several means of performing descriptive statistics with random forests
  that could be interpreted incorrectly as attempting to answer this,
  namely base variable importance, but without a statistically backed
  method for performing variable importance, the use of random forest is
  limited to prediction-only settings.
  
  \subsection{Proposed solutions to this
  problem}\label{proposed-solutions-to-this-problem}
  
  ~~~~~Statisticians Breiman and Cutler proposed a method of permuted
  variable importance to answer this problem. Their method compares the
  variable importance for each variable in a tree-wise manner. For each
  tree, the permuted variable importance of the variable \(X_j\) is:
  
  \[PV^t(x_j) = \frac{\sum_{i \in |B|} {y} - \hat{y}^t}{|B|} - \frac{\sum_{i \in |*B|} {y} - \hat{*y}^t}{|*B|} \]
  
  ~~~~~ Where \(B\) is the matrix representing the feature space, \(|B|\)
  is the number of observations, \(*B\) is the matrix of predictors but
  with \(X_j\) permuted, \(\hat{y}\) is the predicted outcome, and
  \(\hat{*y}^t\) is the predicted outcomes after variable \(X_j\) has been
  permuted. This value is averaged over all the trees. It's important to
  note that if the variable \(X_j\) is not split on in the tree \(t\), the
  tree-wise variable importance will be 0.
  
  ~~~~~ Creating a permutation-based method is certainly an attractive
  solution to our problem. One, it allows us to estimate the distribution
  of variable importance and generate a Z score under the null hypothesis
  that \(PV = 0\).
  
  \[PV(x_j) = \frac{\sum_1^ntree PV^t(x_j)}{\frac{\hat{\sigma}}{\sqrt{ntree}}}\]
  
  ~~~~~ Strobl et al from the University of Munich criticize this method
  in their 2008 technical report, \textbf{Danger: High Power! -- Exploring
  the Statistical Properties of a Test for Random Forest Variable
  Importance}. One, this method has the downside of increasing power with
  increasing numbers of trees in the forest. This is a more or less
  arbitrary parameter which we would hope would not affect our importance
  estimates. Secondly, the null hypothesis under Breiman and Cutler's
  strategy is that the variable importance \(V\) for any variable \(X_j\)
  is not equal to zero given \(Y\), the response. Because random forests
  are most often used in situations with multicolinearity that would make
  other methods like the linear model difficult, Strobl argues that any
  variable importance measure worth its salt should not be mislead by
  correlation within the predictors.
  
  ~~~~~ The researchers at the University of Munich published a fully
  fleshed response to the Breiman and Cutler method in 2008, titled
  \emph{Conditional Variable Importance for Random Forests} that address
  these issues. Strobl et al propose restructuring the Breiman and Cutler
  algorithm to account for conditional dependence among the predictors.
  Their algorithm looks like this:
  
  \begin{algorithm}
  \caption{Conditional Variable Importance for Random Forests}
  \label{cviRF}
  \begin{algorithmic}[1]
  \State Fit a random forest to the model, $R_0$, and calculate base variable importance for each variable $V$ 
  \For{every predictor $X_j \in X_1,...,X_n$}
  \State Conditionally permute $X_j$ given the splits found in $R_0$
  \State Fit a new random forest $R_j$ with the permuted data
  \State Calculate a new variable importance $\hat{V}_j$
  \EndFor
  \State For every variable $X_1,..., X_n$, $$CV(X_j) = \hat{V}_j - V_j$$
  \end{algorithmic}
  \end{algorithm}
  
  ~~~~~ The null hypothesis is that \(CV(X_j) = 0\) given the predictor
  \(Y\) \emph{and all other predictors} \(X_1,..X_n\).This accounts for
  interactions between \(X_j\) and the other predictors. Using the
  simulated data from the previous example, here's an implementation of
  the algorithm outlined here as it is in the \texttt{party} package.
  
  ~~~~~This paper aims to provide a response to this method. One the
  conditional permutation algorithm is notoriously slow with any dataset
  of a size that is appropriate for a random forest. Two, the partitions
  are made from the random forest corresponding to the formula of
  \(Y~X_1,...,X_n\) instead of a model of \(X_j~X_1,...,X_n\).
  
  \chapter{Random Forests: a
  description}\label{random-forests-a-description}
  
  A random forest \(R_f\) is the set of functions \(T_1,...,T_N\) where
  each \(T_j\) is a piece-wise function from the sample space \(\Omega\)
  to the response space \(\Phi\). In general, \(\Omega\) is defined by an
  n x p matrix where each column is a random variable and \(\Phi\) is
  defined by an n x 1 vector \(Y\).
  
  Each tree \(T_j\) is generated on a subset of both \(\Omega\) and
  \(\Phi\) called the training set. It is then tested on a disjoint subset
  of \(\Omega\) called the test set and the values of \(T_j\) on this test
  set are called the predictions.
  
  \emph{T as a piece-wise constant function from the predictors to Y}
  
  \chapter{Simulations and Comparisons}\label{simulations-and-comparisons}
  
  \section{Simulated Data}\label{simulated-data}
  
  ~~~~~Tree-based methods shine in situations with correlated predictors,
  although these situations can pose problems for inference. In a
  situation with correlated predictors \(X_1\) and \(X_2\), and the model
  we are considering is \(Y \sim X_1 + X_2\), it is difficult to say how
  much of the modeled effect on \(Y\) is due to \(X_1\) or \(X_2\). To
  illustrate this idea, compare a few existing methods, and explore
  methods of inference on tree based models three datasets will be
  simulated with different correlation structures. We will be focused more
  on the correlation structure between the predictors than on their
  relationships with the response and this will be reflected in the
  simulations.
  
  ~~~~~To aid in comparisons between the methods, one of the simulated
  datasets considered in this paper will be generated from the same method
  as used in (Strobl et al, 2008???). Under this method, the 13 x 1000
  data set, \(D_1\), has 12 predictors, \(V_1,..,V_{12}\), where
  \(V_j \sim N(0,1)\). The first four are, however, block correlated to
  each other with \(\rho = .9\). They are related to \(Y\) by the linear
  equation:
  \[Y = 5 \cdot V_1 + 5 \cdot V_2 + 2 \cdot V_3 + 0 \cdot V_4 + -5 \cdot V_5 + -5\cdot V_6 + 0\cdot V_7 + 0 \cdot ..... + E, E \sim N(0,\frac 1 2 )\]
  Note that the coefficients for \(V_7,...,V_{12}\) are all zero.
  
  \subsubsection{\texorpdfstring{Table 1: Correlation of \(V_1,..., V_7\)
  and
  \(Y\)}{Table 1: Correlation of V\_1,..., V\_7 and Y}}\label{table-1-correlation-of-v_1...-v_7-and-y}
  
  \begin{tabular}{l|r|r|r|r|r|r|r|r|r}
  \hline
    & V1 & V2 & V3 & V4 & V5 & V6 & V7 & y & beta\\
  \hline
  V1 & 1.000 & 0.915 & 0.908 & 0.907 & -0.034 & 0.006 & 0.012 & 0.829 & 5\\
  \hline
  V2 & 0.915 & 1.000 & 0.914 & 0.914 & -0.020 & -0.001 & -0.001 & 0.830 & 5\\
  \hline
  V3 & 0.908 & 0.914 & 1.000 & 0.903 & -0.017 & -0.007 & 0.007 & 0.808 & 2\\
  \hline
  V4 & 0.907 & 0.914 & 0.903 & 1.000 & -0.002 & -0.015 & 0.023 & 0.789 & 0\\
  \hline
  V5 & -0.034 & -0.020 & -0.017 & -0.002 & 1.000 & 0.044 & 0.005 & -0.388 & -5\\
  \hline
  V6 & 0.006 & -0.001 & -0.007 & -0.015 & 0.044 & 1.000 & -0.005 & -0.364 & -5\\
  \hline
  V7 & 0.012 & -0.001 & 0.007 & 0.023 & 0.005 & -0.005 & 1.000 & -0.141 & -2\\
  \hline
  \end{tabular}
  
  ~~~~~As can be seen from the last column in the table, ``beta'',
  although \(V4\) was not included in the model \(Y \sim V1,..V_{12}\),
  its' strong correlation with more influential predictors \(V_1,...,V_3\)
  insures that it still shows a strong linear correlation with \(Y\). A
  linear model would likely \emph{overstate} the effect of \(V_4\) on
  \(Y\).\footnote{A great deal of effort was undertaken by the author to
    find the definitive, authentic CART algorithm. This implementation
    follows the rough strokes set out in the 1984 text
    \emph{Classification and Regression Trees} to the best of the author's
    ability and may not be exactly the algorithm found in R packages like
    `tree()'}\footnote{Another point that could be said is that, no
    \(V_4\) is not unimportant, \(V_1, V_2,\) and \(V_3\) are just stand
    ins for the real star, \(V_4\), as they are nearly the same
    (\(\rho \sim 1\)). Then the real relationship represented here is
    \(Y \sim (5 + 5 + 2) \cdot V_4 + -5 \cdot V_5 + -5 \cdot V_6 + -2 \cdot V_7\).
    This model is not unsuccessful in capturing the structure of the data,
    and this is typically the practice used to model data with highly
    correlated predictors. If this seems philosophically satisfying to
    you, the rest of this thesis may seem a bit inconsequential. I
    apologize.}
  
  \begin{figure}[htbp]
  \centering
  \includegraphics{Thesis_files/figure-latex/denv1v5-1.pdf}
  \caption{\label{fig:denv1v5}Density Graphs for V1 through V5}
  \end{figure}
  
  ~~~~~ As can be seen above in Figure 1 the densities of \(V_1,...,V_5\)
  are all very similar due to the way they were generated.
  
  \begin{figure}[htbp]
  \centering
  \includegraphics{Thesis_files/figure-latex/yv4-1.pdf}
  \caption{\label{fig:yv4}Plot of Y \textasciitilde{} V4, Correlation = .789}
  \end{figure}
  
  ~~~~~ Figure 2 is an illustration of the relationship between
  \(Y\sim V_4\) with linear correlation of .789.
  
  ~~~~~While \(D_1\) represents a situation with linear correlation
  between the predictors, \(D_2\) does not. Here, the model is the same,
  \(Y~X_1,...,X_12\) where \(Y\) is generated according to the equation:
  \[Y = 5 \cdot X_1 + 5 \cdot X_2 + 2 \cdot X_3 + 0 \cdot X_4 + -5 \cdot X_5 + -5\cdot X_6 + 0\cdot X_7 + 0 \cdot ..... + E, E \sim N(0,\frac 1 2 )\]
  
  However, instead of block correlation with \(\rho = .9\), four variables
  are related to each other by the equations below. Note that
  \(X_1, X_5,...,X_{12} ~ N(0,1)\)
  \[X_2 = X_1 + E, E \sim Exponential(1)\]
  \[X_3 = X_2 + E, E \sim Exponential(1)\]
  \[X_4 = X_3 + E, E \sim Exponential(1)\]
  
  \subsubsection{\texorpdfstring{Table 2: Correlation of \(X_1,..., X_7\)
  and
  \(Y\)}{Table 2: Correlation of X\_1,..., X\_7 and Y}}\label{table-2-correlation-of-x_1...-x_7-and-y}
  
  \begin{tabular}{l|r|r|r|r|r|r|r|r|r}
  \hline
    & X1 & X2 & X3 & X4 & X5 & X6 & X7 & y & beta\\
  \hline
  X1 & 1.000 & 0.693 & 0.605 & 0.552 & -0.043 & 0.009 & -0.006 & 0.760 & 5\\
  \hline
  X2 & 0.693 & 1.000 & 0.847 & 0.745 & 0.004 & 0.006 & -0.018 & 0.845 & 5\\
  \hline
  X3 & 0.605 & 0.847 & 1.000 & 0.877 & 0.007 & 0.005 & -0.024 & 0.785 & 2\\
  \hline
  X4 & 0.552 & 0.745 & 0.877 & 1.000 & 0.011 & 0.006 & -0.032 & 0.696 & 0\\
  \hline
  X5 & -0.043 & 0.004 & 0.007 & 0.011 & 1.000 & -0.008 & 0.020 & -0.318 & -5\\
  \hline
  X6 & 0.009 & 0.006 & 0.005 & 0.006 & -0.008 & 1.000 & -0.046 & -0.310 & -5\\
  \hline
  X7 & -0.006 & -0.018 & -0.024 & -0.032 & 0.020 & -0.046 & 1.000 & -0.133 & -2\\
  \hline
  \end{tabular}
  
  ~~~~~As one can see, Table 2 mirrors Table 1. For this dataset, however,
  the correlation structure is more complicated. \(X_1\) and \(X_2\) are
  highly correlated with \(\rho = .7\).
  
  \begin{figure}[htbp]
  \centering
  \includegraphics{Thesis_files/figure-latex/x2x2-1.pdf}
  \caption{\label{fig:x2x2}Plot of X2\textasciitilde{}X1, Correlation = .7}
  \end{figure}
  
  \begin{figure}[htbp]
  \centering
  \includegraphics{Thesis_files/figure-latex/compsD2-1.pdf}
  \caption{\label{fig:compsD2}Correlation Structure of the First Four
  Variables in D2}
  \end{figure}
  
  ~~~~~As seen in Figure 4, the pattern observed between \(X_1\) and
  \(X_2\) does not carry over to the other correlated predictors.
  
  \begin{figure}[htbp]
  \centering
  \includegraphics{Thesis_files/figure-latex/densityD2-1.pdf}
  \caption{\label{fig:densityD2}Comparisons of the Density Graphs for X1
  through X5}
  \end{figure}
  
  ~~~~~Figure 5 demonstrate how the correlation between a few of the
  predictors and \(Y\) may be effected by slope. Scale is much more a
  factor in this dataset, with some variables like \(X_3\) having a larger
  range than the variables \(X_1 \sim N(0,1)\) or
  \(X_5,...,X_{12} \sim MVN()\).
  
  ~~~~~The last dataset we'll consider is \(D_3\), a data set with even
  more non-linear relationships between the first four variables.
  Otherwise it is very similar to both \(D_1\) and \(D_2\). The first four
  variables are generated as follows:
  
  \[\omega_1 \sim N(1,0)\] \[\omega_2 = log(\omega_1) + E, E \sim N(1,0)\]
  \[\omega_3 = log(\omega_2) + E, E \sim N(1,0)\]
  \[\omega_4 = log(\omega_4) + E, E \sim N(1,0)\]
  
  \begin{tabular}{l|r|r|r|r|r|r|r|r|r}
  \hline
    & W1 & W2 & W3 & W4 & W5 & W6 & W7 & y & beta\\
  \hline
  W1 & 1.000 & -0.056 & -0.040 & 0.041 & 0.002 & -0.034 & -0.028 & 0.322 & 5\\
  \hline
  W2 & -0.056 & 1.000 & -0.533 & -0.279 & -0.002 & 0.049 & -0.003 & 0.668 & 5\\
  \hline
  W3 & -0.040 & -0.533 & 1.000 & -0.002 & -0.019 & -0.031 & -0.010 & -0.096 & 2\\
  \hline
  W4 & 0.041 & -0.279 & -0.002 & 1.000 & -0.007 & -0.008 & -0.079 & -0.223 & 0\\
  \hline
  W5 & 0.002 & -0.002 & -0.019 & -0.007 & 1.000 & -0.012 & -0.019 & -0.382 & -5\\
  \hline
  W6 & -0.034 & 0.049 & -0.031 & -0.008 & -0.012 & 1.000 & 0.004 & -0.358 & -5\\
  \hline
  W7 & -0.028 & -0.003 & -0.010 & -0.079 & -0.019 & 0.004 & 1.000 & -0.159 & -2\\
  \hline
  \end{tabular}
  
  ~~~~~The linear correlation structure in \(D_3\) is not as striking as
  in \(D_1\). The two strongest linear relationships are between
  \(\omega_2\) and \(\omega_3\) with \(\rho = -.534\) and between \(Y\)
  and \(\omega_2\) with \(\rho = .700\).
  
  \begin{figure}[htbp]
  \centering
  \includegraphics{Thesis_files/figure-latex/corstructD3-1.pdf}
  \caption{\label{fig:corstructD3}Correlation Structure of the First Four
  Variables in D3}
  \end{figure}
  
  ~~~~~Figure 6 provides another way of visualizing some of the
  information given in Table 3. Here we can see the densities as well as
  the paired correlations of the first four variables in \(D_3\).
  
  \begin{figure}[htbp]
  \centering
  \includegraphics{Thesis_files/figure-latex/compw1w5-1.pdf}
  \caption{\label{fig:compw1w5}Comparisons of the Density Graphs for W1
  through W5}
  \end{figure}
  
  ~~~~~There is more variation between the densities of
  \(\omega_1,...,\omega_5\) then we have seen in the other data sets.
  \(\omega_2,\omega_3,\) and \(\omega_4\) have greater spread than their
  counterparts that are generated under the normal distribution.
  
  \begin{figure}[htbp]
  \centering
  \includegraphics{Thesis_files/figure-latex/yw2-1.pdf}
  \caption{\label{fig:yw2}Plot of Y\textasciitilde{}W2, Correlation = .7}
  \end{figure}
  
  ~~~~~ As the relationship between \(Y\) and \(\omega_2\) was so
  striking, it is nice to see a scatter plot that represents it.
  
  \section{Models and Comparisons}\label{models-and-comparisons}
  
  \subsubsection{CART: Regression Trees}\label{cart-regression-trees}
  
  ~~~~~As outlined in the 1984 textbook, \emph{Classification and
  Regression Trees}, Brieman, Friedman, Olshen, and Stone described their
  method for creating, pruning, and testing regression trees. There are
  essentially three steps: one, decide on a variable to split over, two,
  partition that variable space in two distinct partitions, and three, set
  our initial predictions for each partition to be mean value of the
  response according to the observed responses corresponding to the values
  in the partitions. Recursively, this process is repeated for each new
  partition until some stopping condition is reached.This is a top down,
  greedy algorithm that functions by creating as large a tree as possible
  and then is pruned down to prevent over fitting.
  
  \begin{figure}[htbp]
  \centering
  \includegraphics{Thesis_files/figure-latex/carts-1.pdf}
  \caption{\label{fig:carts}CART for the Model Y\textasciitilde{}, from D1,D2,
  and D3}
  \end{figure}
  
  ~~~~~Trees can be quite variable, so to get a better idea of the
  differences between the methods let's run a simulation.
  
  \begin{algorithm}
  \caption{Simulation Scheme 2.1}
  \label{sim2.1}
  \begin{algorithmic}[1]
  \For{$i \leq 1000$ }
  \State Randomly sample $\frac 2 3$  of the observations in  $D_2$  to a training set,  $D_{2, train}^i$. The other observations,  $x \in D_2, x \notin D_{2, train}^i$ form the testing set $D_{2, test}^i$
  \State Fit a tree, $T^i$, to the data under the model $Y \sim X_1,...,X_2$ using the observations in      $D_{2}^i$
  \State Calculate the $MSE_{test}$ of the model using the equation:
      $MSE_{test} = \frac 1 n \sum (y_j - \hat{y_j})^2$
  \EndFor
  \end{algorithmic}
  \end{algorithm}
  
  ~~~~~Where \(n\) is the number of observations in \(D_{2, test}^i\),
  \(y_j \in D_{2, test}^i, \hat{y_j} \in T^i(D_{2, test}^i)\) for
  \(1 \leq j \leq n\) This produces two distributions of \(MSE_{test}\),
  one for CART and one for CTree, conditional inference trees.
  
  \begin{figure}[htbp]
  \centering
  \includegraphics{Thesis_files/figure-latex/cartmse-1.pdf}
  \caption{\label{fig:cartmse}Comparison of the Simulated MSEtest
  Distributions of CART}
  \end{figure}
  
  ~~~~~The distribution of 1000 CART trees' \(MSE_{test}\) is roughly
  normal with a variance of \texttt{var(testmseC)}.
  
  \section{Bagged Forests}\label{bagged-forests}
  
  ~~~~~As one can see in the Figure 10, there is a fair amount of
  variability in a single tree, they are heavily dependent on fluctuations
  in the starting data set. As mention briefly in the introduction, bagged
  forests present one solution to this problem. To create a bagged forest,
  as outlined in \emph{An Introduction to Statistical Learning} by James,
  Witten, Hastie and Tibshirani, 2013, many bootstrapped samples are taken
  from the initial dataset and trees are fitted to them. The final
  predictions are, then, averaged over all of the trees. This ensures that
  while each tree has high variance, when they are aggregated the variance
  will decrease.
  
  ~~~~~Let's put that to the test here using our dataset \(D3\) again.
  We'll build 100 forests of 100 trees each and compare the variability of
  the \(MSE\) distributions.
  
  \begin{figure}[htbp]
  \centering
  \includegraphics{Thesis_files/figure-latex/baggedvcart-1.pdf}
  \caption{\label{fig:baggedvcart}the Simulated MSEtest Distributions of
  Bagged Forests and CART}
  \end{figure}
  
  ~~~~~As one can see, the values of \(MSE_{test}\) for the bagged forest
  were entirely below the \(MSE_test\) for the trees and the variance was
  much smaller.
  
  \section{Random Forests}\label{random-forests}
  
  \emph{EXPAND AND BEGIN NOTATION OF R AS RANDOM FOREST}
  
  As the number of trees grown in each forest increases, the
  \(MSE_{test}\) decreases (cite). Still, this can become computationally
  intensive on larger data sets where we would like very accurate models.
  Random forests are often seen as a solution to this problem. In a bagged
  forest, every variable is considered when each split is made but in a
  random forest only \(mtry, mtry \leq p\) are considered. This allows us
  to assume that the trees have a level of independence not found in
  bagged forests, and that a small random forest will often out perform
  the bagged forest.
  
  For an illustration, let's build a random forest on \(D3\) and compare
  the \(MSE\).
  
  \begin{figure}[htbp]
  \centering
  \includegraphics{Thesis_files/figure-latex/baggedvcartvforest-1.pdf}
  \caption{\label{fig:baggedvcartvforest}Simulated MSEtest Distributions of
  CART, Random, and Bagged Forests}
  \end{figure}
  
  \chapter{Random Forest Variable
  Importance}\label{random-forest-variable-importance}
  
  \section{Breiman et al Introduce Permuted Variable Importance
  (1984)}\label{breiman-et-al-introduce-permuted-variable-importance-1984}
  
  \subsection{Variable Importance on a Single
  Tree}\label{variable-importance-on-a-single-tree}
  
  Breiman et al in \emph{Classification and Regression Trees} (1984)
  propose a method for variable importance for individual trees that stems
  from their definition of \(\tilde{s}\), a surrogate split. Surrogate
  splits help Brieman et al deal with several common problems one may
  have: modeling with missing data, diagnosing masking, and variable
  importance. They are defined using logic that resembles that behind
  random forests.
  
  Definitions
  
  Assume the standard structure for tree models. Let \(D\) be the dataset
  composed of \(D = {Y, X_1,...X_p}\), where the model we would like to
  estimate is of the form \(T: Y \sim X_1,...X_p\). For any node
  \(t \in T(D)\), \(s*\) is the best split of the node into daughters
  \(t_r\) and \(t_l\). Take \(X_i \in D\) and let \(S_i\) be the set of
  all of the splits on \(X_i\) in \(T\). Then set \(\bar{S_i}\) equal to
  the complement of \(S_i\), \(\bar{S_i} = S_i^c\). For any possible split
  \(s_i \in S_i \cup \bar{S_i}\), \(s_i\) will split the node \(t\) into
  two daughters, \(t_{i,l}\) and \(t_{i,r}\). Count the number of times
  that \(s*\) and \(s_i\), while splitting differently, generate the same
  left daughter \(t_{l}\) as \(N_{LL}\) and the number of times they
  generate the same right daughter as \(N_{RR}\). Then the probability
  that a case falls within \(t_L \cap t'_L\) is
  \(P(t_L \cap t'_L) = \sum_j \frac{\pi(j) N_j(LL)}{N_j}\) and the
  probability that a case falls within \(t_R \cap t'_R\) is
  \(P(t_R \cap t'_R) = \sum_j \frac{\pi(j) N_j(RR)}{N_j}\). Where
  \(\pi(j)\) is the prior assumed for the the jth variable.Finally, the
  probability that a surrogate split predicts \(s*\) is
  \(P(s*, s_M) = (t_R \cap t'_R) + P(t_L \cap t'_L)\). Then the surrogate
  split is the value of \(s*\) that maximizes this probability. It is
  denoted \(\tilde{s}\)
  
  A surrogate split \(\tilde{s}\),is one that estimates the best possible
  univariate split \(s*\) on node \(t\).
  
  \textbf{Defintion: Variable Importance, Single Tree}
  
  \[VI_{tree}(X_i, T) = \sum_{t \in T} \Delta RSS(\tilde{s_i}, t)\] Or the
  decrease of RSS attributable to \(X_i\) across the tree \(T\). In
  \emph{Classification and Regression Trees}, Brieman et al, outline
  several potential problems with this method that the do not attempt to
  solve. First, that this is only one of a number of reasonable ways to
  define variable importance. Second, the variable importances for
  variables \(X_1,..,X_p\) can be effected by outliers or random
  fluctuations within the data. (Ch 5.3)
  
  \subsection{Variable Importance for a Random
  Forest}\label{variable-importance-for-a-random-forest}
  
  One way to define variable importance for a random forest follows
  directly from Breiman et al's definition for a single tree. Recall that
  each tree in a random forest is fit to a bootstrapped sample of the
  original observations. To estimate the test error, therefor, no cross
  validation is needed - each tree is simply tested against the test set
  of observations that were not in that tree's initial training set. To
  determine variable importance for a predictor \(X_j\), we look at the
  RSS of the each tree's prediction that did not split on \(X_j\). These
  values are then averaged over the subset forest that did not include
  \(X_j\). A large value would imply that in trees that included \(X_j\),
  the predictive capabilities were increased.
  
  To formalize that idea, let's refer to the set of trees that did not
  consider \(X_j\), \(T_{x_j}^c\). Now, \(T_{x_j}^c \subset R\), the
  random forest. The subset of the original data that will be tested on
  each tree, \(t\), is \(\bar{B}^t\). The dimensions of \(\bar{B}^t\) are
  \(\nu_t\) x \(p\), where \(p\) is the number of predictors and
  \(\nu \leq n\). The number of trees in \(T_{x_j}^c\) is \(\mu\) where
  \(\mu \leq ntree\)
  
  Now, base variable importance is:
  
  \[VI_{\alpha}(X_j, R) =  \sum_{t \in T_{x_j}^c} \frac 1 {\nu_t} RSS(t,\bar{B}_t)\]
  
  However, this method poses some problems. Namely, while variable
  importance for random forests is more stable than for the variable
  importance values for CART, (this is because the model is less variable
  in general), it is lacking the traditional inferential capabilities of
  other regression models. In an effort to derive a p-value for variable
  importance values, Breiman 2001b, describes a \emph{permuted variable
  importance} or \(VI_{\beta}\) that does not utilize \(T_{x_j}^c\).
  
  \begin{algorithm}
  \caption{Permuted Variable Importance for Random Forests, $VI_{\beta}$}
  \label{breiman}
  \begin{algorithmic}
  \State Fit a random forest, $R$ on the dataset $D$ fitting the model $Y \sim X_1,...,X_p$.
  \For{each $X_i \in {{X_1,...,X_p}}$}
  \For{each $t \in R$}
  \State Calculate: $\Phi_o =  \frac 1 {\nu_t} RSS(t,\bar{B}^t)$
  \State Permute $X_i$. Now find $\Phi^* =  \frac 1 {\nu_t} RSS(t,\bar{B}_t^*)$
  \State The difference between these values, $\Phi^* - \Phi_o$,  is the variable importance for $X_j$ on $t$,  
  \EndFor
  \State Average over all $t \in R$ 
   $$VI_{\beta}(X_j) = \frac 1 {ntree} \sum^{ntree} \Phi^* - \Phi_o$$
   $$VI_{\beta}(X_j) = \frac 1 {ntree} \sum^{ntree} \frac 1 {\nu_t} RSS(t,\bar{B}_t^*) - \frac 1 {\nu_t} RSS(t,\bar{B}^t)$$
  \EndFor
  \end{algorithmic}
  \end{algorithm}
  
  Again, a large variable importance value suggests that \(X_j\) is a
  valuable predictor for the model.
  
  \section{Strobl et al Respond (2008)}\label{strobl-et-al-respond-2008}
  
  Strobl et al (2008) respond to Breiman's method with one main argument:
  the null hypothesis implied by the permutation distribution utilized in
  permuted variable importance is that \(X_i\) is independent of \(Y\)
  \textbf{and} \({X_j \notin X_1,...,X_p}\) so the null hypothesis will be
  rejected in the case where \(X_j\) is independent of \(Y\) but not some
  subset of the other predictors. As correlation among the predictors is
  very common in data sets that are used for random forests, this is a
  large problem for Breiman's method.
  
  To alleviate this difficulty, Strobl et al propose a permutation scheme
  under the null hypothesis that \(X_j\) given it's relationship with the
  other predictors is independent of \(Y\).
  
  \begin{algorithm}
  \caption{Conditional Variable Importance for Random Forests, $VI_{\gamma}$}
  \label{strobl}
  \begin{algorithmic}[1]
  \State Fit a random forest, $R$ on the dataset $D$ fitting the model $Y \sim X_1,...,X_p$.
  \For{each $X_i \in {{X_1,...,X_p}}$}
  \For{each $t \in R$}
  \State Calculate: $\Psi_o =  \frac 1 {\nu_t} RSS(t,\bar{B}^t)$
  \State Permute $X_i$ according to the partitions on $X_j$ from $t$ (see notes below on this step)
  Now find $\Psi^* =  \frac 1 {\nu_t} RSS(t,\bar{B}_t^*)$
  \State The difference between these values, $\Psi^* - \Psi_o$,  is the variable importance for $X_j$ on $t$,  
  \EndFor
  \State Average over all $t \in R$ 
   $$VI_{\gamma}(X_i,R) = \frac 1 {ntree} \sum^{ntree} \Psi^* - \Psi_o$$
   $$VI_{\gamma}(X_i, R) = \frac 1 {ntree} \sum^{ntree} \frac 1 {\nu_t} RSS(t,\bar{B}_t^*) - \frac 1 {\nu_t} RSS(t,\bar{B}^t)$$
  \EndFor
  \end{algorithmic}
  \end{algorithm}
  
  As an example for \(5\) in the algorithm above, consider the dataset
  \(D_{3,lite}\), where \(D_{3,lite} = {\omega_1, \omega_2, y}\) where
  \(D_{3,lite}\) has dimensions \(n = 100\), \(p = 2\). A simple CART tree
  with 5 splits on the model \(Y\) \textasciitilde{}\(\omega_1, \omega_2\)
  
  \includegraphics{Thesis_files/figure-latex/unnamed-chunk-5-1.pdf}
  \includegraphics{Thesis_files/figure-latex/unnamed-chunk-5-2.pdf}
  
  \begin{tabular}{l|l|l|l|l|l}
  \hline
  Group & Predicted Value of Y & Min(W2) & Max(W2) & Min(W1) & Max(W1)\\
  \hline
  A & -22.5362734865312 & -10.43 & -4.35 & -0.12 & 0.27\\
  \hline
  B & -10.5421878802014 & -4.34 & 0.33 & -1.86 & 0.14\\
  \hline
  C & -2.98460142826064 & -3.96 & 0.28 & 0.17 & 1.93\\
  \hline
  D & 1.89361038304057 & 0.36 & 2.58 & -2.25 & 1.09\\
  \hline
  E & 17.661072262451 & 0.47 & 3.59 & 1.09 & 2.31\\
  \hline
  \end{tabular}
  
  If we permute the \(\omega_1\) values in group \(B\), this is what that
  plot looks like:
  
  \includegraphics{Thesis_files/figure-latex/unnamed-chunk-6-1.pdf}
  
  \section{Inferential Variable
  Importance}\label{inferential-variable-importance}
  
  This thesis hopes to be a response to conditional variable importance as
  outlined by Strobl et al 2008. First is that the practice of permuting
  given the partitions from the model \(Y \sim X_1,...,X_p\) instead of
  \(X_j \sim X_1,..,X_p\). This procedure is reminisent of Breiman et al's
  notions of grouped predictors in the book \emph{Classification and
  Regression Trees}.
  
  \chapter{INFTrees and INFFOREST Variable
  Importance}\label{inftrees-and-infforest-variable-importance}
  
  \section{Theory}\label{theory}
  
  While conditional variable importance (Strobl et al) conditionally
  permutes each variable given the structure signified by the model that
  predicts the response, \(Y \sim X_1,...,X_i,...,X_p\), our method
  conditionally permutes each variable given the structure outlined in a
  new model with the variable of interest as the response,
  \(X_i \sim X_1,...X_{i-1},X_{i+1},...X_p\). This is not the most
  straightforward process, as trees partition the sample space, however,
  in INFTrees these partitions on the variables
  \(X_1,...X_{i-1},X_{i+1},...X_p\) are treated as pseudo partitions on
  the variable of interest, \(X_i\). This is accomplished by first
  partitioning on the sample predictors \(X_1,...X_{i-1},X_{i+1},...X_p\)
  and then inferring the partitions on \(X_i\).
  
  *\textbf{ADD BETTER PLOT FOR EXAMPLE}
  
  \subsection{INFTrees}\label{inftrees}
  
  For a CART, \(T\), representing the model \(Y~X_1,...,X_p\) where
  \(Y,X_1,...,X_p\) are vectors of length n, the INFTrees algorithm
  proceeds as follows:
  
  \begin{algorithm}
  \caption{INFTree, $VI_{inf}(T)$}
  \label{inftree}
  \begin{algorithmic}
  \For{each $X_i \in {{X_1,...,X_p}}$}
  \State Calculate: $\Phi_o =  RSS(T, (Y,X_1,..X_p))$
  \State Fit the tree $T_{X_i}$, where $T_{X_i} : X_i \sim X_1,...,X_{i-1}, X_{i+1},...X_p$
  \State Extract the set $P_{X_i}$ of partitions on $X_i$ from $T_{X_i}$
  \State Permute $X_i$ with respect to $P_{X_i}$
  \State Find $\Phi^* =  RSS(T, (Y,X_1,..., \bar{X_i},...X_p))$
  \State Repeat the above procedure to find the distribution of $\Phi^*$
  \State Test the null hypothesis that $\Phi_o$ is the likely value of $RSS(T, (Y,X_1,..X_p))$
  \EndFor
  \end{algorithmic}
  \end{algorithm}
  
  This procedure allows the null hypothesis that Y is independent of
  \(X_i\) given the values of \(X_1,...X_{i-1},X_{i+1},...X_p\) to be
  tested. Therefor, values of \(VI_{inf}\) could be compared in a similar
  manner to the coefficients of linear regression.
  
  \subsection{INFForests}\label{infforests}
  
  The algorithm for determining \(VI_{inf}(R)\) follows similarly.
  
  \begin{algorithm}
  \caption{INFForests, $VI_{inf}(R)$}
  \label{infforest}
  \begin{algorithmic}[1]
  \State Fit a random forest, $R$ on the dataset $D$ fitting the model $Y \sim X_1,...,X_p$.
  \For{each $X_i \in {{X_1,...,X_p}}$}
  \For{each $t \in R$}
  \State Calculate: $\Xi_o =  \frac 1 {\nu_t} RSS(t,\bar{B}^t)$
  \State Calculate a tree $T_i$ that predicts $X_i \sim X_1,...,X_{i-1}, X_{i+1},...X_p$ using the subset of the observations used to fit $t$  
  \State Permute the subset of $X_i$ contained in $\bar{B}_t$ with respect to the set of partitions $P_{xi}$ from $T_i$.
  \State Now find $\Xi^* =  \frac 1 {\nu_t} RSS(t,\bar{B}_t^*)$
  \State The difference between these values, $\Xi^* - \Xi_o$,  is the variable importance for $X_i$ on $t$
  \EndFor
  \State Test the null hypothesis that $\Xi_o$ is the likely value of $\frac 1 {\nu_t} RSS(t,\bar{B}_t^*)$ using the distribution of values of $\Xi^*$ gathered from each tree in $R$
  \EndFor
  \end{algorithmic}
  \end{algorithm}
  
  \section{\texorpdfstring{Implementation In \texttt{INFTREES} and
  Results}{Implementation In INFTREES and Results}}\label{implementation-in-inftrees-and-results}
  
  \subsection{Notes on the
  Implemetation}\label{notes-on-the-implemetation}
  
  Implementing the \texttt{INFFOREST} and therefor the \texttt{INFTREES}
  algorithms, required creating a suite of functions to create trees and
  random forests. The trees are fit following the standard two-part
  CART-like algorithm.\footnote{A great deal of effort was undertaken by
    the author to find the definitive, authentic CART algorithm. This
    implementation follows the rough strokes set out in the 1984 text
    \emph{Classification and Regression Trees} to the best of the author's
    ability and may not be exactly the algorithm found in R packages like
    `tree()'} The function chooses a variable to split on with linear
  correlation with respect to \(Y\), but instead of looking for
  correlations above a certain threshold which is common, it chooses the
  variable with the highest correlation when compared to its peers. This
  alleviates the situation where a variable with a non-linear relationship
  would be passed over again and again. The splitting is done via
  minimization of the following function with respect to \(i\):
  
  \[RSS_{node} (i,X,Y) = RSS_{leaf}(Y|X <i) + RSS_{leaf}(Y|X \geq i) \]
  \[RSS_{leaf} = \sum (y - \hat{y})^2 \]
  \[\hat{Y}: \hat{y} \in \hat{Y}: \hat{y} = E(Y), \ where\  |\hat{Y}| = |Y|\]
  
  This function considers the regression case only, and only numeric
  predictors. Leafs are created when the resultant split would be
  unsatisfactory, i.e.~at least one daughter node would have five members
  or less. This generates very large trees - a quality that is not an
  issue in random forests but may be problematic in a stand-alone setting.
  At this time, there is also no function to prune the trees.
  
  \subsubsection{\texorpdfstring{Table \_\_: A Home-Grown Tree on
  \(Y~X_1+X_2+X_3+X_4\)}{Table \_\_: A Home-Grown Tree on Y\textasciitilde{}X\_1+X\_2+X\_3+X\_4}}\label{table-__-a-home-grown-tree-on-yx_1x_2x_3x_4}
  
  \begin{tabular}{l|l|l|l|l}
  \hline
  var & n & dev & ypred & split.cutleft\\
  \hline
  X2 & 100 & 14555.7572003193 & -3.0976184477862 & 0.725094920809497\\
  \hline
  X2 & 59 & 5609.64655621111 & 6.01005019872487 & 3.28899699394128\\
  \hline
  <leaf> & 8 & 375.896813195261 & 26.6561770276355 & 0\\
  \hline
  X2 & 51 & 3049.31027720886 & 2.77144206869967 & 1.62893665728107\\
  \hline
  X1 & 26 & 1156.80610732382 & 9.18897712062308 & 1.04172636035769\\
  \hline
  <leaf> & 6 & 124.301193003857 & 15.4053521498218 & 0\\
  \hline
  <leaf> & 20 & 1032.50491431997 & 7.32406461186345 & 0\\
  \hline
  X1 & 25 & 1115.45728514695 & -3.90279438530068 & -0.382523775604218\\
  \hline
  <leaf> & 18 & 731.748867342283 & -1.18274413949534 & 0\\
  \hline
  <leaf> & 7 & 383.708417804669 & -10.8972093030858 & 0\\
  \hline
  X2 & 41 & 2207.5570882574 & -16.2037757683753 & -0.554560357455375\\
  \hline
  X1 & 26 & 749.886162768674 & -9.93411391726191 & -0.252980429197639\\
  \hline
  <leaf> & 10 & 131.02611270747 & -4.17919192000313 & 0\\
  \hline
  <leaf> & 16 & 618.860050061205 & -13.5309401655486 & 0\\
  \hline
  <leaf> & 15 & 919.485108577565 & -27.0711896436385 & 0\\
  \hline
  \end{tabular}
  
  The INFTREE function follows the algorithm above \emph{reference}. The
  partitions on \(X_j\) are generated by fitting a tree, \(T\), to the
  model \(X_j \sim X_1,..., X_{j-1}, X_{j+1},..X_p\) and calculating the
  predictions \(T(X_1,..., X_{j-1}, X_{j+1},..X_p)\). Then permuting
  \(X_j\) with respect to the partitions on \(X_j\) given by those
  predictions. For example, if \(x_j \in X_j\) and the value of
  \(T(x_1,..., x_{j-1}, x_{j+1},..x_p)\) corresponding to \(x_j\) is
  \(\alpha\), \(x_j\) is permuted along with the other values of \(X_j\)
  that also have \(T(x_1,..., x_{j-1}, x_{j+1},..x_p)\) corresponding to
  \(\alpha\).
  
  The values of \(INFFOREST(X_j)\) are scaled in the following way: since
  the INFFOREST function computes the INFFTREES, (or the difference in
  post and pre permutation RSS), values in a tree-wise manner, each tree's
  values are divided by the maximum value. This ensures that the values
  are between zero and one, and that in each tree one variable is clearly
  deemed the \emph{most important}.
  
  \subsection{Results}\label{results}
  
  \emph{NOTE} INFFOREST, like any random forest method involving tree-
  level calculations is a computationally intensive function. The forests
  are large, unpruned at any level, and INFFOREST takes time to compute.
  Because of this reason the datasets discussed in CH2 have been altered
  so that instead of 1000 x 13 dimensional datasets they are 400 x 13.
  This decreases computation time immensely. (\textbf{see figure \_\_\_ in
  appendix})
  
  \includegraphics{Thesis_files/figure-latex/figinfdists-1.pdf}
  \includegraphics{Thesis_files/figure-latex/figinfdists-2.pdf}
  \includegraphics{Thesis_files/figure-latex/figinfdists-3.pdf}
  
  In the situation where there is little correlation between the
  predictors, the distribution of the INFFOREST output is a sharp peak
  ending at one of the end points, zero or one. When there are, however,
  strong correlations between the predictor variables, and \texttt{mtry}
  is suitably large but smaller than \texttt{p}, the trees in the forest
  must decide between them. In these situations, the INFFOREST
  distribution is multimodal, with one peak at one end of the interval,
  \(INFFOREST(X_i) = 1\) and another when \(INFFOREST(X_i) = 0\).
  
  To demonstrate this situation, take the dataset \(D2\), as described
  above. In the random forest corresponding to this model, the variables
  \(X2\) and \(X3\) are considered substitutes for each other. In the
  trees where \(X2\) has \(INFFOREST = 1\), \(X3\) has \(INFFOREST <<\)
  and visa versa.
  
  \begin{verbatim}
  Using  as id variables
  Using  as id variables
  \end{verbatim}
  
  \includegraphics{Thesis_files/figure-latex/figx2x1inf-1.pdf}
  \includegraphics{Thesis_files/figure-latex/figx2x1inf-2.pdf}
  
  (i.e.~the INFFOREST distributions of X2 and X3 in the trees where
  X3\textless{} .5)
  
  Of course, one may be inclined to infer a p-value for the null
  hypothesis that \(INFFOREST = 0\) for each of these variables. This
  could be done straight-forwardly enough in situations where there is not
  strong multicolinearity within the predictors as the distributions are
  reliably half of the familiar bell shaped curve centered around either
  zero or one. It would be quite difficult, however, for INFFOREST alone
  to test the significance of the INFFOREST distribution corresponding to
  correlated, paired predictors and it may not makes sense to do so at
  all. \emph{talk with Andrew about fixing this?}
  
  \chapter*{Conclusion}\label{conclusion}
  \addcontentsline{toc}{chapter}{Conclusion}
  
  \setcounter{chapter}{4} \setcounter{section}{0}
  
  \section{INFFOREST Comparisons With Other
  Methods}\label{infforest-comparisons-with-other-methods}
  
  INFFOREST holds its own amongst the other methods described in Chapter
  4. The conditional permuted variable importance, when ran on the same
  random forest, had more difficulty parsing out the situation with paired
  variables than INFFOREST.
  
  \textbf{PLOT OF INFFOREST OUTPUT FOR LIL D2 NEXT TO COND INF FOR LIL D2}
  
  \emph{Why does this make sense though?}
  
  The permuted variable importance that operates without partitioned
  permuatations ignored the forth variable completely while setting the
  permuted variable importance for \(V2\) to the max value every time.
  Perhaps this
  
  \textbf{PLOT OF INFFOREST OUTPUT FOR LIL D2 NEXT TO PERM INF FOR LIL D2}
  
  In the simulations considered here, it is difficult to judge which
  method perfomed the best. In each simulation, the predictors are related
  to \(Y\) by a linear function where the first three, and the fifth
  through seventh variable had non-zero coefficents. Then, the first four
  are correlated (see @\ref{chap2}).
  
  \section{Data Modeling as a Journey You Take With Some Data You
  Love}\label{data-modeling-as-a-journey-you-take-with-some-data-you-love}
  
  This whole story began with a single node. By itself, a node is nothing
  but some of your data. It's an interval that could be the entire dataset
  or a small sample, but may not be clear what the next move should be.
  Trees are roadmaps through a dataset. Each node is a fork in the road,
  and each split points out the correct direction.
  
  \chapter{The Second Appendix: CTree}\label{the-second-appendix-ctree}
  
  \subsection{Conditional Inference
  Trees}\label{conditional-inference-trees}
  
  ~~~~~As mentioned in the introduction, CART has the tendency to bias
  towards variables with the most possible splits and overfitting. There
  is little head paid to statistical significance or general statistical
  theory. \emph{Conditional Inference Trees} are a method proposed by
  Horthon et al, 2006, that utilizes permutation theory to create and
  algorithm that is sensitive to these issues. A crutial difference
  between CTree and CART is that while CART is a top down algorithm, CTree
  initially assumes each row of the dataset is a node and then gradually
  prunes them.
  
  \begin{algorithm}
  \caption{Conditional Inference Trees}
  \label{ctree}
  \begin{algorithmic}[1]
  \For{$w_i, i \in \{{w_1,...,w_n}\}$}
  \State Test the global null hypothesis of independence between any of the $m$ covariates and the response. 
  \If{$H_O$ cannot be rejected} 
  \State Stop 
  \Else \State Select predictor $X_j$ with the strongest linear association to $Y$ 
  \EndIf
  \State Choose a set $A \in X_j$ such that $A \cup X_j \ A = A$ 
  \State The case weights, $w_{left}$ and $w_{right}$ are then defined as $w_{left,i} = w_i I (x_j \in X_j, \in A)$ and $w_{right,i} = w_i I(x_j \in X_j, x_j \notin A)$
  \EndFor
  \end{algorithmic}
  \end{algorithm}
  
  ~~~~~The case weights, \(w_i \in {w_1,..., w_n}\), correspond to nodes
  are defined as:\[w_i = I(x_i \in {N_t})\] ~~~~~Where \(x_i\) is a vector
  of observations and \(N_t\) is a node in the tree.
  
  \textbf{CTree fitted to \(D_3\)}
  
  \includegraphics{Thesis_files/figure-latex/ctree-1.pdf}
  \includegraphics{Thesis_files/figure-latex/ctree-2.pdf}
  
  \backmatter
  
  \chapter{References}\label{references}
  
  \noindent
  
  \setlength{\parindent}{-0.20in} \setlength{\leftskip}{0.20in}
  \setlength{\parskip}{8pt}
  
  \hypertarget{refs}{}
  \hypertarget{ref-angel2000}{}
  Angel, E. (2000). \emph{Interactive computer graphics : A top-down
  approach with opengl}. Boston, MA: Addison Wesley Longman.
  
  \hypertarget{ref-angel2001}{}
  Angel, E. (2001a). \emph{Batch-file computer graphics : A bottom-up
  approach with quicktime}. Boston, MA: Wesley Addison Longman.
  
  \hypertarget{ref-angel2002a}{}
  Angel, E. (2001b). \emph{Test second book by angel}. Boston, MA: Wesley
  Addison Longman.


  % Index?

\end{document}
