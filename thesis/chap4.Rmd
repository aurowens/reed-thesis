---
header-includes:
- \usepackage{amssymb,amsthm,amsmath}
- \usepackage{chemarr}
- \usepackage{algorithm}
- \usepackage{algpseudocode}
- \usepackage{pifont}
- \usepackage{float}
- \usepackage{tikz}

output: pdf_document
---

```{r include_reedtemplates_4, include = FALSE}
# This chunk ensures that the reedtemplates package is installed and loaded
# This reedtemplates package includes the template files for the thesis and also
# two functions used for labeling and referencing
if(!require(devtools))
  install.packages("devtools", repos = "http://cran.rstudio.com")

if(!require(reedtemplates)){
  library(devtools)
  devtools::install_github("ismayc/reedtemplates")
  }
library(reedtemplates)

library(tree)
library(reedtemplates)
library(MASS)
library(ggplot2)
library(randomForest)
library(maptree)
library(knitr)
library(GGally)
library(reshape)
library(ggplot2)
library(plotly)
library(gridExtra)

#read_chunk('~/Desktop/Thesis/Thesis/infrf.script.v2.R')

#flights <- read.csv("data/flights.csv")
thesis <- c("#245e67", "#90bd98", "#cfd0a0", "#c49e46", "#d28383")
```

```{r infforestcode, echo=FALSE}
###################################GROWING A TREE#########################################
rss.node <- function(i,y,x) { 
  yleftpred <- mean(y[x< i])
  yrightpred <- mean(y[x >= i])
  rssl <- sum((y[x< i]-yleftpred)^2)
  rssr <- sum((y[x >= i] - yrightpred)^2)
  return(rssl + rssr)
}

rss.leaf <- function(a){
  a1 <- rep(mean(a), length(a))
  sum((a-a1)^2)
}

max.cor <- function(yy,sxs){
  
  max <- list()
  cors <- cbind(rep(0,ncol(sxs)),seq(1,ncol(sxs)))
  for(i in 1:ncol(sxs)){
    cors[i,1] <- ifelse(sd(sxs[,i]) == 0,0, suppressWarnings(cor(yy, sxs[,i])))
  }
  
  cors[is.na(cors[,1]), 1] <- 0
  
  if (sum(cors[,1] == max(abs(cors[,1]), na.rm = TRUE), na.rm = TRUE) > 1) {
    
    if(sum(abs(cors[,1])) == 0){
      
      return(NULL)
    }
    max1 <- sample(as.numeric(cors[,1] == max(abs(cors[,1]), na.rm = TRUE)), 1)  
    
  } else {
    
    max1 <- cors[abs(cors[,1]) == max(abs(cors[,1]), na.rm = TRUE),2]
  }
  
  
  max[[length(max)+1]] <- sxs[,max1]
  max[[length(max)+1]] <- names(sxs)[max1]
  return(max)
  
}

split.rf <- function(y,x, min, parent_rss) {
  
  sp <- list()
  if(length(x) <= min) {
    
    return(NULL)
  } else {
    
    #gives a warning if it cannot find an optimal solution and instead gives the max point
    op.partition <- suppressWarnings(optimise(rss.node, interval = range(x), 
                                              upper = max(x), y=y,x=x, maximum = FALSE))
    
    # if(parent_rss < rss.leaf( y[x < op.partition[[1]]])){
    #   return(NULL)
    # }
    # 
    if (length(x[x < op.partition[[1]]]) < min | length(x[x >= op.partition[[1]]]) < min){
      
      return(NULL)
    }
    sp[[length(sp)+1]] <- op.partition[[1]] #partition on x
    sp[[length(sp)+1]]<-  mean(y[x< op.partition[[1]]]) #ypred left daughter
    sp[[length(sp)+1]]<- mean(y[x >= op.partition[[1]]]) #ypred right daughter
    sp[[length(sp)+1]]<- x[x < op.partition[[1]]] #xsleftdaughter
    sp[[length(sp)+1]]<- x[x >= op.partition[[1]]] #xsrightdaughter
    sp[[length(sp)+1]] <- rss.leaf( y[x < op.partition[[1]]]) #ldrss
    
    return(sp)
  }
}

tree.rf <- function(y,xs, mtry, bootsampled, min = 5){
  tree <- list()
  tree.frame <- list()
  tree.exc <- list()
  leaf.partitions <- list()
  bootsample <- seq(1, length(y))
  
  
  #if( length(y) != nrow(xs)){
   # return("error, length y != dim xs")
 # }
  
  noder <- function(y,xs,mtry, min){
    
    ##what i want this function to do:
    ###given y,xs (like an interval of them):
    ####1. find the split between them 
    
    frame <- node1(y,xs, mtry, min)
    #df <-  rbind(df,frame)
    ####2. call itself on each side of the split
    split <- as.numeric(frame[5])
    spliton <- frame[1]
    
    if(frame[1] == "<leaf>"){
      return(frame)
    } else {
      if(is.null(ncol(xs))){
        yhatl <- y[xs < split]
        xsl <- xs[xs < split]
        yhatr <- y[xs >= split]
        xsr <- xs[xs >= split]
      }else{
        yhatl <- y[xs[,spliton[[1]]] < split]
        xsl <- xs[xs[,spliton[[1]]] < split,]
        yhatr <- y[xs[,spliton[[1]]] >= split]
        xsr <- xs[xs[,spliton[[1]]] >= split, ]
      }
      #    if(length(yhatl) < min & length(yhatr) < min){
      #      print(df)
      #      return(df)
      #    } else {
      #y = yhatr
      #xs = xsr
      noder(y = yhatr,xs = xsr,mtry, min)
      noder(y = yhatl,xs = xsl,mtry, min)
    }
    #  }
  }
  node1 <- function(y, xs, mtry, min){
    onex <- FALSE
    
    if(is.null(ncol(xs))){
      onex <- TRUE
    }
    if(onex == FALSE){
      xssrd <- xs[,sample(ncol(xs),mtry)]
      In <- names(xs) %in% names(xssrd)
      In <- !In 
      NotIn <- names(xs)[In]
    } else {
      xssrd <- as.data.frame(xs)
      NotIn <- NULL
    }
    
    frame <- data.frame("", 0,0,0,0)
    frame[,1] <- as.character(frame[,1])
    leaf.p <- data.frame(rep(0,2), rep(0,2), rep(0,2))
    
    
    if(length(y) < min) {
      frame[1,] <- c("<leaf>",nrow(xssrd), rss.leaf(y), mean(y), 0)
      
      tree.frame <<- rbind(tree.frame, frame)
      leaf.p <- sapply(xs, range)
      leaf.partitions[[length(leaf.partitions)+1]] <<- leaf.p
      return(frame)
      
    } else {
      if (onex){
        maxxr <- list(xs, "X")
      } else {
        maxxr <- max.cor(yy = y,sxs= xssrd)
      }
      sprd <- split.rf(y, maxxr[[1]], min, parent_rss = rss.leaf(y))
      if(is.null(sprd)) {
        frame[1,] <- c("<leaf>",nrow(xssrd),rss.leaf(y),  mean(y), 0)
        
        tree.frame <<- rbind(tree.frame, frame)
        leaf.p <- sapply(xs, range)
        leaf.partitions[[length(leaf.partitions)+1]] <<- leaf.p
        return(frame)
        
      } else if(length(maxxr[[1]] < sprd[[1]]) < min |length(maxxr[[1]] >= sprd[[1]]) < min  ) {
        
        frame[1,] <- c("<leaf>",nrow(xssrd),rss.leaf(y), mean(y), 0)
        
        tree.frame <<- rbind(tree.frame, frame)
        leaf.p <- sapply(xs, range)
        leaf.partitions[[length(leaf.partitions)+1]] <<- leaf.p
        return(frame)
      }
      
      frame[1,] <- c(maxxr[[2]],
                     nrow(xssrd),
                     rss.node(sprd[[1]], y, maxxr[[1]]),
                     mean(y),
                     sprd[[1]])
      # frame[2:4] <- as.numeric(frame[2:4])
    }
    tree.exc <<- rbind(tree.exc, NotIn)
    tree.frame <<- rbind(tree.frame, frame)
    return(frame)
  }
  
  if(bootsampled){
    bootsample <- sample(length(y), length(y), replace = TRUE)
    xs <- xs[bootsample,]
    y <- y[bootsample]
    
  }
  #min <-5
  noder(y, xs, mtry,min)
  names(tree.frame) <- c("var", "n", "dev", "ypred","split.cutleft")
  tree[[1]] <- bootsample
  tree[[2]] <- tree.frame
  tree[[3]] <- tree.exc
  return(tree)
}

###################################AUX TREE FUNCTIONS#######################################
predict.tree.rf <- function(t,xs) {
  t <- t[[2]] 
  first.split <- t[1,]
  onex <- FALSE
  
  if(sum(is.na(t$ypred)) > 0){
    return(NULL)
  }
  if(first.split$var == "<leaf>") {
    return(first.split$ypred)
  }
  
  t$n <- as.numeric(t$n)
  t$split.cutleft <- as.numeric(t$split.cutleft)
  rdn <- t$n[2]
  ldn <- t$n[1] - rdn 
  ldname <- as.numeric(row.names(t[t$n == ldn,]))
  
  if(is.null(ncol(xs))){
    onex <- TRUE
  }
  
  if(length(ldname) > 1) {
    
    right.daughter <- t[1:(ldname[length(ldname)]-1),]
    left.daughter <- t[ldname[length(ldname)]:nrow(t),]
    
  } else {
    
    right.daughter <- t[1:(ldname[length(ldname)]),]
    left.daughter <- t[ldname:nrow(t),]
  }
  
  
  j <- 0
  if(onex){
    predictions <- c(rep(100000, length(xs)))
  } else {
  predictions <- c(rep(100000, nrow(xs)))
  }
  ###SKIPPING CONDITION
  #there's no need to map out the whole tree and have it on file, each y just needs the tree to 
  #a. check the split condition, i.e. xs[,var] < split.cutleft 
  #b. if yes, go two ahead
  #b.2. if no, go one ahead
  #c. stop at leaf, pred[i] <- ypred
  
  if(onex) {
    for (i in 1: length(xs)) {
      xsh <- xs[i]
      if(xsh < first.split$split.cutleft){
        #left daughter
        ld <- left.daughter[1,]
        j <- 1
        while (ld$var != "<leaf>"){
          if (xsh < ld$split.cutleft) {
            j <- j+2
          } else {
          j <- j+1
          }
          ld <- left.daughter[j,]
        }
        predictions[i] <- ld$ypred
      } else {
       #right daughter
        rd <- right.daughter[2,]
        j <- 2
        while (rd$var != "<leaf>"){
          if (xsh < rd$split.cutleft) {
            j <- j+2
          } else {
            j <- j+1
          }
          rd <- right.daughter[j,]
        }
        predictions[i] <- rd$ypred
      }    
    }
  } else {
  
    for(i in 1:(nrow(xs))){
      xsh <- xs[i,]
      if(xsh[,first.split$var] < first.split$split.cutleft){
        #left daughter
        ld <- left.daughter[1,]
        j <- 1
        while (ld$var != "<leaf>"){
          if (xsh[,ld$var] < ld$split.cutleft) {
            j <- j+2
          } else {
            j <- j+1
          }
          ld <- left.daughter[j,]
        }
        predictions[i] <- ld$ypred
      } else {
        #right daughter
        rd <- right.daughter[2,]
        j <- 2
       while (rd$var != "<leaf>"){
          if (xsh[,rd$var] < rd$split.cutleft) {
            j <- j+2
          } else {
            j <- j+1
          }
         rd <- right.daughter[j,]
        }
        predictions[i] <- rd$ypred
      }
    }
  }
  return(predictions)
}

rssforest <- function(rf, y, xs){
  rss <- sum((y[-rf[[1]][[1]]] - as.numeric(predict.tree.rf(rf[[1]],xs[-rf[[1]][[1]],])))^2)
  rss.frame <- rss
  for(i in 2:(length(rf))){ ##random forest is a list, pairs of trees + bootsamples
    t <- rf[[i]]
    rss.frame <- rbind(rss.frame,sum((y[-rf[[i]][[1]]] - as.numeric(predict.tree.rf(t,xs[-rf[[i]][[1]],])))^2))
    
    ##send tree off to inftrees with the -bootsampled data
    ##get back frame
  }
  
  ##generate distribution of vi's from frames - pval
  return(mean(rss.frame))
}

###################################GROWING A FOREST#########################################

rforest <- function(y, xs,  mtry, ntree) { 
  #imputs: formula to be tested and the dset to test on. outputs: list,
  #contains: total oob error, trees, and bootsample for the tree  
  rforest <- list() #define empty list for the rf. length = ntree*2+1
  #where each tree is a list, bootsample + frame  
  xsi <- c()
  di <- data.frame()
  # form <- as.character(form)
  # yn <- form[2]
  # yi <- as.numeric(names(d) == yn)
  # y <- d[,yi]
  # xs <- d[,(yi + 1 - 2)*-1]
  
  while (length(rforest) < ntree) {
    rforest[[length(rforest)+1]] <- tree.rf(y,xs,mtry, bootsampled = TRUE)
  }
  return(rforest)
}


###################################INFFOREST#################################################

infforest <- function(rf,y,xs) {
  v <- inftrees(rf[[1]],y[-rf[[1]][[1]]],xs[-rf[[1]][[1]],])
  vi.frame <- v
  for(i in 2:(length(rf))){ ##random forest is a list, pairs of trees + bootsamples
    t <- rf[[i]]
    vi.frame <- rbind(vi.frame,inftrees(t, y[-t[[1]]], xs[-t[[1]],]))
    
    ##send tree off to inftrees with the -bootsampled data
    ##get back frame
  }
  
  ##generate distribution of vi's from frames - pval
  return(vi.frame)
}


infforest.test <- function(inf){
  p <- rep(10000, ncol(inf))
  mu <- c()
  sd <- c()
  for(i in 1:ncol(inf)){
    mu[i] <- mean(inf[,i])
    sd[i] <- sd(inf[,i])
    p[i] <- pnorm(0,mean = mu[i], sd = sd[i])
  }
  return(data.frame("var" =  names(as.data.frame(inf)), "P(var > 0)"= p, "mean" = mu, "sd" = sd))
}

##############################Y PERMUTED VARIABLE IMPORTANCE################################

yperm.inf <- function(rf,y,xs) {
  v <- ytrees(rf[[1]],y[-rf[[1]][[1]]],xs[-rf[[1]][[1]],])
  v1 <- v
  for (i in 2:length(rf)){
    v1 <-  ytrees(rf[[i]],y[-rf[[1]][[1]]],xs[-rf[[1]][[1]],])
    v <- rbind(v,v1)
  }
  
  return(v)
}

##############################CONDITIONAL VARIABLE IMPORTANCE################################

conditional.inf <- function(rf, y, xs) {
  v <- strobltrees(rf[[1]],y[-rf[[1]][[1]]],xs[-rf[[1]][[1]],])
  v1 <- v
  for (i in 2:length(rf)){
    v1 <-  strobltrees(rf[[i]],y[-rf[[1]][[1]]],xs[-rf[[1]][[1]],])
    v <- rbind(v,v1)
  }
  
  return(v)
}

###############################PERMUTED VARIABLE IMPORTANCE################################

permuted.inf <- function(rf, y, xs) {
  v <- briemantrees(rf[[1]],y[-rf[[1]][[1]]],xs[-rf[[1]][[1]],])
  
  for (i in 2:length(rf)){
    v <- rbind(v, briemantrees(rf[[1]],y[-rf[[1]][[1]]],xs[-rf[[1]][[1]],]))
  }
  
  return(v)
  
}

###################################INFTREES SUPPORT###########################################

inftrees <- function(t, y, xs) {
  
  rss.post <- rep(0, ncol(xs))
  vi <- rep(0, ncol(xs))
  
  ta <- t
  t <- t[[2]]
  
  # set.seed(1)
  #for(i in 1:ncol(xs)){
  #  rss.pre[i] <- VIdev(t, names(xs)[i])
  #}
  
  rss.pre <- sum((y - as.numeric(predict.tree.rf(ta, xs)))^2)/length(y)
  
  set.seed(1)
  for(i in 1:ncol(xs)){
    xs.for.permuting.i <- xs
    ti <- tree.rf(xs[,i], xs[,-i], mtry = ncol(xs[,-i]), bootsample = FALSE) 
    if(nrow(ti[[2]]) <= 3) {
        xs.for.permuting.i[,i] <- sample(xs.for.permuting.i[,i])
      } else {
          xi.partitions <- predict.tree.rf(ti, xs[,-i])
          xs.for.permuting.i$groups <- as.factor(xi.partitions)
          for (j in 1:length(levels(xs.for.permuting.i$groups))) {
            for(n in 1:ncol(xs)){
              xs.for.permuting.i[xs.for.permuting.i$group == levels(xs.for.permuting.i$groups)[j],n] <-
                sample(xs.for.permuting.i[xs.for.permuting.i$group == levels(xs.for.permuting.i$groups)[j],n], 
                     length(xs.for.permuting.i[xs.for.permuting.i$group == levels(xs.for.permuting.i$groups)[j],n]))
            }
          }
      }
    predictions.for.y.xi.perm <- predict.tree.rf(ta,xs.for.permuting.i)
    rss.post[i] <- abs(sum((y - as.numeric(predictions.for.y.xi.perm))^2)/length(y) - rss.pre)
    #ex <- sum(names(xs)[i] ==  ta[[3]])/nrow(ta[[3]])
    # if (is.null(ex)){
    #   rss.post[i] <- rss.post[i] *0
    # } else if(ex > .7) {
     # rss.post[i] <- rss.post[i] * (ex)
    # }
  }
  rss.post <- rss.post/max(abs(rss.post))
  names(rss.post) <- names(xs)
  return(rss.post)
}

ytrees <- function(t,y,xs) {
  d <- rbind(y,xs)
  rss.post <- rep(0, ncol(xs))
  ta <- t
  t <- t[[2]]
  predictions <- as.numeric(predict.tree.rf(ta, xs))
  rss.pre <- sum((y - predictions)^2)/length(y)
  xs.for.permuting.i <- xs
  yperm <- y
  for(i in 1:ncol(xs)){
    corrr <- sapply(xs[,-i],cor, y = xs[,i])
    xs.i <- xs[,-i]
    xs.i <-  (xs.i[,abs(corrr) > .2])
    xs.i <- cbind(xs[,i], xs.i)
    if(ncol(xs.i) == 1){
      yperm <- sample(y)
      rss.post[i] <- (rss.pre-sum((yperm - predictions)^2)/length(y) )
      xs.i <- xs
       yperm <- y
    } else {
      fr <- t[t$var %in% names(xs.i)[-1],]
      if(nrow(fr) == 0) {
      yperm <- sample(y)
      rss.post[i] <- (rss.pre-sum((yperm - predictions)^2)/length(y) )
      xs.i <- xs
      yperm <- y
      } else {
        for (j in 1:length(levels(as.factor(fr$var)))) {
          fri <- fr[fr$var == levels(as.factor(fr$var))[j],]
          for(n in 1:nrow(fri)){
            yperm[xs.i[,fri$var[n]] < fri$split.cutleft[n]] <- sample(y[xs.i[,fri$var[n]] < fri$split.cutleft[n]])
          }
        }
        rss.post[i] <- (rss.pre - sum((yperm - predictions)^2)/length(y))
        xs.i <- xs
        yperm <- y
      }
    }
  }
  if(sum(abs(rss.post)) == 0) {
    return(rss.post)
  } else{
   # rss.post <- rss.post/max(abs(rss.post))
    # names(rss.post) <- names(xs)
    return(rss.post)
  }

}


strobltrees <- function(t,y,xs) {
  rss.post <- rep(0, ncol(xs))
  
  ta <- t
  t <- t[[2]]
  
  predictions <- as.numeric(predict.tree.rf(ta, xs))
  rss.pre <- sum((y - predictions)^2)/length(y)
  
  xs.for.permuting.i <- xs
  
  for(i in 1:ncol(xs)){
    corrr <- sapply(xs[,-i],cor, y = xs[,i])
    xs.i <- xs[,-i]
    xs.i <-  (xs.i[,abs(corrr) > .2])
    xs.i <- cbind(xs[,i], xs.i)
    
    if(ncol(xs.i) == 1){
      xs.i <- sample(xs.i)
      xs.ii <- xs
      xs.ii[,i] <- xs.i[,1]
      names(xs.ii) <- names(xs)
      predictions.for.y.xi.perm <- predict.tree.rf(ta,xs.ii)
      rss.post[i] <- abs(sum((y - as.numeric(predictions.for.y.xi.perm))^2)/length(y) - rss.pre)
      xs.i <- xs
    } else {
      
      fr <- t[t$var %in% names(xs.i)[-1],]
      if(nrow(fr) == 0) {
        xs.i <- sample(xs.i)
        xs.ii <- xs
        xs.ii[,i] <- xs.i[,1]
        names(xs.ii) <- names(xs)
        predictions.for.y.xi.perm <- predict.tree.rf(ta,xs.ii)
        rss.post[i] <- abs(sum((y - as.numeric(predictions.for.y.xi.perm))^2)/length(y) - rss.pre)
        xs.i <- xs
      } else {
        for (j in 1:length(levels(as.factor(fr$var)))) {
          fri <- fr[fr$var == levels(as.factor(fr$var))[j],]
          for(n in 1:nrow(fri)){
            xs.i[xs.i[,fri$var[n]] < fri$split.cutleft[n] ,1] <- sample(xs.i[xs.i[,fri$var[n]] < fri$split.cutleft[n] ,1])
          }
        }
        xs.ii <- xs
        xs.ii[,i] <- xs.i[,1]
        names(xs.ii) <- names(xs)
        predictions.for.y.xi.perm <- predict.tree.rf(ta,xs.ii)
        rss.post[i] <- abs(sum((y - as.numeric(predictions.for.y.xi.perm))^2)/length(y) - rss.pre)
        xs.i <- xs
        ex <- sum(names(xs)[i] ==  ta[[3]])/nrow(ta[[3]])
        if(ex > .8){
          rss.post[i] <- rss.post[i] * (ex)
        }
      }
    }
  }
  if(sum(abs(rss.post)) == 0) {
    return(rss.post)  
  } else{
    #rss.post <- rss.post/max(abs(rss.post))
    # names(rss.post) <- names(xs)
    return(rss.post)
  }
  
}

breimantrees <- function (t,y,xs){
  ta <- t
  t <- t[[2]]
  
  rss.pre <- sum((y - as.numeric(predict.tree.rf(ta, y, xs)))^2)
  
  rss.post <- rep(0, ncol(xs))
  xs.for.permuting <- xs
  for (i in 1:ncol(xs)){
    xs.for.permuting[,i] <- sample(xs.for.permuting[,i], replace = TRUE)
    predictions.for.y.xi.perm <- predict.tree.rf(ta,y,xs.for.permuting)
    rss.post[i] <- abs(sum((y - as.numeric(predictions.for.y.xi.perm))^2) - rss.pre)
    xs.for.permuting <- xs
  }
  rss.post <- rss.post/max(abs(rss.post))
  names(rss.post) <- names(xs)
  return(rss.post)
}

```
```{r loaddatad2, echo=FALSE}
rep(0, 12) -> mu #mean of each variable

diag(12) -> sigma #creates a 12 by 12 diagonal matrix with 1's down the diagonal

sigma -> sigma1
.9 -> sigma1[1,2]
.9 -> sigma1[2,1] #adding the block correlation between the first four variables
.9 -> sigma1[1,3]
.9 -> sigma1[3,1]
.9 -> sigma1[3,2]
.9 -> sigma1[2,3]
.9 -> sigma1[1,4]
.9 -> sigma1[4,1]
.9 -> sigma1[2,4]
.9 -> sigma1[4,2]
.9 -> sigma1[4,3]
.9 -> sigma1[3,4]

1000 -> n #number of observations
set.seed(1)
mvrnorm(n =n, mu = mu, Sigma = sigma1) -> sim1000 #sampling from the multivariate normal 
c(5,5,2,0,-5,-5,0,0,0,0,0,0) -> bts #these are the betas 

rnorm(1000, mean = 0, sd = .5) -> e #error terms

rep(0, 1000) -> ys #init a vector of zeros

for( i in 1:1000){ #go row by row and create the ys based on the function of e, bts, and sim1000
ys[i] <- sim1000[i,1]*bts[1]+
    sim1000[i,2]*bts[2]+ 
    sim1000[i,3]*bts[3]+ 
    sim1000[i,4]*bts[4]+ 
    sim1000[i,5]*bts[5]+ 
    sim1000[i,6]*bts[6]+
    sim1000[i,7]*bts[7]+ 
    sim1000[i,8]*bts[8]+ 
    sim1000[i,9]*bts[9]+ 
    sim1000[i,10]*bts[10]+ 
    sim1000[i,11]*bts[11]+ 
    sim1000[i,12]*bts[12] +
    e[i]
}

d1 <- as.data.frame(sim1000)
d1$y <- ys



n <- 10000
c(5,5,2,0,-5,-5,0,0,0,0,0,0) -> bts #these are the betas 
rnorm(1000, mean = 0, sd = .5) -> e #error terms



x1 <- rnorm(1000)

x2 <- 2*log(abs(x1)) + rnorm(1000)

x3 <- 2*log(abs(x2)) + rnorm(1000)

x4 <- 2*log(abs(x3)) + rnorm(1000)

mvrnorm(n =n, mu = mu, Sigma = sigma) -> simx5x12

data.frame(x1,x2,x3,x4,simx5x12) -> d2
              
c("X1","X2","X3","X4","X5","X6", "X7","X8","X9","X10","X11","X12") -> names(d2)

rep(0, 1000) -> ys #init a vector of zeros

for( i in 1:1000){ #go row by row and create the ys based on the function of e, bts, and d2
ys[i] <- d2[i,1]^2*bts[1]+
    d2[i,2]^2*bts[2]+ 
    d2[i,3]^2*bts[3]+ 
    d2[i,4]^2*bts[4]+ 
    d2[i,5]*bts[5]+ 
    d2[i,6]*bts[6]+
    d2[i,7]*bts[7]+ 
    d2[i,8]*bts[8]+ 
    d2[i,9]*bts[9]+ 
    d2[i,10]*bts[10]+ 
    d2[i,11]*bts[11]+ 
    d2[i,12]*bts[12] +
    e[i]
}

d2$y <- ys

```

#INFFOREST Variable Importance

Variable importance measures must contend with the following relationships in the data: for each $V_j$ in $V$, first, there is the relationship between $Y$ and $V_j$, secondly, the relationship between $V_j$ and the other predictors, $V_{-j}$, and thirdly, the relationship between $Y$ and the other predictors $V_{-j}$. 

\usetikzlibrary{positioning}
\begin{center}
\begin{tikzpicture}[level/.style={sibling distance=60mm/#1}]
\node[circle,draw] (y) {$Y$};
\node[circle,draw] (xj) [below left = of y] {$V_j$};
\node[circle,draw] (xnotj) [below right = of y] {$V_{-j}$};
\draw[<->] (y.south) -- (xj.north);
\draw[<->] (y.south) -- (xnotj.north);
\draw[<->] (xj.east) -- (xnotj.west);
\end{tikzpicture}
\end{center}

In permuted variable importance, the null hypothesis is that $Y$ is independent of $V_j$, regardless of the relationship between $V_j$ and $V_{-j}$. By permuting $V_j$ blindly and then calculating the RSS, the relationship between $V_j$ and $Y$ and the relationship between $V_j$ and $V_{-j}$ are broken. The other variables, $V_{-j}$ are not permuted and since the RSS is calculated using the original model fitting $Y \sim V$, the relationship between $Y$ and the $V_{-j}$ is maintained. 

\usetikzlibrary{positioning}
\begin{center}
\begin{tikzpicture}[level/.style={sibling distance=60mm/#1}]
  \node[circle,draw] (y1) {$Y$};
\node[circle,draw] (xj) [below left = of y1] {$V_j$};
\node[circle,draw] (xnotj) [below right = of y1] {$V_{-j}$};
\draw[red,thick,dashed] (y.south) -- (xj.north);
\draw[<->] (y.south) -- (xnotj.north);
\draw[red,thick,dashed] (xj.east) -- (xnotj.west);
\end{tikzpicture}
\end{center}

In conditional variable importance, however, the null hypothesis that is tested is that $V_j$ is independent of $Y$ given the relationship between $V_j$ and $V_{-j}$ and the relationship between $V_{-j}$ and $Y$. Therefore, the permutations on $V_j$ are done in such a way that the relationship between $V_j$ and $Y$ are broken, while approximately maintaining the relationships between $V_j$ and $V_{-j}$ and $V_{-j}$ and $Y$. 

\usetikzlibrary{positioning}
\begin{center}
\begin{tikzpicture}[level/.style={sibling distance=60mm/#1}]
  \node[circle,draw]  (y2) {$Y$};
\node[circle,draw] (xj) [below left = of y2] {$V_j$};
\node[circle,draw] (xnotj) [below right = of y2] {$V_{-j}$};
\draw[red,thick,dashed] (y.south) -- (xj.north);
\draw[<->] (y.south) -- (xnotj.north);
\draw[<->] (xj.east) -- (xnotj.west);
\end{tikzpicture}
\end{center}



This permutation structure has the following implications: 
1. If $V_j$ has a weak relationship with $Y$ but a strong relationship with $V_{-j}$, the conditional variable importance value will be high. 
2. If $V_j$ is approximately independent of $V_{-j}$, but a good predictor of $Y$, then the conditional variable importance of $V_j$ will be high. 

Conditional variable importance provides a method of statistical inference on random forests, but it does not answer the same question as statistical inference on linear models. Namely, what is the relationship between $V_j$ and $Y$ given the other $V_{-j}$ variables in the model? The INFFOREST method of variable importance permutes under the null hypothesis that $V_j$ given $V_{-j}$ is independent of $Y$. This leads us to break the relationships between $Y$ and the $V_j$ en mass, according to the respective structure of $V_j$ and $V_{-j}$. 


\usetikzlibrary{positioning}
\begin{center}
\begin{tikzpicture}[level/.style={sibling distance=60mm/#1}]
\node [circle,draw] (y) {$Y$};
\node [circle,draw] (xj) [below left = of y] {$V_j$};
\node [circle,draw] (xnotj) [below right = of y] {$V_{-j}$};
\draw[red,thick,dashed] (y.south) -- (xj.north);
\draw[red,thick,dashed] (y.south) -- (xnotj.north);
\draw[<->] (xj.east) -- (xnotj.west);
\end{tikzpicture}
\end{center}

This has the following conclusions: 
1. If $V_j$ is a good predictor of $Y$ but independent of the rest of the predictors, the INFFOREST variable importance will be high.
2. If $V_j$ is a good predictor of $Y$ but is heavily correlated with at least one of the other predictors, the INFFOREST variable importance will be low. It is assumed that the information gained from adding $V_j$ to the model could be gained from one of the other predictors. 

##Algorithm and Implementation

The INFFOREST variable importance is a method of permuted variable importance not unlike that of conditionally permuted variable importance. INFFOREST values are calculated at the tree level, using the partitions on $V_j$ from a tree created to predict the model $V_j \sim V_1,..., V_{j-1}, V_{j+1},...,V_p$. This auxiliary tree is fit by considering all $p-1$ predictors at each split and so may be quite large or quite small depending on the richness of the correlation structure around $V_j$. The auxiliary tree is also fit using the OOB sample, $\hat{B}_t$, for the tree at question. If the auxiliary tree results in a single leaf (i.e. there are no splits), then $\hat{B}_t$ is permuted blindly, without partitions. If the auxiliary tree results in two leaves, there will be two partitions on $\hat{B}_t$ to permute $\hat{B}_t$ within, and so on. After permuting $\hat{B}_t$ within these partitions, the RSS is calculated for that tree using the permuted dataset. The absolute difference of the RSS after permutation and the RSS before permuting the sample is INFFOREST variable importance for that tree. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Note that for this reason, the INFFOREST variable importance is always greater than or equal to zero, and is standardized by the max INFFOREST variable importance value given by that tree. As the variable importance values are calculated for each tree for each variable, once the method is completed there is a distribution of potential variable importance values for $V_j$, one for each tree. These distributions may or may not be normal, depending on the multicolinearity of the predictors. The INFFOREST variable importance algorithm works as follows:	

\begin{algorithm}
\caption{INFForests, $VI_{inf}(R)$}
\label{infforest}
\begin{algorithmic}[1]
\State Fit a random forest, $R$ on the dataset $D$ fitting the model $Y \sim V_1,...,V_p$.
\For{each $V_i \in {{V_1,...,V_p}}$}
\For{each $t \in R$}
\State Calculate: $\Xi_o =  \frac 1 {\nu_t} RSS(t,\bar{B}_t)$
\State Calculate a tree $T_i$ that predicts $V_i \sim V_1,...,V_{i-1}, V_{i+1},...V_p$ using the subset of the observations used to fit $t$  
\State Permute $\bar{B}_t$ with respect to the set of partitions $P_{xi}$ from $T_i$.
\If $T_i$ is a leaf
\State Permute the $V_i$ values blindly with respect to no partitions. Set $\bar{B}_t^*$ to be equal to $\bar{B}_t$ except $V_i$ is permuted. 
\EndIf
\State Now find $\Xi^* =  \frac 1 {\nu_t} RSS(t,\bar{B}_t^*)$
\State The difference between these values, $\Xi^* - \Xi_o$,  is the variable importance for $V_i$ on $t$
\EndFor
\State Test the null hypothesis that $0$ is the likely value of $\frac 1 {\nu_t} RSS(t,\bar{B}_t^*)$ using the distribution of values of $\Xi^*$ gathered from each tree in $R$
\EndFor
\end{algorithmic}
\end{algorithm}

INFFOREST variable importance operates under the null hypothesis that $Y$ is independent of $V_J$ given the correlation structure of $V_j$ and the other $V_{-j}$ predictors, or that the true INFFOREST variable importance for $V_j$ is 0. The alternative hypothesis is that $Y$ and $V_j$ are not independent given the correlation structure of $V_j$ and the other predictors or that the INFFOREST variable importance for $V_j$ is greater than zero.  After INFFOREST values have been computed for the entire forest, they are treated as samples from the population of possible INFFOREST values for $V_j$ given the random forest $R_f$, a significance test can be run under the null hypothesis stated above. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Recall the data sets $D_1$ and $D_2$, introduced in chapter 2. Both datasets contain 12 predictors and one response, where there is some type of correlation structure between the first four variables. In $D_1$, this structure is linear, and in $D_2$ it is not. The median INFFOREST values for a random forest fit to a subset of each data set are presented in the following tables. The p-values listed in table \ref{tab:tabviD1} are the observed proportion of INFFOREST values for that variable that are above zero. 

```{r, echo = FALSE, message=FALSE, warning=FALSE, cache = TRUE}
set.seed(1)
r1 <- rforest(d1$y[1:200], d1[1:200,1:12], mtry = 7, ntree = 100)
#r2 <- rforest(d2$y[1:200], d2[1:200,1:12], mtry = 7, ntree = 70)
v1 <- infforest(r1, d1$y[1:200], d1[1:200,1:12])
#v2 <- infforest(r2, d2$y[1:100], d2[1:100,1:12])
v1[is.nan(v1)] <- NA
v1 <- v1[complete.cases(v1),]
#v2[is.nan(v2)] <- NA
#v2 <- v2[complete.cases(v2),]

vi.d1 <-  rep(0,12)
for (i in 1:12){
  vi.d1[i] <- median(v1[,i])
}
pval <- rep(0,12)

for(i in 1:12) {
  pval[i] <- sum(v1[,i] == 0)/nrow(v1)  
}

vi.d1 <- data.frame("median" = (vi.d1))

vi.d1$Coefficient <- bts
vi.d1$pval <- pval

#vi.d2 <-  rep(0,12)
#for (i in 1:12){
#   vi.d2[i] <- median(v2[,i])
 #}
#pval <- rep(0,12)
 
#for(i in 1:12) {
  # pval[i] <- sum(v2[,i] == 0)/nrow(v2)  
# }
#vi.d2 <- data.frame("Median INFFOREST values" = vi.d2)
 
#vi.d2$Coefficient <- bts
#vi.d2$pval <- pval
```

```{r, echo=FALSE, fig.pos='H',  message=FALSE}
row.names(vi.d1) <- names(d1[,-13])
a <- kable(vi.d1, caption="\\label{tab:tabviD1}Median INFFOREST variable importance values from random forests of 100 trees fit on the first simulated data set")

#row.names(vi.d2) <- names(d2[,-13])
#b <- kable(vi.d2)
```

```{r, echo=FALSE, message=FALSE}
#grid.arrange(a,b, col = 2)
a
```

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;At significance level $\alpha = .05$, we would reject the null hypothesis that the true INFFOREST value for these variables is zero for variables $V_1,...,V_4$ and $V_6$. We have found that in the context of the other predictors these predictors have a significant relationship with $Y$. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The parameters of a random forest are the data, the formula, the number of trees ($ntree$), and the number of variables to consider as possible candidates at each split ($mtry$). We'll investigate how INFFOREST variable importance on $D_1$ changes as the last two parameters are altered. First, random forests will be created for the following values of $mtry$: $mtry = 4,7,12$. The random forest behind table \ref{tab:tabviD1} was created using $mtry = 7$. These forests will all have the same number of trees, $ntree = 50$. 

```{r mtry4, echo=FALSE, message=FALSE, warnings = FALSE, cache = TRUE}
set.seed(1)
r4 <- rforest(d1$y[1:200],d1[1:200, 1:12], mtry = 4, ntree = 50)
v4 <- infforest(r4, d1$y[1:200], d1[1:200,1:12])

v4[is.nan(v4)] <- NA
v4 <- v4[complete.cases(v4),]

v4.d1 <-  rep(0,12)
for (i in 1:12){
  v4.d1[i] <- median(v4[,i])
}
q1 <- rep(0,12)

for(i in 1:12) {
  q1[i] <- quantile(v4[,i], .05)
}

q3 <- rep(0,12)

for(i in 1:12) {
  q3[i] <- quantile(v4[,i], .95)
}

v4.d1 <- data.frame("Median INFFOREST values" = (v4.d1))

v4.d1$Coefficient <- bts
v4.d1$q1 <- q1
v4.d1$q3 <- q3
```

```{r mtry7, echo=FALSE, message=FALSE, warnings = FALSE, cache = TRUE}

q1 <- rep(0,12)

for(i in 1:12) {
  q1[i] <- quantile(v1[,i], .05)
}

q3 <- rep(0,12)

for(i in 1:12) {
  q3[i] <- quantile(v1[,i], .95)
}

vi.d1 <- data.frame("median" = (vi.d1))

vi.d1$Coefficient <- bts
vi.d1$q1 <- q1
vi.d1$q3 <- q3
```

```{r mtry12, echo=FALSE, message=FALSE, warnings = FALSE, cache = TRUE}
set.seed(1)
r12 <- rforest(d1$y[1:200],d1[1:200, 1:12], mtry = 12, ntree = 50)
v12 <- infforest(r12, d1$y[1:200], d1[1:200,1:12])

v12[is.nan(v12)] <- NA
v12 <- v12[complete.cases(v12),]

v12.d1 <-  rep(0,12)
for (i in 1:12){
  v12.d1[i] <- median(v12[,i])
}
q1 <- rep(0,12)

for(i in 1:12) {
  q1[i] <- quantile(v12[,i], .05)
}

q3 <- rep(0,12)

for(i in 1:12) {
  q3[i] <- quantile(v12[,i], .95)
}

v12.d1 <- data.frame("median" = (v12.d1))

v12.d1$Coefficient <- bts
v12.d1$q1 <- q1
v12.d1$q3 <- q3
```

```{r, echo = FALSE, message=FALSE}
vi.d1$variable <- factor(c(names(d1[,-13])), levels = names(d1[,-13]))
v4.d1$variable <- factor(c(names(d1[,-13])),levels = names(d1[,-13]))
v12.d1$variable <- factor(c(names(d1[,-13])),levels = names(d1[,-13]))

p <- ggplot(data = v4.d1, aes( x = variable, y =  Median.INFFOREST.values, group = 1)) +
  geom_point() +
  ylab(" ")+
  xlab(" ")+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  geom_line()+
  geom_ribbon(ymin = v4.d1$q1, ymax = v4.d1$q3, data = v4.d1, alpha = .4)+
  scale_y_continuous(limits = c(0,1))


q<- ggplot(data = vi.d1, aes( x = variable, y = median.median, group = 1)) +
  geom_point() +
  ylab(" ")+
  xlab(" ")+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  geom_line()+
  geom_ribbon(aes(ymin = vi.d1$q1, ymax = vi.d1$q3), alpha = .4)+
  scale_y_continuous(limits = c(0,1))


r <- ggplot(data = v12.d1, aes( x = variable, y = median, group = 1)) +
  geom_point()+
  ylab(" ")+
  xlab(" ")+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  geom_line()+
  geom_ribbon(aes(ymin = v12.d1$q1, ymax = v12.d1$q3), alpha = .4)+
  scale_y_continuous(limits = c(0,1))


```


```{r , warnings=FALSE, echo=FALSE, message = FALSE, fig.pos='H', fig.height=3, fig.cap="\\label{fig:figmtry}Distribution of INFFOREST variable importance values for data set D1 in random forests with mtry = 4,7,12."}
grid.arrange(p,q,r, ncol=3)
```

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In figure \ref{fig:figmtry} the shading represents the most common values of INFFOREST for that variable. The shaded area represents a 95% confidence interval around the average value. As $mtry$ approaches the full number of predictors for the data set, the distributions become less variable. The parameter $mtry$ is generally taken to be between one third and just over one half of the predictors in the data set, but should be optimized. 

```{r rssforest, echo= FALSE, warnings = FALSE, message = FALSE}

rss4 <- rssforest(r4, d1$y[1:200], d1[1:200, 1:12])
rss7 <- rssforest(r1, d1$y[1:200], d1[1:200, 1:12])
rss12 <- rssforest(r12, d1$y[1:200], d1[1:200, 1:12])

mtryrss <- data.frame("mtry" = c(4,7,12), "RSS" = c(rss4,rss7, rss12))
```

```{r, echo=FALSE, fig.pos='H', message=FALSE, warning=FALSE}
kable(mtryrss, caption = "\\label{tab:tabrssRF}Out of bag RSS values for random forests on data set D1 with mtry equal to 4, 7, or 12")
```

While the significance of the variables changed slightly for different values of $mtry$, in applications this may not be anything more than a practical inconvenience. The value of $mtry$ that optimizes the tree is $mtry = 7$ as seen in table \ref{tab:tabrssRF}, and this is the model that would be used both for prediction and for inference. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Unlike $mtry$ which must be optimized for each data set manually, traditionally the number of trees to fit in a forest follows a simpler rule: more trees are better. There is some risk that, given a high enough value of $mtry$, after a certain number of trees are fit, they will begin to be more correlated with each other than they would have been otherwise. To demonstrate the consistency in the INFFOREST variable importance significance testing, two new random forests will be constructed. Each will follow the same formula as the random forest from table \ref{tab:tabviD1}, $Y \sim V$, and will have $mtry = 7$, and $ntree = 50, 200$ (the random forest where $ntree = 100$ was fit in the previous example). 

```{r n10, echo=FALSE, message=FALSE, cache=TRUE, warnings=FALSE}
set.seed(1)
r10 <- rforest(d1$y[1:200], d1[1:200, 1:12], mtry = 7, ntree = 50)
v10 <- infforest(r10, d1$y[1:200], d1[1:200, 1:12])
v10[is.nan(v10)] <- NA
v10 <- v10[complete.cases(v10),]

v10.d1 <-  c(rep(0,12))
for (i in 1:12){
  v10.d1[i] <- median(v10[,i])
}
pval <- rep(0,12)

for(i in 1:12) { 
  pval[i] <- sum(v10[,i] == 0)/nrow(v10)  
}
v10.d1 <- data.frame("median" = v10.d1)
v10.d1$pval <- as.vector(pval)

```

```{r n75, echo=FALSE, message=FALSE, warnings = FALSE, cache=TRUE}
set.seed(1)
r75 <- rforest(d1$y[1:200], d1[1:200, 1:12], mtry = 7, ntree = 200)
v75 <- infforest(r75, d1$y[1:200], d1[1:200, 1:12])
v75[is.nan(v75)] <- NA
v75 <- v75[complete.cases(v75),]

v75.d1 <-  c(rep(0,12))
for (i in 1:12){
  v75.d1[i] <- median(v75[,i])
}
pval <- rep(0,12)

for(i in 1:12) {
  pval[i] <- sum(v75[,i] == 0)/nrow(v75)  
}
v75.d1 <- data.frame("median" = v75.d1)
v75.d1$pval <- as.vector(pval)

vi.d1 <- vi.d1[,c(1,3)]
names(vi.d1) <- c("median", "pval")

```

```{r , echo=FALSE, message=FALSE, fig.pos='H'}
row.names(vi.d1) <- NULL
row.names(v10.d1) <- names(d1[,1:12])
vi.d1$pval <- round(vi.d1$pval, 2)
v10.d1$pval <- round(v10.d1$pval, 2)
v75.d1$pval <- round(v75.d1$pval, 2)
vi.d1$median <- round(vi.d1$median, 2)
v10.d1$median <- round(v10.d1$median, 2)
v75.d1$median <- round(v75.d1$median, 2)

kable(list(v10.d1, vi.d1, v75.d1), caption = "\\label{tab:tabntree}INFFOREST Variable Importance for random forests with 50, 100, and 200 trees")
```


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In the first we found $V_8$ to be significant and in the second forest, with 100 trees we found that $V_9$ was significant. Neither of these variables were used to generate $Y$ and are roughly uncorrelated with the other predictors. As $ntree \rightarrow \infty$, the INFFOREST values become more consistant. Consider table \ref{tab:tabntree500}, that represents the INFFOREST values from a random forest fit on $Y \sim V$ with $mtry = 7$ and $ntree = 500$.

```{r n500, echo=FALSE, message=FALSE, cache=TRUE, warning=FALSE}
set.seed(1)
r500 <- rforest(d1$y[1:200], d1[1:200, 1:12], mtry = 7, ntree = 500)
v500 <- infforest(r500, d1$y[1:200], d1[1:200, 1:12])
v500[is.nan(v500)] <- NA
v500 <- v500[complete.cases(v500),]

v500.d1 <-  c(rep(0,12))
for (i in 1:12){
  v500.d1[i] <- median(v500[,i])
}
pval <- rep(0,12)

for(i in 1:12) { 
  pval[i] <- sum(v500[,i] == 0)/nrow(v500)  
}
v500.d1 <- data.frame("median" = v500.d1)
v500.d1$pval <- as.vector(pval)
```

```{r tab500, echo=FALSE, warning=FALSE, message=FALSE,error=FALSE}

row.names(v500.d1) <- names(d1[,1:12])

v500.d1$pval <- round(v500.d1$pval, 2)

v500.d1$median <- round(v500.d1$median, 2)


kable(v500.d1, caption = "\\label{tab:tabntree500}INFFOREST Variable Importance for a random forest on the data set D1 with 500 trees")
```

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;There are some slight differences between the INFFOREST values for the forests with 200 and 500 trees, but the same variables are found to be significant. We note that the confidence intervals also decrease. Compare the intervals in INFFOREST when $ntree = 50$ (repeated from figure \ref{fig:figmtry}), to the intervals when $ntree = 500$ in the figure \ref{fig:500tree} below:

```{r, echo=FALSE, warning=FALSE, message=FALSE,error=FALSE}
q1 <- rep(0,12)

for(i in 1:12) {
  q1[i] <- quantile(v500[,i], .05)
}

q3 <- rep(0,12)

for(i in 1:12) {
  q3[i] <- quantile(v500[,i], .95)
}

v500.d1$q1 <- q1
v500.d1$q3 <- q3

v500.d1$variable <- factor(c(names(d1[,-13])),levels = names(d1[,-13]))

s <- ggplot(data = v500.d1, aes( x = variable, y =  median, group = 1)) +
  geom_point() +
  ylab(" ")+
  xlab(" ")+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  geom_line()+
  geom_ribbon(ymin = v500.d1$q1, ymax = v500.d1$q3, data = v500.d1, alpha = .4)+
  scale_y_continuous(limits = c(0,1))
```

```{r , warnings=FALSE, echo=FALSE, message = FALSE, fig.pos='H', fig.height=3, fig.cap="\\label{fig:500tree}Distribution of INFFOREST variable importance values for data set D1 in random forests with 50 and 500 trees."}
grid.arrange(q,s, ncol=2)
```



