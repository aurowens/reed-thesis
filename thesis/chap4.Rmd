---
header-includes:
- \usepackage{graphicx,latexsym}
- \usepackage{amssymb,amsthm,amsmath}
- \usepackage{longtable,booktabs,setspace}
- \usepackage{algorithm}
- \usepackage{algpseudocode}
- \usepackage{pifont}
output: pdf_document
---
```{r include_reedtemplates_4, include = FALSE}
# This chunk ensures that the reedtemplates package is installed and loaded
# This reedtemplates package includes the template files for the thesis and also
# two functions used for labeling and referencing
if(!require(devtools))
  install.packages("devtools", repos = "http://cran.rstudio.com")

if(!require(reedtemplates)){
  library(devtools)
  devtools::install_github("ismayc/reedtemplates")
  }
library(reedtemplates)

library(tree)
library(reedtemplates)
library(MASS)
library(ggplot2)
library(randomForest)
library(maptree)
library(knitr)
library(GGally)
library(reshape)
library(ggplot2)
library(plotly)
#flights <- read.csv("data/flights.csv")
thesis <- c("#245e67", "#90bd98", "#cfd0a0", "#c49e46", "#d28383")
```

```{r inftree,echo = FALSE, warning=FALSE, message=FALSE}
################INFTREES############
intervalReturn <- function(split.cutleft){ 
  intervals <- data.frame(split.cutleft)
  intervals <- filter(intervals, split.cutleft != "")
  intervals <- dplyr::select(intervals, split.cutleft)
  intervals$split.cutleft <- gsub("<", "", intervals$split.cutleft)
  intervals$split.cutleft <- as.numeric(intervals$split.cutleft)
  return(arrange(intervals, (split.cutleft)))
}

partitionCreator <- function(i, x, data) {
  p <- c(1:(1+nrow(i)))
  bins <- data.frame (bin = rep(1, nrow(data)), v = data[,x])
  for(j in 1:nrow(i)){
    bins[bins$v > i[j,1],1] <- p[j+ 1]
  }
  return(bins$bin)
}

condPermuter <- function(d2, x) {
  for(i in 1:length(unique(d2$p))){
    #set.seed(1)
    ifelse(length(d2[d2$p== i,x]) == 1, d2[d2$p == i, x], 
           d2[d2$p == i, x] <- sample(d2[d2$p== i,x]))
  }
  return(d2[,x])
}

VIdevBagged<- function(b, x){
  return(as.numeric(as.data.frame(b$mtrees[[1]]$btree$frame)[,c(1,4)] %>% filter(var == x) %>% dplyr::summarise(sum = sum(dev))))
}

VIdev<- function(t2, x){
  fr <- as.data.frame(t2$frame)[,c(1,3)]
  if ( x %in% fr$var){
    return(fr %>% dplyr::filter(var == x) %>% dplyr::summarise(sum = sum(dev)))
  } 
  return(0)
}

variable.importance.dev <- function(t1, x, d){
  if(length(t1$frame$splits[,1]) == 1) {
    return(VIdev(t1, x))
  } else {
    interval <- intervalReturn(t1$frame$splits[,1])
    d$p <- partitionCreator(interval, x, d)
    d[,x] <- condPermuter(d, x)
    t2 <- tree(y ~ ., data = d, y = TRUE)
    v1 <- VIdev(t2, x)
    #rm(t1, t2)
    return(v1)
  }
}

inftrees <- function(xs, data, n) {
  x <- names(xs)
  vi <- rep(0,length(x))
  vo <- rep(0, length(x))
  
  t0 <- tree(y~., data)
  
  set.seed(1)
  for(i in 1:length(x)){
    vo[i] <- VIdev(t0, x[i])
  }
     set.seed(1)
      for(i in 1:length(x)){
        form <- as.formula(paste(x[i], "~."))
        ti <- tree(form, xs)
        vi[i] <- variable.importance.dev(ti, x[i], data)
      }
  
  v <- data.frame("variable" = x, "inftree.variable.importance" = as.numeric(vi), "base.variable.importance" = as.numeric(vo))
  return(v)
}


```

```{r d3ch4, echo = FALSE}
w1 <- rnorm(1000)

w2 <- 2*log(abs(w1)) + rnorm(1000)

w3 <- 2*log(abs(w2)) + rnorm(1000)

w4 <- 2*log(abs(w3)) + rnorm(1000)

n <- 1000

rep(0, 8) -> mu #mean of each variable

diag(8) -> sigma #creates a 12 by 12 diagonal matrix with 1's down the diagonal

mvrnorm(n =n, mu = mu, Sigma = sigma) -> simw5w12

data.frame(w1,w2,w3,w4,simw5w12) -> d3
              
c("W1","W2","W3","W4","W5","W6", "W7","W8","W9","W10","W11","W12") -> names(d3)

rep(0, 1000) -> ys #init a vector of zeros
c(5,5,2,0,-5,-5,-2,0,0,0,0,0) -> bts 

rnorm(1000, mean = 0, sd = .5) -> e 

for( i in 1:1000){ #go row by row and create the ys based on the function of e, bts, and d3
ys[i] <- d3[i,1]*bts[1]+
    d3[i,2]*bts[2]+ 
    d3[i,3]*bts[3]+ 
    d3[i,4]*bts[4]+ 
    d3[i,5]*bts[5]+ 
    d3[i,6]*bts[6]+
    d3[i,7]*bts[7]+ 
    d3[i,8]*bts[8]+ 
    d3[i,9]*bts[9]+ 
    d3[i,10]*bts[10]+ 
    d3[i,11]*bts[11]+ 
    d3[i,12]*bts[12] +
    e[i]
}

d3$y <- ys

```

#INFTrees and INFFOREST Variable Importance
##Theory

While conditional variable importance (Strobl et al) conditionally permutes each variable given the structure signified by the model that predicts the response, $Y \sim X_1,...,X_i,...,X_p$, our method conditionally permutes each variable given the structure outlined in a new model with the variable of interest as the response, $X_i \sim X_1,...X_{i-1},X_{i+1},...X_p$. This is not the most straightforward process, as trees partition the sample space, however, in INFTrees these partitions on the variables $X_1,...X_{i-1},X_{i+1},...X_p$ are treated as psuedo partitions on the variable of interest, $X_i$. This is accomplished by first partitioning on the sample predictors $X_1,...X_{i-1},X_{i+1},...X_p$  and then infering the partitions on $X_i$. As a visualizaiton of this, lets return to the $D_{3}$ dataset discussed in chapter 2. 


```{r,echo = FALSE, warning=FALSE, message=FALSE}
test <- sample(1000, 200)

d3lite <- round(d3,2)

d3lite <- dplyr::select(d3lite, W1, W2, y)

t1 <- tree(y~ W1+ W2, data = d3lite[-test,])
t1$frame$yval <- round(t1$frame$yval,2)
t1 <- prune.tree(t1, best = 5)
```

```{r t1ch4, echo = FALSE, fig.cap="A Tree of the Model Y~W1,W2", results="asis"}
plot(t1)
text(t1)
```

Lets say we are interested in the variable importance of $\omega_2$. Then using the conditional variable importance (Strobl et al)'s permutation scheme, we would first look at the partitions on $\omega_2$ from this tree. 


```{r,echo = FALSE, warning=FALSE, message=FALSE}
yhats <- predict(t1, d3lite[test,-5])
d3lite$group <- as.factor(yhats)
d3lite$group <- ifelse(d3lite$group == levels(d3lite$group)[1], "A",
                ifelse(d3lite$group == levels(d3lite$group)[2], "B",
                ifelse(d3lite$group == levels(d3lite$group)[3], "C",
                ifelse(d3lite$group == levels(d3lite$group)[4], "D", "E"))))
d3lite$ys <- yhats

library(ggplot2)
library(plotly)


```



Clearly, the values of $\omega_2$ are less important to the patitioning structure than the interations of $\omega_2$ and the other variables. 



```{r,echo = FALSE, warning=FALSE, message=FALSE}
dp <- d3lite

as <- filter(dp, group == "A")
as$W2 <- sample(as$W2)

dp[dp$group == "A",] <- as
```

```{r p3ch4, echo = FALSE, fig.cap="Partitions on the Predictor Space W2 from Y~W1,..,W4", results="asis"}
ggplot(dp, aes(x=W2, y=W1, color = group)) +
  geom_point()+
  scale_color_manual(values = thesis)
```

As you can see in Figure \@ref(fig:blah) above, ...

Under the INFTrees method, before permuting, fit another tree to the model $\omega_2 \sim \omega_1$ 

```{r,echo = FALSE, warning=FALSE, message=FALSE}
t2 <- tree(W2 ~ W1, data = d3lite[-test,])
t2$frame$yval <- round(t2$frame$yval,2)
t2 <- prune.tree(t2, best = 5)
```

```{r t2ch4, echo = FALSE, fig.cap="A Tree of the Model W2~W1", results="asis"}

plot(t2)
text(t2)
```


The partitions on $\omega_2$ implied by this model are:

```{r,echo = FALSE, warning=FALSE, message=FALSE}
yhats <- predict(t2, d3lite[test,-5])
d3lite$group <- as.factor(yhats)
d3lite$group <- ifelse(d3lite$group == levels(d3lite$group)[1], "A",
                ifelse(d3lite$group == levels(d3lite$group)[2], "B",
                ifelse(d3lite$group == levels(d3lite$group)[3], "C",
                ifelse(d3lite$group == levels(d3lite$group)[4], "D", "E"))))
d3lite$ys <- yhats
```


```{r p4ch4, echo = FALSE, fig.cap="Partitions on the Predictor Space W2 from W2~W1", results="asis"}
ggplot(d3lite, aes(x=W2, y = W1, color = group)) +
  geom_point()+
  scale_color_manual(values = thesis)
```


```{r,echo = FALSE, warning=FALSE, message=FALSE}
dp <- d3lite

as <- filter(dp, group == "A")
as$W2 <- sample(as$W2)

dp[dp$group == "A",] <- as
```

```{r p5ch4, echo = FALSE, fig.cap="The Result of Permuting W2 WRT The Partitions", results="asis"}
ggplot(dp, aes(x=W2, y=W1, color = group)) +
  geom_point()+
  scale_color_manual(values = thesis)
```



###INFTrees

For a CART, $T$, representing the model $Y~X_1,...,X_p$ where $Y,X_1,...,X_p$ are vectors of length n, the INFTrees algorithm proceeds as follows:


\begin{algorithm}
\caption{INFTree, $VI_{inf}(T)$}
\label{inftree}
\begin{algorithmic}
\For{each $X_i \in {{X_1,...,X_p}}$}
\State Calculate: $\Phi_o =  RSS(T, (Y,X_1,..X_p))$
\State Fit the tree $T_{X_i}$, where $T_{X_i} : X_i \sim X_1,...,X_{i-1}, X_{i+1},...X_p$
\State Extract the set $P_{X_i}$ of partitions on $X_i$ from $T_{X_i}$
\State Permute $X_i$ with respect to $P_{X_i}$
\State Find $\Phi^* =  RSS(T, (Y,X_1,..., \bar{X_i},...X_p))$
\State Repeat the above procedure to find the distribution of $\Phi^*$
\State Test the null hypothesis that $\Phi_o$ is the likely value of $RSS(T, (Y,X_1,..X_p))$
\EndFor
\end{algorithmic}
\end{algorithm}

This procedure allows the null hypothesis that Y is independent of $X_i$ given the values of $X_1,...X_{i-1},X_{i+1},...X_p$ to be tested. Therefor, values of $VI_{inf}$ could be compared in a similar manner to the coefficients of linear regression. 

###INFForests

The algorithm for determining $VI_{inf}(R)$ follows similarly.

\begin{algorithm}
\caption{INFForests, $VI_{inf}(R)$}
\label{infforest}
\begin{algorithmic}[1]
\State Fit a random forest, $R$ on the dataset $D$ fitting the model $Y \sim X_1,...,X_p$.
\For{each $X_i \in {{X_1,...,X_p}}$}
\For{each $t \in R$}
\State Calculate: $\Xi_o =  \frac 1 {\nu_t} RSS(t,\bar{B}^t)$
\State Calculate a tree $T_i$ that predicts $X_i \sim X_1,...,X_{i-1}, X_{i+1},...X_p$ using the subset of the observations used to fit $t$  
\State Permute the subset of $X_i$ contained in $\bar{B}_t$ with respect to the set of partions $P_{xi}$ from $T_i$.
\State Now find $\Xi^* =  \frac 1 {\nu_t} RSS(t,\bar{B}_t^*)$
\State The difference between these values, $\Xi^* - \Xi_o$,  is the variable importance for $X_i$ on $t$
\EndFor
\State Test the null hypothesis that $\Xi_o$ is the likely value of $\frac 1 {\nu_t} RSS(t,\bar{B}_t^*)$ using the distribution of values of $\Xi^*$ gathered from each tree in $R$
\EndFor
\end{algorithmic}
\end{algorithm}

##Implementation In `INFTREES` and Results

###Notes on the Implemetation

Implementing the `INFFOREST` and therefor the `INFTREES` algorithms, required creating a suite of functions to create trees and random forests. The trees are fit following the standard two-part CART-like algorithm. [^1] The function chooses a variable to split on with linear correlation with respect to $Y$, but instead of looking for correlations above a certain threshold which is common, it chooses the variable with the highest correlation when compared to its peers. This alleivates the situation where a variable with a non-linear relationship would be passed over again and again. The splitting is done via minimization of the following function with respect to $i$:

$$RSS_{node} (i,X,Y) = RSS_{leaf}(Y|X <i) + RSS_{leaf}(Y|X \geq i) $$
$$RSS_{leaf} = \sum (y - \hat{y})^2 $$
$$\hat{Y}: \hat{y} \in \hat{Y}: \hat{y} = E(Y), \ where\  |\hat{Y}| = |Y|$$

This function considers the regression case only, and only numeric predictors. Leafs are created when the resultant split would be unsatisfactory, i.e. at least one daughter node would have five members or less. This generates very large trees - a quality that is not an issue in random forests but may be problematic in a stand-alone setting. At this time, there is also no function to prune the trees. 

The INFTREE function follows the algorithm above *reference*. The partitions on $X_j$ are generated by fitting a tree, $T$, to the model $X_j \sim X_1,..., X_{j-1}, X_{j+1},..X_p$ and calculating the predictions $T(X_1,..., X_{j-1}, X_{j+1},..X_p)$. Then permuting $X_j$ with respect to the partitions on $X_j$ given by those predictions. For example, if $x_j \in X_j$ and the value of $T(x_1,..., x_{j-1}, x_{j+1},..x_p)$ corresponding to $x_j$ is $\alpha$, $x_j$ is permuted along with the other values of $X_j$ that also have $T(x_1,..., x_{j-1}, x_{j+1},..x_p)$ corresponding to $\alpha$.



[^1]: A great deal of effort was undertaken by the author to find the defenative, authentic CART algorithm. This implementation follows the rough strokes set out in the 1984 text *Classification and Regression Trees* to the best of the author's ability and may not be exactly the algorithm found in R packages like 'tree()'


###Results 


*NOTE* INFFOREST, like any random forest method involving tree- level calculations is a computationally intensive function. The forests are large, unpruned at any level, and INFFOREST takes time to compute. Because of this reason the datasets discussed in CH2 have been altered so that instead of 1000 x 13 dimensional datasets they are 400 x 13. This decreases computation time immensely. (**see figure ___ in appendix**)

**FIGURE OF INFFOREST DISTRIBUTION FOR EACH DATASET**

There a little suprises in the distru

In the situation where there is little correlation between the predictors, the distribution of the INFFOREST output is a sharp peak ending at one of the end points, zero or one. When there are, however, strong correlations between the predictor variables, and `mtry` is suitably large but smaller than `p`, the trees in the forest must decide between them. In these situations, the INFFOREST distribution is multimodal, with one peak at one end of the interval, $INFFOREST(X_i) = 1$ and another when $INFFOREST(X_i) = 0$. 

To demonstrate this situation, take the dataset $D2$, as described above. In the random forest corresponding to this model, the variables $X2$ and $X3$ are considered substitutes for each other. In the trees where $X2$ has $INFFOREST = 1$, $X3$ has $INFFOREST <<$ and visa versa. 

**FIGURE OF BOTH INFFOREST DISTRIBUTIONs OF X2 AND X3 TOGETHER FOLLOWED BY THEIR DISTRIBUTIONS CORRESPONDING TO THE SAME TREES**

(i.e. the INFFOREST distributions of X2 and X3 in the trees where X3< .5)

Of course, one may be inclined to infer a p-value for the null hypothesis that $INFFOREST = 0$ for each of these variables. This could be done straight-forwardly enough in situations where there is not strong multicolinearity within the predictors as the distributions are relaibly half of the familiar bell shaped curve centered around either zero or one. It would be quite difficult, however, for INFFOREST alone to test the significance of the INFFOREST distribution corresponding to correlated, paired predictors and it may not makes sense to do so at all. *talk with Andrew about fixing this?* 

 

