---
header-includes:
- \usepackage{graphicx,latexsym}
- \usepackage{amssymb,amsthm,amsmath}
- \usepackage{longtable,booktabs,setspace}
- \usepackage{algorithm}
- \usepackage{algpseudocode}
- \usepackage{pifont}
output: pdf_document
---
```{r include_reedtemplates_4, include = FALSE}
# This chunk ensures that the reedtemplates package is installed and loaded
# This reedtemplates package includes the template files for the thesis and also
# two functions used for labeling and referencing
if(!require(devtools))
  install.packages("devtools", repos = "http://cran.rstudio.com")

if(!require(reedtemplates)){
  library(devtools)
  devtools::install_github("ismayc/reedtemplates")
  }
library(reedtemplates)

library(tree)
library(reedtemplates)
library(MASS)
library(ggplot2)
library(randomForest)
library(maptree)
library(knitr)
library(GGally)
library(reshape)
library(ggplot2)
library(plotly)
library(gridExtra)

#read_chunk('~/Desktop/Thesis/Thesis/infrf.script.v2.R')

#flights <- read.csv("data/flights.csv")
thesis <- c("#245e67", "#90bd98", "#cfd0a0", "#c49e46", "#d28383")
```

```{r infforestcode, echo=FALSE}

############################################################################################
#####################################INFFORESTS#############################################
############################################################################################
############################################################################################

########################################TESTING#############################################

# data("iris")
# d <- iris[1:4]
# y <- iris$Sepal.Length
# xs <- iris[2:4]
# mtry <- 2
# form <- as.formula("Sepal.Length ~.")
# 
# ######from chap2.Rmd
# ##d1
# 
# d <- d2[1:100,]
# xs <- d2[1:100, 1:12]
# y <- d2$y[1:100]
# mtry = 6
# form <- as.formula("y~.")
# 
# t1 <- tree.rf(y,xs,mtry)
# 
# start <- sys.time()
# r <- rforest(y,xs, mtry, ntree = 100)
# print(start - Sys.time())
# 
# p <- predict.tree.rf(r[[1]], y[-r[[1]][[1]]], xs[-r[[1]][[1]],])
# 
# vi <- inftrees(r[[2]], y[-r[[2]][[1]]], xs[-r[[2]][[1]],])
# 
# virf <- infforest(r,y,xs)
# virfStrobl <- conditional.inf(r,y,xs)
# virf_permuted <- permuted.inf(r,y,xs)
# 
# 
# 
# strobltrees(r[[3]],y[-r[[3]][[1]]],xs[-r[[3]][[1]],])


###################################GROWING A TREE#########################################


###################################GROWING A TREE#########################################
rss.node <- function(i,y,x) { 
  yleftpred <- mean(y[x< i])
  yrightpred <- mean(y[x >= i])
  rssl <- sum((y[x< i]-yleftpred)^2)
  rssr <- sum((y[x >= i] - yrightpred)^2)
  return(rssl + rssr)
}

rss.leaf <- function(a){
  a1 <- rep(mean(a), length(a))
  sum((a-a1)^2)
}

max.cor <- function(yy,sxs){
  
  max <- list()
  cors <- cbind(rep(0,ncol(sxs)),seq(1,ncol(sxs)))
  for(i in 1:ncol(sxs)){
    cors[i,1] <- ifelse(sd(sxs[,i]) == 0,0, suppressWarnings(cor(yy, sxs[,i])))
  }
  
  cors[is.na(cors[,1]), 1] <- 0
  
  if (sum(cors[,1] == max(abs(cors[,1]), na.rm = TRUE), na.rm = TRUE) > 1) {
    
    if(sum(abs(cors[,1])) == 0){
      
      return(NULL)
    }
    max1 <- sample(as.numeric(cors[,1] == max(abs(cors[,1]), na.rm = TRUE)), 1)  
    
  } else {
    
    max1 <- cors[abs(cors[,1]) == max(abs(cors[,1]), na.rm = TRUE),2]
  }
  
  
  max[[length(max)+1]] <- sxs[,max1]
  max[[length(max)+1]] <- names(sxs)[max1]
  return(max)
  
}

split.rf <- function(y,x, min) {
  
  sp <- list()
  if(length(x) <= min) {
    
    return(NULL)
  } else {
    
    #gives a warning if it cannot find an optimal solution and instead gives the max point
    op.partition <- suppressWarnings(optimise(rss.node, interval = range(x), 
                                              upper = max(x), y=y,x=x, maximum = FALSE))
    
    
    if (length(x[x < op.partition[[1]]]) < min | length(x[x >= op.partition[[1]]]) < min){
      
      return(NULL)
    }
    sp[[length(sp)+1]] <- op.partition[[1]] #partition on x
    sp[[length(sp)+1]]<-  mean(y[x< op.partition[[1]]]) #ypred left daughter
    sp[[length(sp)+1]]<- mean(y[x >= op.partition[[1]]]) #ypred right daughter
    sp[[length(sp)+1]]<- x[x < op.partition[[1]]] #xsleftdaughter
    sp[[length(sp)+1]]<- x[x >= op.partition[[1]]] #xsrightdaughter
    sp[[length(sp)+1]] <- rss.leaf( y[x < op.partition[[1]]]) #ldrss
    
    return(sp)
  }
}

tree.rf <- function(y,xs, mtry, bootsampled){
  tree <- list()
  tree.frame <- list()
  tree.exc <- list()
  leaf.partitions <- list()
  bootsample <- seq(1, length(y))
  if( length(y) != nrow(xs)){
    return("error, length y != dim xs")
  }
  
  noder <- function(y,xs,mtry, min){
    
    ##what i want this function to do:
    ###given y,xs (like an interval of them):
    ####1. find the split between them 
    
    frame <- node1(y,xs, mtry, min)
    #df <-  rbind(df,frame)
    ####2. call itself on each side of the split
    split <- as.numeric(frame[5])
    spliton <- frame[1]
    
    if(frame[1] == "<leaf>"){
      return(frame)
    } else {
      
      yhatl <- y[xs[,spliton[[1]]] < split]
      xsl <- xs[xs[,spliton[[1]]] < split,]
      yhatr <- y[xs[,spliton[[1]]] >= split]
      xsr <- xs[xs[,spliton[[1]]] >= split, ]
      
      #    if(length(yhatl) < min & length(yhatr) < min){
      #      print(df)
      #      return(df)
      #    } else {
      #y = yhatr
      #xs = xsr
      noder(y = yhatr,xs = xsr,mtry, min)
      noder(y = yhatl,xs = xsl,mtry, min)
    }
    #  }
  }
  node1 <- function(y, xs, mtry, min){
    
    xssrd <- xs[,sample(ncol(xs),mtry)]
    In <- names(xs) %in% names(xssrd)
    In <- !In 
    NotIn <- names(xs)[In]
    
    frame <- data.frame("", 0,0,0,0)
    frame[,1] <- as.character(frame[,1])
    leaf.p <- data.frame(rep(0,2), rep(0,2), rep(0,2))
    
    
    if(length(y) < min) {
      frame[1,] <- c("<leaf>",nrow(xssrd), rss.leaf(y), mean(y), 0)
      
      tree.frame <<- rbind(tree.frame, frame)
      leaf.p <- sapply(xs, range)
      leaf.partitions[[length(leaf.partitions)+1]] <<- leaf.p
      return(frame)
      
    } else {
      
      maxxr <- max.cor(yy = y,sxs= xssrd)
      sprd <- split.rf(y, maxxr[[1]], min)
      if(is.null(sprd)) {
        frame[1,] <- c("<leaf>",nrow(xssrd),rss.leaf(y),  mean(y), 0)
        
        tree.frame <<- rbind(tree.frame, frame)
        leaf.p <- sapply(xs, range)
        leaf.partitions[[length(leaf.partitions)+1]] <<- leaf.p
        return(frame)
        
      } else if(length(maxxr[[1]] < sprd[[1]]) < min |length(maxxr[[1]] >= sprd[[1]]) < min  ) {
        
        frame[1,] <- c("<leaf>",nrow(xssrd),rss.leaf(y), mean(y), 0)
        
        tree.frame <<- rbind(tree.frame, frame)
        leaf.p <- sapply(xs, range)
        leaf.partitions[[length(leaf.partitions)+1]] <<- leaf.p
        return(frame)
      }
      
      frame[1,] <- c(maxxr[[2]],
                     nrow(xssrd),
                     rss.node(sprd[[1]], y, maxxr[[1]]),
                     mean(y),
                     sprd[[1]])
      # frame[2:4] <- as.numeric(frame[2:4])
    }
    tree.exc <<- rbind(tree.exc, NotIn)
    tree.frame <<- rbind(tree.frame, frame)
    return(frame)
  }
  
  if(bootsampled){
    bootsample <- sample(length(y), length(y), replace = TRUE)
    xs <- xs[bootsample,]
    y <- y[bootsample]
    
  }
  min <- 5
  noder(y, xs, mtry,min)
  names(tree.frame) <- c("var", "n", "dev", "ypred","split.cutleft")
  tree[[1]] <- bootsample
  tree[[2]] <- tree.frame
  tree[[3]] <- tree.exc
  return(tree)
}

###################################AUX TREE FUNCTIONS#######################################
predict.tree.rf <- function(t,xs) {
  t <- t[[2]] 
  first.split <- t[1,]
  if(sum(is.na(t$ypred)) > 0){
    return(NULL)
  }
  if(first.split$var == "<leaf>") {
    return(first.split$ypred)
  }
  
  t$n <- as.numeric(t$n)
  t$split.cutleft <- as.numeric(t$split.cutleft)
  rdn <- t$n[2]
  ldn <- t$n[1] - rdn 
  ldname <- as.numeric(row.names(t[t$n == ldn,]))
  
  if(length(ldname) > 1) {
    
    right.daughter <- t[1:(ldname[length(ldname)]-1),]
    left.daughter <- t[ldname[length(ldname)]:nrow(t),]
    
  } else {
    
    right.daughter <- t[1:(ldname[length(ldname)]),]
    left.daughter <- t[ldname:nrow(t),]
  }
  
  
  j <- 0
  predictions <- c(rep(100000, nrow(xs)))
  ###SKIPPING CONDITION
  #there's no need to map out the whole tree and have it on file, each y just needs the tree to 
  #a. check the split condition, i.e. xs[,var] < split.cutleft 
  #b. if yes, go two ahead
  #b.2. if no, go one ahead
  #c. stop at leaf, pred[i] <- ypred
  
  for(i in 1:(nrow(xs))){
    
    xsh <- xs[i,]
    
    if(xsh[,first.split$var] < first.split$split.cutleft){
      #left daughter
      ld <- left.daughter[1,]
      j <- 1
      while (ld$var != "<leaf>"){
        if (xsh[,ld$var] < ld$split.cutleft) {
          j <- j+2
        } else {
          j <- j+1
        }
        ld <- left.daughter[j,]
      }
      predictions[i] <- ld$ypred
    } else {
      #right daughter
      rd <- right.daughter[2,]
      j <- 2
      while (rd$var != "<leaf>"){
        if (xsh[,rd$var] < rd$split.cutleft) {
          j <- j+2
        } else {
          j <- j+1
        }
        rd <- right.daughter[j,]
      }
      predictions[i] <- rd$ypred
    }
  }
  return(predictions)
}

###################################GROWING A FOREST#########################################

rforest <- function(y, xs,  mtry, ntree) { 
  #imputs: formula to be tested and the dset to test on. outputs: list,
  #contains: total oob error, trees, and bootsample for the tree  
  rforest <- list() #define empty list for the rf. length = ntree*2+1
  #where each tree is a list, bootsample + frame  
  xsi <- c()
  di <- data.frame()
  # form <- as.character(form)
  # yn <- form[2]
  # yi <- as.numeric(names(d) == yn)
  # y <- d[,yi]
  # xs <- d[,(yi + 1 - 2)*-1]
  
  while (length(rforest) < ntree) {
    rforest[[length(rforest)+1]] <- tree.rf(y,xs,mtry, bootsampled = TRUE)
  }
  return(rforest)
}


###################################INFFOREST#################################################

infforest <- function(rf,y,xs) {
  v <- inftrees(rf[[1]],y[-rf[[1]][[1]]],xs[-rf[[1]][[1]],])
  vi.frame <- v
  for(i in 2:(length(rf))){ ##random forest is a list, pairs of trees + bootsamples
    t <- rf[[i]]
    vi.frame <- rbind(vi.frame,inftrees(t, y[-t[[1]]], xs[-t[[1]],]))
    
    ##send tree off to inftrees with the -bootsampled data
    ##get back frame
  }
  
  ##generate distribution of vi's from frames - pval
  return(vi.frame)
}


infforest.test <- function(inf){
  p <- rep(10000, ncol(inf))
  mu <- c()
  sd <- c()
  for(i in 1:ncol(inf)){
    mu[i] <- mean(inf[,i])
    sd[i] <- sd(inf[,i])
    p[i] <- pnorm(0,mean = mu[i], sd = sd[i])
  }
  return(data.frame("var" =  names(as.data.frame(inf)), "P(var > 0)"= p, "mean" = mu, "sd" = sd))
}

##############################CONDITIONAL VARIABLE IMPORTANCE################################

conditional.inf <- function(rf, y, xs) {
  v <- strobltrees(rf[[1]],y[-rf[[1]][[1]]],xs[-rf[[1]][[1]],])
  v1 <- v
  for (i in 2:length(rf)){
    v1 <-  strobltrees(rf[[i]],y[-rf[[1]][[1]]],xs[-rf[[1]][[1]],])
    v <- rbind(v,v1)
  }
  
  return(v)
}

###############################PERMUTED VARIABLE IMPORTANCE################################

permuted.inf <- function(rf, y, xs) {
  v <- briemantrees(rf[[1]],y[-rf[[1]][[1]]],xs[-rf[[1]][[1]],])
  
  for (i in 2:length(rf)){
    v <- rbind(v, briemantrees(rf[[1]],y[-rf[[1]][[1]]],xs[-rf[[1]][[1]],]))
  }
  
  return(v)
  
}

###################################INFTREES SUPPORT###########################################

inftrees <- function(t, y, xs) {
  
  rss.post <- rep(0, ncol(xs))
  vi <- rep(0, ncol(xs))
  
  ta <- t
  t <- t[[2]]
  
  # set.seed(1)
  #for(i in 1:ncol(xs)){
  #  rss.pre[i] <- VIdev(t, names(xs)[i])
  #}
  
  rss.pre <- sum((y - as.numeric(predict.tree.rf(ta, xs)))^2)/length(y)
  
  set.seed(1)
  for(i in 1:ncol(xs)){
    xs.for.permuting.i <- xs
    ti <- tree.rf(xs[,i], xs[,-i], mtry = ncol(xs[,-i]), bootsample = FALSE) 
    xi.partitions <- predict.tree.rf(ti, xs[,-i])
    xs.for.permuting.i$groups <- as.factor(xi.partitions)
    
    for (j in 1:length(levels(xs.for.permuting.i$groups))) {
      xs.for.permuting.i[xs.for.permuting.i$group == levels(xs.for.permuting.i$groups)[j],i] <-
        sample(xs.for.permuting.i[xs.for.permuting.i$group == levels(xs.for.permuting.i$groups)[j],i], 
               length(xs.for.permuting.i[xs.for.permuting.i$group == levels(xs.for.permuting.i$groups)[j],i]),
               replace = TRUE)
    }
    
    predictions.for.y.xi.perm <- predict.tree.rf(ta,xs.for.permuting.i)
    rss.post[i] <- abs(sum((y - as.numeric(predictions.for.y.xi.perm))^2)/length(y) - rss.pre)
    ex <- sum(names(xs)[i] ==  ta[[3]])/nrow(ta[[3]])
    rss.post[i] <- rss.post[i]*ex
  }
  names(rss.post) <- names(xs)
  return(rss.post)
}
# 
# ytrees <- function(t,y,xs) {
#   d <- rbind(y,xs)
#   rss.post <- rep(0, ncol(xs))
#   ta <- t
#   t <- t[[2]]
#   predictions <- as.numeric(predict.tree.rf(ta, xs))
#   rss.pre <- sum((y - predictions)^2)/length(y)
#   xs.for.permuting.i <- xs
#   yperm <- y
#   for(i in 1:ncol(xs)){
#     corrr <- sapply(xs[,-i],cor, y = xs[,i])
#     xs.i <- xs[,-i]
#     xs.i <-  (xs.i[,corrr > .2])
#     xs.i <- cbind(xs[,i], xs.i)
#     if(ncol(xs.i) == 1){
#       yperm <- sample(y)
#       rss.post[i] <- abs(sum((yperm - predictions)^2)/length(y) - rss.pre)
#       xs.i <- xs
#        yperm <- y
#     } else {
#       fr <- t[t$var %in% names(xs.i)[-1],]
#       if(nrow(fr) == 0) {
#       yperm <- sample(y)
#       rss.post[i] <- abs(sum((yperm - predictions)^2)/length(y) - rss.pre)
#       xs.i <- xs
#       yperm <- y
#       } else {
#         for (j in 1:length(levels(as.factor(fr$var)))) {
#           fri <- fr[fr$var == levels(as.factor(fr$var))[j],]
#           for(n in 1:nrow(fri)){
#             yperm[xs.i[,fri$var[n]] < fri$split.cutleft[n]] <- sample(y[xs.i[,fri$var[n]] < fri$split.cutleft[n]],1])
#           }
#         }
#         rss.post[i] <- abs(sum((yperm - predictions)^2)/length(y) - rss.pre)
#         xs.i <- xs
#         yperm <- y
#       }
#     }
#   }
#   if(sum(abs(rss.post)) == 0) {
#     return(rss.post)  
#   } else{
#     rss.post <- rss.post/max(abs(rss.post))
#     # names(rss.post) <- names(xs)
#     return(rss.post)
#   }
#   
# }


strobltrees <- function(t,y,xs) {
  rss.post <- rep(0, ncol(xs))
  
  ta <- t
  t <- t[[2]]
  
  predictions <- as.numeric(predict.tree.rf(ta, xs))
  rss.pre <- sum((y - predictions)^2)/length(y)
  
  xs.for.permuting.i <- xs
  
  for(i in 1:ncol(xs)){
    corrr <- sapply(xs[,-i],cor, y = xs[,i])
    xs.i <- xs[,-i]
    xs.i <-  (xs.i[,corrr > .2])
    xs.i <- cbind(xs[,i], xs.i)
    
    if(ncol(xs.i) == 1){
      xs.i <- sample(xs.i)
      xs.ii <- xs
      xs.ii[,i] <- xs.i[,1]
      names(xs.ii) <- names(xs)
      predictions.for.y.xi.perm <- predict.tree.rf(ta,xs.ii)
      rss.post[i] <- abs(sum((y - as.numeric(predictions.for.y.xi.perm))^2)/length(y) - rss.pre)
      xs.i <- xs
    } else {
      
      fr <- t[t$var %in% names(xs.i)[-1],]
      if(nrow(fr) == 0) {
        xs.i <- sample(xs.i)
        xs.ii <- xs
        xs.ii[,i] <- xs.i[,1]
        names(xs.ii) <- names(xs)
        predictions.for.y.xi.perm <- predict.tree.rf(ta,xs.ii)
        rss.post[i] <- abs(sum((y - as.numeric(predictions.for.y.xi.perm))^2)/length(y) - rss.pre)
        xs.i <- xs
      } else {
        for (j in 1:length(levels(as.factor(fr$var)))) {
          fri <- fr[fr$var == levels(as.factor(fr$var))[j],]
          for(n in 1:nrow(fri)){
            xs.i[xs.i[,fri$var[n]] < fri$split.cutleft[n] ,1] <- sample(xs.i[xs.i[,fri$var[n]] < fri$split.cutleft[n] ,1])
          }
        }
        xs.ii <- xs
        xs.ii[,i] <- xs.i[,1]
        names(xs.ii) <- names(xs)
        predictions.for.y.xi.perm <- predict.tree.rf(ta,xs.ii)
        rss.post[i] <- abs(sum((y - as.numeric(predictions.for.y.xi.perm))^2)/length(y) - rss.pre)
        xs.i <- xs
        ex <- sum(names(xs)[i] ==  ta[[3]])/nrow(ta[[3]])
        if(ex > .7){
          rss.post[i] <- rss.post[i] * (ex+1)
        }
      }
    }
  }
  if(sum(abs(rss.post)) == 0) {
    return(rss.post)  
  } else{
    rss.post <- rss.post/max(abs(rss.post))
    # names(rss.post) <- names(xs)
    return(rss.post)
  }
  
}

breimantrees <- function (t,y,xs){
  ta <- t
  t <- t[[2]]
  
  rss.pre <- sum((y - as.numeric(predict.tree.rf(ta, y, xs)))^2)
  
  rss.post <- rep(0, ncol(xs))
  xs.for.permuting <- xs
  for (i in 1:ncol(xs)){
    xs.for.permuting[,i] <- sample(xs.for.permuting[,i], replace = TRUE)
    predictions.for.y.xi.perm <- predict.tree.rf(ta,y,xs.for.permuting)
    rss.post[i] <- abs(sum((y - as.numeric(predictions.for.y.xi.perm))^2) - rss.pre)
    xs.for.permuting <- xs
  }
  rss.post <- rss.post/max(abs(rss.post))
  names(rss.post) <- names(xs)
  return(rss.post)
}


```
```{r loaddatad2, echo=FALSE}
rep(0, 12) -> mu #mean of each variable

diag(12) -> sigma #creates a 12 by 12 diagonal matrix with 1's down the diagonal

sigma -> sigma1
.9 -> sigma1[1,2]
.9 -> sigma1[2,1] #adding the block correlation between the first four variables
.9 -> sigma1[1,3]
.9 -> sigma1[3,1]
.9 -> sigma1[3,2]
.9 -> sigma1[2,3]
.9 -> sigma1[1,4]
.9 -> sigma1[4,1]
.9 -> sigma1[2,4]
.9 -> sigma1[4,2]
.9 -> sigma1[4,3]
.9 -> sigma1[3,4]

1000 -> n #number of observations
set.seed(1)
mvrnorm(n =n, mu = mu, Sigma = sigma1) -> sim1000 #sampling from the multivariate normal 
c(5,5,2,0,-5,-5,-2,0,0,0,0,0) -> bts #these are the betas 

rnorm(1000, mean = 0, sd = .5) -> e #error terms

rep(0, 1000) -> ys #init a vector of zeros

for( i in 1:1000){ #go row by row and create the ys based on the function of e, bts, and sim1000
ys[i] <- sim1000[i,1]*bts[1]+
    sim1000[i,2]*bts[2]+ 
    sim1000[i,3]*bts[3]+ 
    sim1000[i,4]*bts[4]+ 
    sim1000[i,5]*bts[5]+ 
    sim1000[i,6]*bts[6]+
    sim1000[i,7]*bts[7]+ 
    sim1000[i,8]*bts[8]+ 
    sim1000[i,9]*bts[9]+ 
    sim1000[i,10]*bts[10]+ 
    sim1000[i,11]*bts[11]+ 
    sim1000[i,12]*bts[12] +
    e[i]
}

d1 <- as.data.frame(sim1000)
d1$y <- ys



n <- 10000
c(5,5,2,0,-5,-5,-2,0,0,0,0,0) -> bts #these are the betas 
rnorm(1000, mean = 0, sd = .5) -> e #error terms

#library(MASS)
x1 <- rnorm(1000)

x2 <- x1 + rexp(1000)

x3 <- x2 + rexp(1000)

x4 <- x3 + rexp(1000)

diag(8) -> sigma

rep(1, 8)-> mu

mvrnorm(n =n, mu = mu, Sigma = sigma) -> simv5v12

data.frame(x1,x2,x3,x4,simv5v12) -> d2
              
c("X1","X2","X3","X4","X5","X6", "X7","X8","X9","X10","X11","X12") -> names(d2)

rep(0, 1000) -> ys #init a vector of zeros

for( i in 1:1000){ #go row by row and create the ys based on the function of e, bts, and d2
ys[i] <- d2[i,1]*bts[1]+
    d2[i,2]*bts[2]+ 
    d2[i,3]*bts[3]+ 
    d2[i,4]*bts[4]+ 
    d2[i,5]*bts[5]+ 
    d2[i,6]*bts[6]+
    d2[i,7]*bts[7]+ 
    d2[i,8]*bts[8]+ 
    d2[i,9]*bts[9]+ 
    d2[i,10]*bts[10]+ 
    d2[i,11]*bts[11]+ 
    d2[i,12]*bts[12] +
    e[i]
}

d2$y <- ys

w1 <- rnorm(1000)

w2 <- 2*log(abs(w1)) + rnorm(1000)

w3 <- 2*log(abs(w2)) + rnorm(1000)

w4 <- 2*log(abs(w3)) + rnorm(1000)

mvrnorm(n =n, mu = mu, Sigma = sigma) -> simw5w12

data.frame(w1,w2,w3,w4,simw5w12) -> d3
              
c("W1","W2","W3","W4","W5","W6", "W7","W8","W9","W10","W11","W12") -> names(d3)

rep(0, 1000) -> ys #init a vector of zeros

for( i in 1:1000){ #go row by row and create the ys based on the function of e, bts, and d3
ys[i] <- d3[i,1]*bts[1]+
    d3[i,2]*bts[2]+ 
    d3[i,3]*bts[3]+ 
    d3[i,4]*bts[4]+ 
    d3[i,5]*bts[5]+ 
    d3[i,6]*bts[6]+
    d3[i,7]*bts[7]+ 
    d3[i,8]*bts[8]+ 
    d3[i,9]*bts[9]+ 
    d3[i,10]*bts[10]+ 
    d3[i,11]*bts[11]+ 
    d3[i,12]*bts[12] +
    e[i]
}

d3$y <- ys

```

#Chapter 4

##INFFORESTS

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The INFFOREST variable importance is a method of permuted variable importance not unlike that of conditionally permuted variable importance (Algorthim 3). Permuted variable importance is calculated at the tree level, using the partitions on $X_j$ from a tree created to predict the model $X_j \sim X_1,..., X_{j-1}, X_{j+1},...,X_p$. This auxiliary tree is fit by considering all $p-1$ predictors at each split and so may be quite large or quite small depending on the richness of the correlation structure around $X_j$. The auxiliary tree is also fit using the OOB sample for the tree at question. If the auxiliary tree results in a single leaf, i.e. there are no splits, then $X_j$ is permuted blindly, without partitions. If the auxiliary tree results in two leaves, there will be two partitions on $X_j$ to permute $X_j$ within, and so on. After permuting $X_j$ within these partitions, the RSS is calculated for that tree using the OOB sample of predictors, including the now-permuted $X_j$. The absolute difference of the RSS after permutation and the RSS with the untouched OOB sample is INFFOREST variable importance for that tree. Note that for this reason, the INFFOREST variable importance is always greater than or equal to zero, and is standardized by the max INFFOREST variable importance value given by that tree. As the variable importance values are calculated for each tree for each variable, once the method is completed there is a distribution of potential variable importance values for $X_j$, one for each tree. These distributions may or may not be normal, depending on the multicollinearity of the predictors. The INFFOREST variable importance algorithm works as follows:	

\begin{algorithm}
\caption{INFForests, $VI_{inf}(R)$}
\label{infforest}
\begin{algorithmic}[1]
\State Fit a random forest, $R$ on the dataset $D$ fitting the model $Y \sim X_1,...,X_p$.
\For{each $X_i \in {{X_1,...,X_p}}$}
\For{each $t \in R$}
\State Calculate: $\Xi_o =  \frac 1 {\nu_t} RSS(t,\bar{B}^t)$
\State Calculate a tree $T_i$ that predicts $X_i \sim X_1,...,X_{i-1}, X_{i+1},...X_p$ using the subset of the observations used to fit $t$  
\State Permute the subset of $X_i$ contained in $\bar{B}_t$ with respect to the set of partitions $P_{xi}$ from $T_i$.
\State Now find $\Xi^* =  \frac 1 {\nu_t} RSS(t,\bar{B}_t^*)$
\State The difference between these values, $\Xi^* - \Xi_o$,  is the variable importance for $X_i$ on $t$
\EndFor
\State Test the null hypothesis that $0$ is the likely value of $\frac 1 {\nu_t} RSS(t,\bar{B}_t^*)$ using the distribution of values of $\Xi^*$ gathered from each tree in $R$
\EndFor
\end{algorithmic}
\end{algorithm}

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;INFFOREST variable importance operates under the null hypothesis that $Y$ is independent of $X_J$ given the correlation structure of $X_j$ and the other $p-1$ predictors, or that the true INFFOREST variable importance for $X_j$ is 0. The alternative hypothesis is that $Y$ and $X_j$ are not independent given the correlation structure of $X_j$ and the other predictors or that the INFFOREST variable importance for $X_j$ is greater than zero.  After INFFOREST values have been computed for the entire forest, they are treated as samples from the population of possible INFFOREST values for $X_j$ given the random forest $R_f$, and a significance test can be under the null hypothesis. 


##Implementation In `INFTREES` and Results

###Notes on the Implemetation

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Implementing the `INFFOREST` and therefor the `INFTREES` algorithms, required creating a suite of functions to create trees and random forests. The trees are fit following the standard two-part CART-like algorithm. [^1] The function chooses a variable to split on with linear correlation with respect to $Y$, but instead of looking for correlations above a certain threshold which is common, it chooses the variable with the highest correlation when compared to its peers. This alleviates the situation where a variable with a non-linear relationship would be passed over again and again. The splitting is done via minimization of the following function with respect to $i$:

$$RSS_{node} (i,X,Y) = RSS_{leaf}(Y|X <i) + RSS_{leaf}(Y|X \geq i) $$
$$RSS_{leaf} = \sum (y - \hat{y})^2 $$
$$\hat{Y}: \hat{y} \in \hat{Y}: \hat{y} = E(Y), \ where\  |\hat{Y}| = |Y|$$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This function considers the regression case only, and only numeric predictors. Leafs are created when the resultant split would be unsatisfactory, i.e. at least one daughter node would have five members or less. This generates very large trees - a quality that is not an issue in random forests but may be problematic in a stand-alone setting. At this time, there is also no function to prune the trees. 


####Table 2: A Home-Grown Tree on $Y~X_1+X_2+X_3+X_4$

```{r, echo=FALSE, warning=FALSE, message=FALSE, cache=TRUE}
t1 <- tree.rf(d2$y[1:50],d2[1:50, 1:4],mtry = 4, bootsampled = FALSE)
t <- t1[[2]]
t$var[t$var == "<leaf>"] = "leaf"
kable(t)
```

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The tree output is read in the following way: each row corresponds to a node of the tree which considers `n` observations. The mean of the $Y$ values included in the node are `ypred`. If there is an optimal and allowable split, [^2] then the chosen variable, `var`, and the $RSS_{node}$, `dev`, are recorded.[^3] The value of the variable in question that acts as the split point is recorded as `split.cutleft`. If there is no split on the node in question, then `var` will be recorded as `<leaf>` and the `dev` value will be the value of $RSS_{leaf}$ at this node. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The tree output is read roughly from top to bottom, with a coda in the middle. The first row corresponds to the first node, or the node that includes the entire dataset. The second row is the beginning of the right subtree or the right daughter of first node. This pattern continues, favoring the right daughter, until a leaf is reached. The left daughter of the first node is found after all of the splits off of the right daughter have finished but is easily identified as the row with a value of `n` that is exactly the difference between the `n` values of the first two rows. In the case where the right daughter contained many more observations of the original dataset, there may be a node within the right subtree that contains the same number of observations as the left daughter of the first node. In this case, the left daughter is simply the second row with this property. The pattern of following the right daughter until a leaf is reached continues with the left subtree. 




[^3]: Recall that we only allow splits to take place that split the data into two groups, each with more than five members. 

[^4]: It's the convention to call the $RSS_{node}$ the deviance at a node $N$, but, of course, this only makes sense when the node is a leaf. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The INFTREE function follows the algorithm referenced earlier. The partitions on $X_j$ are generated by fitting a tree, $T$, to the model $X_j \sim X_1,..., X_{j-1}, X_{j+1},..X_p$ and calculating the predictions $T(X_1,..., X_{j-1}, X_{j+1},..X_p)$. Then permuting $X_j$ with respect to the partitions on $X_j$ given by those predictions. For example, if $x_j \in X_j$ and the value of $T(x_1,..., x_{j-1}, x_{j+1},..x_p)$ corresponding to $x_j$ is $\alpha$, $x_j$ is permuted along with the other values of $X_j$ that also have $T(x_1,..., x_{j-1}, x_{j+1},..x_p)$ corresponding to $\alpha$.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The values of $INFFOREST(X_j)$ are scaled in the following way: since the INFFOREST function computes the INFFTREES, (or the difference in post and pre permutation RSS), values in a tree-wise manner, each tree's values are divided by the maximum value. This ensures that the values are between zero and one, and that in each tree one variable is clearly deemed the *most important*. 

[^5]: A great deal of effort was undertaken by the author to find the definitive, authentic CART algorithm. This implementation follows the rough strokes set out in the 1984 text *Classification and Regression Trees* to the best of the author's ability and may not be exactly the algorithm found in R packages like 'tree()'

###Results 

```{r forest,echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
set.seed(1)
 
rf1 <- rforest(d1$y,d1[, 1:12],mtry = 4,ntree = 50)

rf2 <- rforest(d2$y,d2[, 1:12],mtry = 4,ntree = 100)
# rf3 <- rforest(d3$y[1:200],d3[1:200, 1:12],mtry = 4,ntree = 200)
```

```{r infforest,echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
set.seed(3)

inf1 <- infforest(rf1, d1$y,d1[, 1:12])
inf2 <- infforest(rf2, d2$y,d2[, 1:12])
# inf3 <- infforest(rf3, d3$y[1:200],d3[1:200, 1:12])
inf1[inf1 == "NaN"] <- NA
inf1[inf1 == "Inf"] <- NA
inf11 <- inf1[complete.cases(inf1),]

inf2[inf2 == "NaN"] <- NA
inf2[inf2 == "Inf"] <- NA
inf22 <- inf2[complete.cases(inf2),]
# 
# inf3[inf3 == "NaN"] <- NA
# inf3[inf3 == "Inf"] <- NA
# inf33 <- inf3[complete.cases(inf3),]
```

```{r mtry, echo=FALSE, message=FALSE, cache=TRUE, warning=FALSE}
v <- as.data.frame(inf11)
v$mtry <- rep(4, nrow(v))
set.seed(1)
r12 <- rforest(d1$y,d1[, 1:12],mtry = 6,ntree = 100)
v12 <- as.data.frame(infforest(r12, d1$y,d1[, 1:12]))
v12[v12 == "NaN"] <- NA
v12[v12 == "Inf"] <- NA
v12 <- v12[complete.cases(v12),]
set.seed(1)
r13 <- rforest(d1$y,d1[, 1:12],mtry = 8,ntree = 100)
v13 <- as.data.frame(infforest(r13, d1$y,d1[1:200, 1:12]))
v13[v13 == "NaN"]<- NA
v13[v13 == "Inf"]<- NA
v13 <- v13[complete.cases(v13),]
```



```{r infforestDist, echo=FALSE, message=FALSE, warning=FALSE}

b <- data.frame("variable" = names(as.data.frame(inf11)),
                  "median" = sapply(as.data.frame(inf11), median),
                 "betas" = bts)

kable(b)

#inff1 <- melt(as.data.frame(inf11)) 
#inff2 <- melt(as.data.frame(inf2)) 
#inff3 <- melt(as.data.frame(inf3)) 
```

```{r mtrycom, echo=FALSE, warning=FALSE, message=FALSE}

b$variable <- factor(b$variable, levels = b$variable)


c <- data.frame("variable" = names(as.data.frame(v12)),
                  "median" = sapply(as.data.frame(v12), median),
                  "iqr" = sapply(as.data.frame(v12), IQR),
                 "betas" = bts)
d <- data.frame("variable" = names(as.data.frame(v13)),
                  "median" = sapply(as.data.frame(v13), median),
                  "iqr" = sapply(as.data.frame(v13), IQR),
                 "betas" = bts)


c$variable <- factor(c$variable, levels = c$variable)

d$variable <- factor(d$variable, levels = d$variable)


b$q1 <- rep(0,12)
b$q3 <- rep(0,12)
for(i in 1:12){
  b$q1[i] <- quantile(inf11[,i], 1/4)
  b$q3[i] <- quantile(inf11[,i], 3/4)
}

c$q1 <- rep(0,12)
c$q3 <- rep(0,12)
for(i in 1:12){
  c$q1[i] <- quantile(v12[,i], 1/4)
  c$q3[i] <- quantile(v12[,i], 3/4)
}
d$q1 <- rep(0,12)
d$q3 <- rep(0,12)
for(i in 1:12){
  d$q1[i] <- quantile(v13[,i], 1/4)
  d$q3[i] <- quantile(v13[,i], 3/4)
}
p <- ggplot(data = b, aes( x = variable, y = median, group = 1)) +
  geom_point() +
   ylab(" ")+
  xlab(" ")+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  geom_line()+
  geom_ribbon(ymin = b$q1 , ymax = b$q3, data = b, alpha = .4)+
  scale_y_continuous(limits = c(0,30))


q<- ggplot(data = c, aes( x = variable, y = median, group = 1)) +
  geom_point() +
  ylab(" ")+
  xlab(" ")+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  geom_line()+
  geom_ribbon(aes(ymin = c$q1, ymax = c$q3), alpha = .4)+
  scale_y_continuous(limits = c(0,30))

r <- ggplot(data = d, aes( x = variable, y = median, group = 1)) +
  geom_point()+
  ylab(" ")+
  xlab(" ")+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  geom_line()+
  geom_ribbon(aes(ymin = d$q1, ymax = d$q3), alpha = .4)+
  scale_y_continuous(limits = c(0,30))
 
```

```{r figmtry, warning=FALSE, echo=FALSE, message = FALSE, fig.cap="Median Values of INFFOREST Variable Importance for mtry = 4,6 and 8"}
grid.arrange(p,q,r, ncol=3)
```

As in (ref Strobl et al 2008), the median INFFOREST variable importance scores are reported here for the dataset $D_1$. [^4]

[^6]: INFFOREST, like any random forest method involving tree- level calculations is a computationally intensive function. The forests are large, unpruned at any level, and INFFOREST takes time to compute. Because of this reason the datasets discussed in CH2 have been altered so that instead of 1000 x 13 dimensional datasets they are 200 x 13. This decreases computation time immensely.

As noted in several publications (Strobl et al, Breiman, Intro to Stat Learning), random forests structure is dependent on the value of `mtry`.[^5] INFFOREST variable importance remains fairly consistent as `mtry` fluctuates.  

[^7]: `mtry` indicates the number of predictors to be considered at each split and can range from 2 to the $p$




