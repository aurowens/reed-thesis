---
title: "Conditional Permuation Importance on A Single Tree"
author: "Aurora Owens"
date: "October 17th, 2016"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
    highlight: kate
    theme: spacelab
---

```{r loadlib, warning=FALSE, message=FALSE}
library(ggplot2)
library(dplyr)
library(tree)
library(randomForest)
library(maptree)
library(gridExtra)
```

#To do:

##Impl't the null distribution method to acquire a p value

- $H_o$ : The true permuted variable importance is 0 or $X

- $H_a$ : The true permuted variable importance is not equal to 0




##Move all functions over to the package (maybe combine a few of them)

#Current hesitations/thoughts

How do I simulate the distribution under H_o?

If I was able to find a p--value of VI for one variable on one tree, would it be valid for models that use ensembles of trees?

This procedure only applies when
 (1) The predictors are correlated enough for the tree algorithm to pick up on it *Important area for further testing*
 (2) Both predictors are chosen by the tree algorithm as suitable predictors of Y

```{r function}
intervalReturn <- function(split.cutleft){ 
intervals <- data.frame(split.cutleft)
intervals <- filter(intervals, split.cutleft != "")
intervals <- select(intervals, split.cutleft)
intervals$split.cutleft <- gsub("<", "", intervals$split.cutleft)
intervals$split.cutleft <- as.numeric(intervals$split.cutleft)
return(arrange(intervals, (split.cutleft)))
}

partitionCreator <- function(i = interval, x = data) {
  p <- c(1:(1+nrow(i)))
  bins <- data.frame (bin = rep(1, length(x)), x = x)
  for(j in 1:nrow(i)){
    bins[bins$x > i[j,1],1] <- p[j+ 1]
  }
  return(bins$bin)
}

condPermuter <- function(data1) {
  data1$p <- factor(data1$p)
  for(i in 1:length(levels(data1$p))){
    #set.seed(1)
    data1[data1$p == data1$p[i], 2] <- sample(data1[data1$p== data1$p[i],2])
  }
  return(data1)
}

```

#Data Simulation

A Three Dimensional Example Where $X_1$ and $X_2$ Are Non-Linearly Correlated

```{r}

set.seed(1)

x1 <- rnorm(1000) #maybe generate from the uniform

set.seed(2)

x2 <- exp(x1) +rnorm(1000) 

#x2 <- 5* sin(3*x1) + 2*cos(3*x1+ .5) + rnorm(1000)

set.seed(3)

#y <- 4*x1 + 0*x2 + rnorm(1000)
#Works


#y <- 0*x1 + 4*x2 + rnorm(1000)
#Works

#y <- 5*x1 + 1*x2 + rnorm(1000)
#Works

y <- 5*x1 + 4*x2 + rnorm(1000) 
#Gives x2 >IM> x1
```

We can see by plotting x1 and x2 that their relationship is not linear, but their relationship does still have some positive linear correlation. 

```{r, echo = FALSE}
data <- data.frame("y" = y, "x1" = x1, "x2" = x2)
ggplot(aes(x = x1, y = x2), data = data) + 
  geom_point(color = "#1B493E")+ 
  ggtitle("Relation of X1 and X2")

cor(x1,x2)
```


Their linear correlation is .68037

```{r, echo=FALSE}
p <- ggplot(aes(x = x1, y = y), data = data) +
  geom_point(color = "#5F1B2B")+
  ggtitle("Y ~X1")

q <- ggplot(aes(x = x2, y = y), data = data)+
  geom_point(color = "#c34e22")+
  ggtitle("Y ~X2")
grid.arrange(p, q, ncol=2)
```

Y appears to be positively correlated with both X1 and X2

#Applying the VIM Procedure

This measure follows the procedure of permuted variable importance detailed in Strobl et al 2007 and Breiman 2001b but with a new permutation sceme and adapted to fit a single tree. 

First, an initial tree is fit using the out-of-the-box data set. 

```{r}
set.seed(1)
tree.o <- tree( (y)~  (x1) +  (x2), data = data)
#tree.o1 <- prune.tree(tree.o, k = 40) Didn't really make a difference in error.o
yhat.o <- predict(tree.o)
error.o <- sum(abs(data$y - yhat.o))/nrow(data)
#draw.tree(tree.o, nodeinfo = FALSE)

plot(tree.o)
text(tree.o)
```

Now let's consider the X2. How important is X2 for predicting Y? We know that X2 and X1 are correlated, but instead of that being an issue we can use that for our advantage. Strobl et al used conditional permuation on a grid defined by the splits on the original tree, but here we are using the grid defined by a new tree that models $X2~X1$. 

```{r}
#library(infTrees)
set.seed(222)
tree1 <- tree( (x2)~ (x1), data = data)
#tree.1 <- prune.tree(tree1, k = 4)
plot(tree1)
text(tree1)

#plot(tree.1)
#text(tree.1)
#tree1$frame
#Partition X2 wrt the splits on X1
intervalX2 <- intervalReturn(split.cutleft = tree1$frame$splits[,1])

#Bin by partition
data$partition.x2 <- partitionCreator(intervalX2, data$x1)
```

Now that we have our grid defined by the splits on X1 in the $X2~X1$ tree, we can permute X2 with respect to this grid.  

```{r}
data11  <-data.frame("p" = data$partition.x2, "x" = data$x2, "onex" = data$x1, "y" = data$y, "orig.x2" =data$x2)

per1 <- condPermuter(data11)

data.permuted.x2 <- per1 %>% rename("partition.x2" = p, "x2.permuted" = x , "x1" = onex)
```

Lastly, we can compare the accuracy of the new tree with permuted data to the accuracy of the original tree to develop a measure of variable importance under the null hypothesis that Y is independent of X2, given that X2 is dependent on X1

```{r}
set.seed(1)
t.x2.permuted <- tree( (y)~  (x1) +  (x2.permuted), data = data.permuted.x2)
yhat.x2.permuted <- predict(t.x2.permuted)
error.x2.permuted <- sum(abs(data.permuted.x2$y - yhat.x2.permuted))/nrow(data.permuted.x2)
x2Imp <- error.x2.permuted- error.o 
(x2Imp)
```

As this value is greater than zero, X2 is an important predictor of Y. By running the same procedure on X1 we can compare the VI of the two predictors.

We can repeat this procedure many times to generate the distribution of `x2Imp`

```{r pvalX2}
x2ImpDist <- rep(0,2000)
set.seed(1)
for(i in 1:2000) {
  data11  <-data.frame("p" = data$partition.x2, "x" = data$x2, "onex" = data$x1, "y" = data$y, "orig.x2" =data$x2)
  per1 <- condPermuter(data11)
  t <- tree( (y)~  (onex) +  (x), data = per1)
  yhat <- predict(t)
  e <- sum(abs(per1$y - yhat))/nrow(per1)
  x2ImpDist[i] <- e- error.o 
}

x2ImpDist <- as.data.frame(x2ImpDist)
ggplot(aes(x2ImpDist), data = x2ImpDist) + geom_histogram(bins = 30) 
```


```{r}
#Do the same with X1
set.seed(222)
tree2 <- tree( (x1)~ (x2), data = data)
plot(tree2)
text(tree2)

#tree1$frame
#Partition X1 wrt the splits on X2
intervalX1 <- intervalReturn(tree2$frame$splits[,1])

#Bin by partition 
data$partition.x1 <- partitionCreator(intervalX1, data$x2)


#Permute X1 wrt these partitions
 
data11  <-data.frame("p" = data$partition.x1, "x" = data$x1, "twox" = data$x2, "y" = data$y, "orig.x1" =data$x1)

per1 <- condPermuter(data11)

data.permuted.x1 <- per1 %>% rename("partition.x1" = p, "x1.permuted" = x , "x2" = twox)
```

```{r}
#Fit a new tree
set.seed(1)
t.x1.permuted <- tree( (y)~  (x1.permuted) +  (x2), data = data.permuted.x1)
yhat.x1.permuted <- predict(t.x1.permuted)
error.rate.x1.permuted <- sum(abs(data.permuted.x1$y - yhat.x1.permuted))/nrow(data.permuted.x1)
x1Imp <- error.rate.x1.permuted- error.o  
```


```{r, echo=FALSE}
tt2 <- ttheme_minimal()
d <- data.frame("variable" = c("x1", "x2"), "variable importance" = c(x1Imp, x2Imp))
grid.table(d, theme = tt2)
```

Now we see that X1 is indeed reported as a more important predictor of Y than X2

```{r pvalX1}
x1ImpDist <- rep(0,2000)
set.seed(1)
for(i in 1:2000){
  data111  <-data.frame("p" = data$partition.x1, "x" = data$x1, "twox" = data$x2, "y" = data$y, "orig.x1" =data$x1)
  per11 <- condPermuter(data111)
  t1 <- tree( (y)~  (x) +  (twox), data = per11)
  yhat1 <- predict(t1)
  e1 <- sum(abs(per11$y - yhat1))/nrow(per11)
  x1ImpDist[i] <- e1- error.o 
}

x1ImpDist <- as.data.frame(x1ImpDist)
ggplot(aes(x1ImpDist), data = x1ImpDist) + geom_histogram(bins = 100) 

```
